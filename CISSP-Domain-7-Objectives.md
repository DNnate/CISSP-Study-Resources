[Domain 7](#domain7-top) **Security Operations**

- **Allowed/Blocked listing**: allowed or blocked entities, register of entities that are being provided (or blocked) for a particular privilege, service, mobility, access or recognition including web, IP, geo, hardware address, files/programs; entities on the allowed list will be accepted, approved and/or recognized; deprecated AKA whitelist/blacklist; systems also alert IT security personnel an access attempt involves a resource not on a pre-approved list; can also incorporate anti-malware
- **Alternate site**: contingency or Continuity of Operations (COOP) site used to assume system or org operations, if the primary site is not available
- **Backup**: copies of files or programs to facilitate recovery. 📝Cloud computing services provide an excellent location for backup storage because they are accessible from any location
- **Baseline**: total inventory of all of a system's components (e.g. hardware, software, data, admin controls, documentation, user instruction); types of baselines include enumerated (which are inventory lists, generated by system cateloging, discovery or enumeration), build security (minimal set of securtiy controls for each CI, see below), modification/update/patch baselines (subsets of total system baseline), or configuration baseline (which should include a revision/version identifier associated with each CI)
- **Configuration Item (CI)**: aggregation of information system components designated for configuration management and treated as a single entity in the config management process
- **Cyber forensics**: gathering, retaining, analyzing data for investigative purposes, while maintaining the integrity of that data
- **Disruption**: unplanned event that causes a system to be inoperable for a length of time
- **Egress monitoring**: monitoring the flow of info out of an org's boundaries
- **Entity**: any form of a user including hardware device, software daemon, task, processing thread or human, which is attempting to use or access system resources; e.g. endpoint devices are entities that human (or non-human) users use to access a system; should be subject to access control and accounting
- **Hackback**: actions taken by a victim of hacking to compromise the systems of the alleged attacker
- **Heuristics**: method of machine learning which identifies patterns of acceptable activity, so that deviations from the patterns will be identified
	- Heuristic-based antimalware software has a higher likelihood of detecting a zero-day exploit than signature-based methods. Heuristic-based software does not require frequent ­signature updates because it does not rely upon monitoring systems for the presence of known malware. The trade-off with this approach is that it has a higher false positive rate
than signature detection methods.
- 🍎**Security Event**: observable occurance in a network or system
    - A security event is any observable occurrence in a system or network that is relevant to the security of the system or data. Events can be benign, suspicious, or malicious, and not all security events are harmful or indicative of a problem.
    - Security events can be routine (such as a successful user login) or unusual (such as multiple failed login attempts).
    - Many security events do not indicate a security breach or issue by themselves. They might be part of normal operations or could be false positives.
    - Examples include
        - A user logging into a system.
        - An email containing a suspicious attachment.
        - A firewall blocking an inbound connection attempt.
        - An unusual spike in network traffic.
- 🍎**Security Incident**: an event which 📝potentially or 📝actually jeopardizes the CIA of an information system or the info the system processes, stores, transmits. Security incidents negatively affect the confidentiality, integrity, or availability 📝CIA of information or assets and/or violate a security policy. e.g backup failure, unauthorized vulnerability scan, ransomware
    - Any 📝attempt to undermine the security of an organization or violation of a security policy is a security incident.
    - A security incident is a specific type of security event that has been identified as a threat to the security of information or systems. It represents a breach of security policies, an attempted or successful unauthorized access, or any activity that poses a risk to the confidentiality, integrity, or availability of information.
    - A security incident may lead to data loss, system downtime, unauthorized access, or other forms of damage.
    - Examples include
        - A successful phishing attack where an attacker gains access to sensitive data.
        - A malware infection spreading across the network.
        - An unauthorized user accessing confidential information.
        - A denial-of-service (DoS) attack that disrupts service availability.
    - ⚒️**Indicator**: technical artifact or observable occurrence suggesting that an attack is imminent, currently underway, or already occured
    - ⚒️**Indicators of Compromise (IoC)**: a signal that an intrusion, malware, or other predefined hostile or hazardous set of events has or is occurring
- **Information Security Continuous Monitoring (ICSM)**: maintaining ongoing awareness of information security, vulnerabilities and threats to support organizational risk management decisions; ongoing monitoring sufficient to ensure and assure effectiveness of security controls
- **Information Sharing and Analysis Center (ISAC)**: entity or collab created for the purposes of analyzing critical cyber and related info to better understand security problems and interdependencies to ensure CIA
- **Log**: record of actions/events that have taken place on a system
- **Precursor**: signal from events suggesting a possible change of conditions, that may alter the current threat landscape
- **Regression testing**: testing of a system to ascertain whether recently approved modifications have changed its performance, or if other approved functions have introduced unauthorized behaviors
- **Root Cause Analysis**: principle-based systems approach for the identification of underlying causes associated with a particular risk set or incidents
- **View-Based access controls**: access control that allows the database to be logically divided into components like records, fields, or groups allowing sensitive data to be hidden from non-authorized users; admins can set up views by user type, allowing only access to assigned views

[7.1](#7.1) Understand and comply with investigations (OSG-9 Chpt 19)
- 🟢**Investigation**: a formal inquiry and systematic process that involves gathering information to determine the cause of a security incident or violation
- Investigators must be able to conduct reliable investigations that will hold up in court; securing the scene is an essential and critical part of every investigation
    - securing the scene might include any/all of the following:
        - sealing off access to the area or crime scene
        - taking images of the scene
        - documenting evidence: Companies have an obligation to preserve evidence whenever they believe that the threat
of litigation is imminent.
        - ensuring evidence (e.g. computers, mobile devices, portable drives etc) is not contacted, tampered with, or destroyed. To be admissible, evidence must be 📝relevant, 📝material to the case at hand, and 📝competent (legally obtained).
        - Proportionality: The benefits of additional discovery must be proportional to the additional costs that they will require. This prevents additional discovery requests from becoming inordinately expensive, and the requester will typically have to justify these requests to the judge presiding over the case.
        - _Interviews_ occur when investigators meet with an individual who may have information relevant to their investigation but is not a suspect. If the individual is a suspect, then the meeting is an _interrogation_.
    - general principles:
        - 🍪identify and secure the scene
        - 🍪protect/preserve evidence: proper collection of evidence preserves its integrity and the chain of custody. For example, The Linux tool 📝dd creates a bit-by-bit copy of the target drive that is well suited to forensic use, and special forensic versions of dd exist that can provide even more forensic features. 
            - The procedures for the protection of evidence are very specific. The precise, regulated, and controlled procedures for the safe and secure retrieval and cataloging of evidence must be followed; otherwise, the evidence cannot be admissible in legal proceedings.
            - All collected evidence must be cataloged and tracked with a chain of custody that creates a paper trail showing the seizure, custody, control, transfer, analysis, and disposal of evidence. The 📝chain of custody (also called the chain of evidence) documents all individuals who handled the evidence and helps ensure that evidence can be used in court proceedings.
            - When investigating computer crimes, an investigator should never use the original evidence. Instead, a bit-by-bit copy of the original drive should be created and used for the investigation. If an investigator taints the original evidence, it may be dismissed in court. Original evidence should always be collected and documented using a Chain of Custody. You could taint the original evidence by 🚫logging on to the endpoint or device without a 📝forensice disk controller\📝write bloxcker.
            - 🔥A Faraday bag prevents electromagnetic interference and the ability to remotely wipe or otherwise interact with a device remotely. It is used to ensure evidence on mobile device is not tampered with remotely. It t prevent wireless communication to and from the mobile device and prevent the transference or reception of radio waves that could potentially be used to tamper evidence.
            - 🔥A forensic disk controller or hardware write-block device is a specialized type of computer hard disk controller made for the purpose of gaining read-only access to computer hard drives without the risk of damaging the drive's contents. One of the main functions of a forensic drive controller is preventing any command sent to a device from modifying data stored on the device. For this reason, forensic drive controllers are also often referred to as 📝write blockers.
            - 🍾It  intercept write commands from the host operating system, preventing them from reaching the drive.
            - 🍾It returns data requested by a read operation
            - 🍾It returns access-significant information from the device
            - 🍾It reports errors from the device back to the forensic host.
        - 🍪identification and examination of the evidence
        - 🍪further analysis of the most compelling evidence
        - 🍪final reporting of findings
    
- 💥**Locard exchange principle**: whenever a crime is commited something is taken, and something is left behind. Locard's principle suggests that any contact between two objects results in an exchange of materials. In the context of digital forensics, this principle implies that there is always some form of digital trace or residue left behind when devices interact or when data is transferred. 
- The purpose of an investigation is to:
    - identify the root cause of the incident
    - prevent future occurrences
    - mitigate the impact of the incident on the organization
- Types of investigations:
    - 📕**administrative**: an investigation that is focused on policy violations. They are internal to an orgaisation.
        - Operational investigations have the loosest standards for collection of information. They are not intended to produce evidence because they are for internal operational purposes only. In addition to resolving the operational issue, operational investigations often conduct a root cause analysis that seeks to identify the reason that an operational issue occurred.
        - Internal investigations usually operate under the authority of senior managers, who grant access (i.e., voluntary surrender) to all information and resources necessary to conduct the investigation. 
    - 📕**criminal**: conducted by law enforcement, this type of investigation tries to determine if there is cause to believe (beyond a reasonable doubt) that someone committed a crime
        - the goal is to gather evidence that can be used to convict in court
        - the job of a security professional is to preserve evidence, ensure law enforcement has been contacted, and assist as necessary
        - follows the 📝beyond a reasonable doubt standard of proof
        - Involving law enforcement is a complicated decision. Sometimes, it is legally mandated to notify law enforcement. However, in certain circumstances, involving law enforcement can potentially lead to incidents becoming public. Senior management should always be consulted before law enforcement is involved. A CISSP should remember the (ISC)² Code of Ethics and never allow themselves to become culpable for a crime.
    - 📕**civil**: non-criminal investigation for matters such as contract disputes
        - the goal of a civil investigation is to gather evidence that can be used to support a legal claim in court, and is typically triggered from an imminent or on-going lawsuit
        - the level of proof is much lower for a civil compared to a criminal investigation
        - follow the 📝preponderance of the evidence standard. 
    - 📕**regulatory**: investigation initiated by a government regulator when there is reason to believe an organization is not in compliance
        -  this type of investigation varies significantly in scope and could look like any of the other three types of investigation depending on the severity of the allegations
        - as with criminal investigations, it is key to preserve evidence, and assist the regulator’s investigators
- ❄️**Electronic Discovery**: Electronic Discovery (e-Discovery) is a term used to describe the process of identifying and producing Electronically Stored Information (ESI) requested by a court subpoena. In 🧠legal proceedings, each side has a duty to preserve evidence related to the case and, through the discovery process, share information with their adversary in the proceedings. This discovery process applies to both paper records and electronic records, and the electronic discovery (or eDiscovery) process facilitates the processing of electronic information for disclosure.
- Electronic discovery (e-discovery) standardizes the collection and preservation of data. It describes the reference model for the collection and preservation of evidence The Electronic Discovery Reference Model (EDRM) describes a standard process for conducting eDiscovery with nine aspects:
    - ✏️Information Governance: Ensures that information is well organized for future eDiscovery efforts.
    - ✏️Identification: Locates the information that may be responsive to a discovery request when the organization believes that litigation is likely.
    - ✏️Preservation: Ensures that potentially discoverable information is protected against alteration or deletion.
    - ✏️Collection: Gathers the relevant information centrally for use in the eDiscovery process.
    - ✏️Processing: Screens the collected information to perform a “rough cut” of irrelevant information, reducing the amount of information requiring detailed screening.
    - ✏️Review: Examines the remaining information to determine what information is relevant to the request and removing any information protected by 📝attorney-­client privilege.
    - ✏️Analysis: Performs deeper inspection of the content and context of remaining information.
    - ✏️Production: Places the information into a format that may be shared with others and delivers it to other parties, such as opposing counsel.
    - ✏️Presentation: Displays the information to witnesses, the court, and other parties.

- 7.1.1 🔴Evidence collection and handling
    - Evidence (or artifacts) collection is complex, should be done by professionals, and can be thrown out of court if incorrectly handled
    - It’s important to preserve original evidence
    - 🔥**International Organization on Computer Evidence (IOCE)** six principles for media, network and software analysis:
        - all general forensic and procedural principles must be applied to digital evidence collection 
        - seizing digital evidence shouldn't change the evidence
        - accessing original digital evidence should only be done by trained professionals
        - all activity relating to seizure, access, storage, or transfer of digital evidence must be fully documented, preserved, and available for review
        - a person in possession of digital evidence is responsible for all actions taken with respect to that evidence
        - any agency that is responsible for seizing, accessing, storing, or transferring digital evidence is responsible for compliance with these principles
    - 🔥**Scientific Working Group on Digital Evidence (SWGDE)** developed principles for standardized recovery of computer-based evidence:
        - legal system consistency
        - use of a common language
        - durability
        - ability to cross international and state boundaries
        - instill confidence in evidence integrity
        - forensic evidence applicability at the individual, agency, and country levels
    - 🔥**ISO/IEC 27037: Guidelines for Identification, Collection, Acquisition, and Preservation of Digital Evidence**: the international standard on digital evidence handling, with four phases:
        - identification
        - collection 
        - acquisition
        - preservation
    - Types of evidence:
        - 🧑‍⚖️**primary evidence**:
            - most reliable and used at trial
            - original documents (e.g. legal contracts), no copies or duplicates
        - 🧑‍⚖️**secondary evidence**:
            - less powerful and reliable than primary evidence (e.g. copies of originals, witness oral evidence etc)
            - if primary evidence is available secondary of the same content is not valid
        - 🧑‍⚖️**Real Evidence**: also known as object evidence. this type of evidence includes 📝physical objects, such as computers, hard drives, and other storage devices, that can be brought into a court of law. Real evidence must be either 📝uniquely identified by a witness or authenticated through a 📝documented chain of custody.  Real evidence is object evidence—tangible things that can be brought into a court of law.
            - Real evidence is defined as any 📝object that can be used to prove or disprove a fact. This would include DNA evidence, sound recordings, video recordings, weapons, fingerprints, etc. 
            - Police investigators, evidence technicians, attorneys, and anyone else involved in the collection, processing, analysis, and production of evidence must maintain the chain of custody 
        - 🧑‍⚖️**Documentary Evidence**: includes any 📝written items brought into court to prove a fact at hand. This type of evidence must also be authenticated. Note📝Server logs, 📝system logs are an example of documentary evidence. Two additional evidence rules apply specifically to documentary evidence:
             - ✴️The best evidence rule states that when a document is used as evidence in a court proceeding, the original document must be introduced. Best evidence is a form of documentary evidence, but specifically it is the original document rather than a copy or description.
             - ✴️The parol evidence rule states that when an agreement between parties is put into written form, the written document is assumed to contain all the terms of the agreement and no verbal agreements may modify the written agreement.
        - 🧑‍⚖️**Direct evidence**: this type of evidence is based on the observations of a witness or expert opinion and can be used to prove a fact at hand (with backup evidence support). Direct evidence may come from witnesses who give oral testimony based on their observations. Direct evidence cannot be hearsay, which is second-hand testimony.
            - 👽**Expert witness** may 📝express their opinions or conclusions without providing direct evidence. An expert witness is believed to have greater insight because they have professional experience and expertise in the topic that their testimony covers. They can add insight and information to legal proceedings for various crimes. Expert witnesses should not be confused with standard witnesses who provide direct observational evidence about the case.
        - 🧑‍⚖️**Conclusive evidence**: this type of evidence is irrefutable and cannot be contradicted.
        - 🧑‍⚖️**Circumstantial evidence**: this type of evidence is based on inference and can be used to support a conclusion, but not prove it. Circumstantial evidence can prove an intermediate fact used to assume another fact.
        - 🧑‍⚖️**Corroborative evidence**: this type of evidence is used to support other evidence and can be used to strengthen a case. Corroborative evidence is used to prove an idea or point that cannot stand on its own.
        - 🧑‍⚖️**Demonstrative Evidence** is evidence used to support testimonial evidence. It consists of items that may or may not be admitted into evidence themselves but are used to help a witness explain a concept, to illustrate a concept or clarify an issue. e.g a chart or diagram
        - 🧑‍⚖️**Testimonial/Firsthand Evidence** is, quite simply, evidence consisting of the testimony of a witness, either verbal testimony in court or written testimony in a recorded deposition. Witnesses must take an oath agreeing to tell the truth. Firsthand evidence comes from an eyewitness. The witnesses actually saw or heard the activity themselves.  The witness is an expert if they have professional experience and expertise in the topic that they are testifying on.
        - 🧑‍⚖️**Hearsay evidence**: type of evidence that is based on statements made by someone outside of court and is generally not admissible. The hearsay rule says that a witness cannot testify about what someone else told them, except under very specific exceptions. However, the courts have applied the hearsay rule to include the concept that attorneys may not introduce logs into evidence unless they are 📝authenticated by the system administrator. In this scenario, the administrator might also be able to provide a sworn affidavit. 
        - 🧑‍⚖️**Best evidence rule**: states that the original evidence should be presented in court, rather than a copy or other secondary evidence
    - It is important to note that evidence should be collected and handled in a forensically sound manner to ensure that it is admissible in court and to avoid any legal issues
    - 🔥**The chain of custody**: focuses on having control of the evidence -- who collected and handled what evidence, when, and where. The chain of custody is chronological documentation or a paper trail showing the seizure, custody, control, transfer, analysis, and disposal of evidence. The chain of custody (also called the chain of evidence) documents all individuals who handled the evidence and helps ensure that evidence can be used in court proceedings. The chain of custody (also called the 📝evidence chain) is the process of logging access and location of evidence and its condition during investigations. It documents who had control of evidence at any given point to ensure that it is admissible during trial. It also documents what was done with the evidence, where it was, how it was treated, how it was moved from location to location, and when it was handled.
        - think about establishing the chain of custody as:
            - tag, 
            - bag, and 
            - carry the evidence
    - Generally, It is critical that evidence is related to the case. It must be convincing and reliable (at least in most legal systems).
    - Five rules of evidence: five evidence characteristics providing the best chance of surviving legal and other scrutiny:
        - 🚦**authentic**: evidence is not fabricated or planted, and can be proven through crime scene photos, or bit-for-bit copies of storage
        - 🚦**accurate**: evidence that has integrity (not been modified)
        - 🚦**complete**: evidence must be complete, and all parts available and shared, whether they support the case or not
        - 🚦**convincing**: evidence must be easy to understand, and convey integrity
        - 🚦**admissible**: evidence must be accepted as part of a case

- 7.1.2 Reporting and documentation
    - Each investigation should result in a final report that documents the goals of the investigation, the procedures followed, the evidence collected, and the final results
    - Preparing formal documentation prepares for potential legal action, and even internal investigations can become part of employment disputes
    - Identify in advance a single point of contact who will act as your liasion with law enforcement, providing a go-to person with a single perspective, potentially improving the working relationship
    - Participate in the FBI’s InfraGard program

- 7.1.3 Investigative techniques
    - Whether in response to a crime or incident, an organizational policy breach, troubleshooting a system or network issue etc, digital forensic methodologies can assist in finding answers, solving problems, and in some cases, help in successfully prosecuting crimes
    - The forensic investigation process should include the following:
        - identification and securing of a crime scene
        - proper collection of evidence that preserves its integrity and the chain of custody
        - examination of all evidence
        - further analysis of the most compelling evidence
        - final reporting
    - Sources of information and evidence:
        - 🔨oral/written statements: given to police, investigators, or as testimony in court by people who witness a crime or who may have pertient information
        - 🔨written documents: checks, printed contracts, handrwitten letters/notes
        - 🔨computer systems: components, local/portable storage, memory etc
        - 🔨visual/audio: visual and audio evidence pertient to a security investigation could include photographs, video, taped recordings, and surveillance footage from security cameras

- 7.1.4 Digital forensics tools, tactics, and procedures
    - Digital forensics: the scientific examination and analysis of data from storage media so that the information can be used as part of an investigation to identify the culprit or the root cause of an incident
    - 🍍**Live evidence**: data stored in a running system e.g. random access memory (RAM), cache, and buffers
        - When dealing with a potential rootkit, a live response is crucial because rootkits often manipulate or hide their presence by altering the operating system and can evade detection if the system is shut down or rebooted. A live response allows you to collect volatile data (such as running processes, open network connections, and in-memory evidence) directly from the system while it is still running, which is essential for detecting and analyzing rootkits.
        - Memory collection is part of Live evidence collection and is your go-to technique when faced with a potential rootkit attack requiring immediate attention. Given the stealthy nature of 📝rootkits, which operate deep within the system's architecture, capturing data directly from memory (RAM) is crucial. This process not only aids in identifying and understanding the current, active state of the system, including any anomalous processes, but also ensures that the evidence remains uncontaminated by the rootkit, as it can actively disguise traces on non-volatile storage. This technique becomes invaluable in real-time examination and mitigating ongoing security threats effectively.
        - Examining a live system can change the state of the evidence 
            - small changes like interacting with the keyboard, mouse, loading/unloading programs, or of course powering off the system, can change or eliminate live evidence
        - Whenever a forensic investigation of a storage drive is conducted, two identical bit-for-bit copies of the original drive should be created first
    - 🍍**eDiscovery**: Electronic discovery, or E-discovery, is the process of identifying, collecting, analyzing and producing Electronically Stored Information (ESI) for 📝legal purposes and in legal proceedings. It is used in legal investigations, litigation, and compliance. Electronic review involves the examination and analysis of ESI during the e-discovery process, typically for legal purposes, compliance, or investigation.

- 7.1.5 Artifacts (e.g. computer, network, mobile device)
    - Forensic artifacts: remnants of a system or network breach/attempted breach, which and may or may not be relevant to an investigation or response
    - Artifacts can be found in numerous places, including:
        - computer systems
        - web browsers
        - mobile devices
        - hard drives, flash drives
- Forensics: Several investigative techniques can be used when conducting analysis:
    - 💢Media Analysis: a branch of computer forensic analysis, involves the identification and extraction of information from storage media e.g magnetic media (e.g., hard disks, tapes) or optical media (e.g., CDs, DVDs, Blu-­ray discs). Analysts should never access hard drives or other media from a live system. Instead, they should power off the system (after collecting other evidence), remove and then attach the storage device to a dedicated forensic workstation, using a write blocker. Write blockers are hardware adapters that physically sever the portion of the cable used to connect the storage device that would write data to the device, reducing the likelihood of accidental tampering with the device. The analyst should immediately calculate a cryptographic hash of the device contents and then use forensic tools to create a forensic image of the device: a bitwise copy of the data stored on the device. The analyst should then compute the cryptographic hash of that image to ensure that it is identical to the original media contents.
        - media analysis: examining the bits on a hard drive that are intact dispite not having an index 
    - 💢Memory Analysis: This is a tricky undertaking, since it can be difficult to work with memory without actually altering its contents. When gathering the contents of memory, analysts should use trusted tools to generate a memory dump file and place it on a forensically prepared device, such as a USB drive. As with other types of digital evidence, the analyst should compute a cryptographic hash of the dump file to later prove its authenticity.
    - 💢Network Analysis: This is often difficult to reconstruct due to the volatility of network data—­if it isn’t deliberately recorded at the time it occurs, it generally is not preserved. Network forensic analysis depend on the use of preexisting security controls that log network activity.
        - 📝SPAN ports and 📝network taps are the most reliable mechanisms for gathering network traffic. Packet capture software is also a possible technique, but it is less reliable because it must run on one of the targeted systems. NetFlow data is a reliable source of evidence, but it only captures information about the parties involved in a communication and the amount of data exchanged. It does not include the contents of network traffic. 
    - 💢Software Analysis: review of software code, looking for backdoors, logic bombs, or other security vulnerabilities. 📝Maintenance hooks, otherwise known as backdoors, provide developers with easy access to a system, bypassing normal security controls. If not removed prior to finalizing code, they pose a significant security vulnerability if an attacker discovers the maintenance hook.
        - software analysis: focuses on an applications and malware, determining how it works and what it's trying to do, with a goal of attribution 
    - 💢Hardware/Embedded Device Analysis: This may include a review of Personal computers, Smartphones, Tablet computers, Embedded computers in cars, security systems, and other devices. Analysts conducting these reviews must have specialized knowledge of the systems under review. 
- **Major Categories of Computer Crime**: A computer crime is a crime (or violation of a law or regulation) that involves a computer. The crime could be against the computer, or the computer could have been used in the actual commission of the crime.
    - ✈️Military and intelligence attacks:attacks are launched primarily to obtain secret and restricted information from law enforcement or military and technological research sources. 
    - ✈️Business attacks: The gathering of a competitor’s confidential intellectual property, also called corporate espionage or industrial espionage. focus on illegally jeopardizing the confidentiality, integrity, or availability of information and systems operated by a business
    - ✈️Financial attacks:  carried out to unlawfully obtain money or services. Financial attacks may also take the form of cybercrime for hire
    - ✈️Terrorist attacks: The purpose of a terrorist attack is to disrupt normal life and instill fear, whereas a military or intelligence attack is designed to extract secret information. 
    - ✈️Grudge attacks:  attacks that are carried out to damage an organization or a person. The damage could be in the loss of information or information processing capabilities or harm to the organization or a person’s reputation. The motivation behind a grudge attack is usually a feeling of resentment
    - ✈️Disgruntled Employee attacks: Most industries agree that one of the most significant threats a company faces is from its own employees. For this reason, companies should employ principles like segregation of duties, split knowledge, and least privilege.
    - ✈️Thrill attacks: are the attacks launched only for the fun of it. Attackers who lack the ability to devise their own attacks will often download programs that do their work for them. These attackers are often called script kiddies 
    - ✈️Hacktivist attacks: These attackers, known as hacktivists (a combination of hacker and activist), often combine political motivations with the thrill of hacking. Anonymous and LulzSec and use tools like the Low Orbit Ion Cannon (LOIC) to create large-­scale DoS attacks. Hacktivism generally occurs because an individual is politically motivated to carry out an information security attack. Examples of hacktivism include the defacing of government websites or tarnishing a private organization or person's reputation because of political positions.
        - At the extreme end of hacktivism, 📝suicide hackers engage in highly destructive activity with the knowledge that they will most likely be caught.

[7.2](#7.2) Conduct logging and monitoring activities (OSG-9 Chpts 17,21)

- 7.2.1 Intrusion detection and prevention
    - **Intrusion**: a security event, or a combination of multiple security events that constitutes an incident; occurs when an attacker attempts to bypass or can bypass or thwart security mechanisms and access an organization’s resources without the authority to do so
        - 🎈A false negative occurs when there is an attack but the IDS/IPS doesn't detect it and raise an alarm.
        - 🎈A false positive occurs when an IDS/IPS incorrectly raises an alarm, even though there isn't an attack. 
    - **Intrusion detection**: a specific form of monitoring events, usually in real time, to detect abnormal activity indicating a potential incident or intrusion
    - 🍀**Intrusion Detection System (IDS)**: a security service that monitors and analyzes network or system events for the purpose of finding/providing realtime/neartime warnings of unauthorized attempts to access system resources; automates the inspection of logs and real-time system events to detect intrusion attempts and system failures. It provides only 📝_passive_ responses, such as alerting administrators to a suspected attack
        - an IDS is intended as part of a defense-in-depth security plan
        - An IDS is most likely to connect to a switch port configured as a 📝mirrored port or a 📝Network TAP
        - A network Traffic Access Point (TAP) is a physical device used to capture network flows between devices. A network TAP can only reproduce traffic and cannot alter or change the traffic as it flows through it. Network TAPs are commonly used when installing an Intrusion Detection System (IDS) 
        - 🍏Anomaly-based intrusion detection systems may identify a zero-day vulnerability because it deviates from normal patterns of activity. An anomaly-based IDS requires a 📝baseline, and it then monitors traffic for any anomalies or changes when compared to the baseline. It's also called 📝behavior based and 📝heuristics based. 
        - 🍏Pattern-based detection (also known as 📝knowledge-based detection and 📝signature-based detection) uses known signatures to detect attacks. A pattern-matching IDS (also known as signature-based detection) has a low false-positive rate.
            - 🍊**Host-­based IDSs (HIDSs)** can monitor activity on a single system only. A drawback is that attackers can discover and disable them. HIDS may be able to detect unauthorized processes running on a system. Host-based IDSs have some difficulty with detecting and tracking down DoS attacks.
            - 🍊**Network-­based IDS (NIDS)** can monitor activity on a network, and an NIDS isn’t as visible to attackers. They may  not notice rogue processes. NIDSs usually have the ability to detect and analyze encapsulation. Network-based IDSs are usually able to detect the initiation of an attack or the ongoing attempts to perpetrate an attack (including denial of service, or DoS). They are, however, unable to provide information about whether an attack was successful or which specific systems, user accounts, files, or applications were affected.
    - 🍀**Intrusion Prevention Systems (IPS)**: a security service that uses available info to determine if an attack is underway, alerting and also blocking attacks from reaching intended target; includes detection capabilities, you’ll also see them referred to as intrusion detection and prevention systems (IDPSs)
    - NIST SP 800-94 Guide to Intrusion Detection and Prevention Systems provides comprehensive coverage of both IDS and IPS
    - They provide 📝active response to a security event.
    - A network-based intrusion prevention system (NIPS) is placed in line with the traffic and can prevent attacks from reaching an internal network
    - An IPS makes forwarding decisions in real-time and must be strategically placed to be effective. An IPS can be installed between a router and a firewall, between two switches, between a switch and a router, or in any other location where the corporation wants to watch and control traffic that they do not want. Most commonly, it would be installed after the Internet-connected router and the first firewall.
        - 🍊**NIPS** can also detect and protect against attacks using pattern-matching (also known as signature-based detection and knowledge-based detection). A NIPS can take action to prevent an attack. It operates at the network level and inspects packets flowing through the network. E.g Firepower, Fortigate NGFW, Palo Alto NGFW, Snort. It Operates at the network level, monitoring all traffic flowing through the network.
        - 🍊**HIPS**: A HIPS protects individual hosts or endpoints by monitoring and analyzing activity at the host level, detecting malicious behavior, and stopping unauthorized actions. e.g McAfee Host Intrusion Prevention, CrowdStrike Falcon, Symantec Endpoint Protection (SEP). It  Operates at the host level, protecting individual devices or servers.
- 7.2.2 ❄️**Security Information and Event Management (SIEM)**
    - NIST Special Publication 800-92, the Guide to Computer Security Log Management, describes four types of common challenges to log management:
        - 🔨Many log sources
        - 🔨Inconsistent log content
        - 🔨Inconsistent timestamps
        - 🔨Inconsistent log formats
    - A security information and event management (SIEM) tool is designed to centralize logs from many locations in many formats and to ensure that logs are read and analyzed despite differences between different systems and devices.
    - Security Information and Event Management (SIEM): systems that collects/ingest logs from multiple sources, compile and analyze log entries, and report relevant information
    - A SIEM system aggregates the system logs from multiple systems. The SIEM then correlates the logs from the many sources within the network to 'connect the dots' and determine if there has been a likely compromise. This generates the Indication of Compromise 📝(IoC). Using a SIEM, an administrator can create reports that show access history and are useful for security personnel to verify physical or logical access.
        - A SIEM can be configured to generate alerts if certain logs or patterns are observed.
        - SIEM systems are complex and require expertise to install and tune
        - Provide real-time monitoring, traffic analysis, & notification of potential attack (Traffic analysis focuses more on the patterns and trends of data rather than the actual content)
        - require a properly trained team that understands how to read and interpret info, and escalation procedures to follow when a legitimate alert is raised
        - SIEM systems represent technology, process, and people, and each is important to overall effectiveness
        - 📝PFELK, 📝Security Onion, and 📝Ntopng are all SIEMs. PFELK is a combination of Elastic, Logstash, and Kibana (ELK) designed to integrate with the open-source routing software PFSense. Another tool that integrates with PFSense is Ntopng, which is a downloadable package available within the PFSense routing software.
        - a SIEM includes significant intelligence functionality, allowing large amounts of logged events and analysis and correlation of the same to occur very quickly
    - SIEM capabilities include:
        - 🎵Log Centralization & Aggregation
        - 🎵Normalization data into a common event schema
        - 🎵Data Integrity
        - 🎵Correlation
        - 🎵Secure storage
        - 🎵Analysis: Broad visibility across data, apps, identities, endpoints, and infrastructure
        - 🎵Automated and Continous/Investigative Monitoring and response
        - 🎵Alerting and Reporting
🔄 Log Collection ↪️ SIEM ↪️ SOAR ↪️ SOC Team 🔄 Log Collection
- 7.2.2.1 Security Orchestration, Automation, and Response (SOAR)
    - ❄️**Security Orchestration, Automation, and Response (SOAR)**: refers to a group of technologies that allow orgs to respond to some incidents automatically. It is a centralized alert and response automation tool with threat specific playbooks and runbooks. Response is in a digital work-flow format and could be automated or sem-automated (requiring a single click). It often uses AI, Machine Learning and Threat Intelligence.
        - 🐍**Playbook**: a document or checklist that defines how to verify/define an incident and the action taken. Playbook is the paperwork.
        - 🐍**Runbook**: implements the playbook data into an automated tool. When integrated with SIEM, some runbooks are invokable from the SIEM. Runbook is the automated technology.
    - SOAR allows security admins to define these incidents and the response, typically using playbooks and runbooks
    - Both SOAR and SIEM integrated platforms can help detect and, in the case of SOAR, respond to threats against your software development efforts. It reduces Mean Time to Detection MTTD and accelerates response.
    - Security information and event management (SIEM) systems do correlate information from multiple sources and perform analysis, but they stop short of providing automated playbook responses. That is the realm of security orchestration, automation, and response (SOAR) platforms.
        - devs can be resistent to anything that slows down the development process, and this is where DevSecOps can help build the right culture, and balance the needs of developers and security
        - 📝Splunk is a Security Orchestration, Automation, and Response (SOAR) tool that is mostly proprietary with a very limited open-source trial. Currently, Splunk is one of the most popular SOARs used in enterprise environments.

- **Syslog**: RFC 5424, the Syslog Protocol, describes the syslog protocol, which is used to send event notification messages. A centralized syslog server receives these syslog messages from devices on a network. The protocol defines how to format the messages and how to ✏️send them to the syslog server but ✏️not how to handle them. Syslog has historically been used in Unix and Linux systems. These systems include the syslogd daemon, which handles all incoming syslog messages, similar to how a SIEM server provides centralized logging. Some syslogd extensions, such as syslog-­ng and rsyslog, allow the syslog server to accept messages from any source, not just Unix and Linux systems.
    - Implementations of syslog vary, but most provide a setting for severity level, allowing configuration of a value that determines what messages are sent. Typical severity levels include debug, informational, notice, warning, error, critical, alert, and emergency. The facility code is also supported by syslog, but is associated with which services are being logged.
    - Syslog uses UDP port 📝514. TCP-based implementations of syslog typically use port 📝6514.
- 📗**Sampling or Data Extraction**, is the process of extracting specific elements from a large collection of data to construct a meaningful representation or summary of the whole. In other words, sampling is a form of data reduction that allows someone to glean valuable information by looking at only a small sample of data in an audit trail.
- Both statistical and nonstatistical sampling are valid mechanisms to create summaries or overviews of large bodies of audit data. However, statistical sampling is more reliable and mathematically defensible.
     - 🍮Statistical sampling uses precise mathematical functions to extract meaningful information from a large volume of data. There is always a risk that sampled data is not an accurate representation of the whole body of data, and statistical sampling can identify the margin of error.
     - 🍮Nonstatistical sampling is discretionary sampling, or sampling at the auditor’s discretion. It doesn’t offer an accurate representation of the whole body of data and will ignore events that don’t reach the 📝clipping level threshold. However, it is effective when used to focus on specific events. Additionally, nonstatistical sampling is less expensive and easier to implement than statistical sampling.
          - ⚒️Clipping Levels: Clipping is a form of nonstatistical sampling. It selects only events that exceed a clipping level, which is a predefined threshold for the event. The system ignores events until they reach this threshold.
              - For example, failed logon attempts are common in any system, since users can easily enter the wrong password once or twice. Instead of raising an alarm for every single failed logon attempt, a clipping level can be set to raise an alarm only if it detects five failed logon attempts within a 30-­minute period.
              - Many account lockout controls use a similar clipping level. They don’t lock the account after a single failed logon. Instead, they count the failed logons and lock the account only when the predefined threshold is reached.
              - Clipping levels are widely used in the process of auditing events to establish a baseline of routine system or user activity. The monitoring system raises an alarm to signal abnormal events only if the baseline is exceeded.
              - The clipping level is a threshold for the number of error occurrences before it's considered suspicious or sets off an alarm. For example, ten attempts with an incorrect password would trigger an account lockout and password reset.
              - Clipping level is the more theoretical term for what is now commonly called thresholds.
	  
- 7.2.3 🔴Continuous monitoring
    - After a SIEM is set up, configured, tuned, and running, it must be routinely updated and continuously monitored to function effectively
    - Effective continuous monitoring encompasses 📝technology, 📝processes, and 📝people
    - Continuous monitoring steps are:
        - Define
        - Establish
        - Implement
        - Analyze/report
        - Respond
        - Review/update
    - **monitoring**: the process of reviewing information logs, looking for something specific
        - necessary to detect malicious actions by subjects as well as attempted intrusions and system failures
        - can help reconstruct events, provide evidence for prosecution, and create reports for analysis
        - continuous monitoring ensures that all events are recorded and can be investigated later if necessary
    - **log analysis**: a detailed and systematic form of monitoring where logged info is analyzed for trends and patterns as well as abnormal, unauthorized, illegal, and policy-violating activities
        - log analysis isn’t necessarily in response to an incident, it’s a periodic task
    
- 7.2.4 Egress monitoring
    - ✴️**Egress monitoring** refers to monitoring outgoing traffic to detect unauthorized data transfer outside the org (AKA data exfiltration). It’s important to monitor traffic exiting as well as entering a network
    - Common methods used to detect or prevent data exfiltration are data loss prevention (DLP) techniques and monitoring for steganography. Note stegaography (embedded and not visible to naked eye) and watermark (visible to naked eye)
        - 🛠️Network-­based data loss prevention (DLP) system monitors outgoing traffic (egress monitoring) and can thwart data exfiltration attempts.
        - 🛠️Firewalls: For egress traffic, only Traffic with a destination address on an external network should generally be allowed. Also, All packets leaving the network should have a source address from your public IP address block.
          - For Ingress traffic, Packets with internal source IP addresses should not be allowed to enter the network from the outside because they are likely spoofed. Packets with internal source IP addresses should be able to exit the network from the inside, and packets with external source IP addresses should be able to enter the network from the outside, as these are both normal network activity. Packets with public IP addresses should be able to pass through the firewall in both directions, assuming that they meet other security requirements.
- 7.2.5 Log management
    - **Log management**: refers to all the methods used to collect, process, and protect log entries (see SIEM definition above)
    - **rollover logging**: allows admins to set a maximum log size, when the log reaches that max, the system begins overwriting the oldest events in the log
- 7.2.6 Threat intelligence (e.g. threat feeds, threat hunting)
    - 🍎**Threat hunting** is the process of actively searching for infections or attacks within a network. The basic assumption of a threat hunting exercise is the so-called 📝presumption of compromise. Threat intelligence refers to the actionable intelligence created after analyzing incoming data, such as threat feeds. Threat hunters use threat intelligence to search for specific threats. Additionally, they may use a kill chain model to mitigate these threats. Threat hunters regularly monitor threat feeds and using that information to check systems within the network. Their goal is to discover any infections or attacks that haven’t been detected by existing tools.
    - 🍎**Threat Feeds**:Threat feeds provide organizations with a steady stream of raw data. By analyzing threat feeds, security administrators can learn of current threats. They can then use this knowledge to search through the network, looking for signs of these threats.
    - 🍎**Threat intelligence**: an umbrella term encompassing threat research and analysis and emerging threat trends; gathering data on potential threats, including various sources to get timely info on current threats; information that is aggregated, transformed, analyzed, interpreted, or enriched to provide the necessary context for the decision-making process. Examples of threat intelligence feed standards include. 
        - 🐍CAPEC, or Common Attack Pattern Enumeration and Classification, is a dictionary of known attack patterns.
        - 🐍STIX is the Structured Threat Information eXpression language used to describe threats in a standardized way
        - 🐍TAXII, the Trusted Automated eXchange of Indicator Information, defines how threat information can be shared and exchanged. 
    - 🟦**Kill chain**: military model (used for both offense and defense):
        - find/identify a target through reconnaissance
        - get the target’s location
        - track the target’s movement
        - select a weapon to use on the target
        - engage the target with the selected weapon
        - evaluate the effectiveness of the attack
    - Orgs have adapted this model for cybersecurity: Lockheed Martin created the **Cyber Kill Chain** framework including seven ordered stages of an attack:
        - ♋**reconnaissance**: attackers gather info on the target
        - ♋**weaponize**: attackers identify an exploit that the target is vulnerable to, along with methods to send the exploit
        - ♋**delivery**: attackers send the weapon to the target via phishing attacks, malicious email attachments, compromised websites, or other common social engineering methods
        - ♋**exploitation**: the weapon exploits a vulnerability on the target system
        - ♋**installation**: code that exploits the vulnerability then installs malware with a backdoor allowing attacker remote access
        - ♋**command and control**: attackers maintain a command and control system, which controls the target and other compromised systems. Social media is commonly used as a command-and-control system for botnet activity.
        - ♋**actions on objectives**: attackers execute their original goals such as theft of money, or data, destruction of assets, or installing additional malicious code (eg. ransomware)

- 🟦**The MITRE ATT&CK Matrix**: (created by MITRE and viewable at attack.mitre.org) is a knowledge base of identified tactics, techniques, and procedures (TTPs) used by attackers in various attacks. It is complementary to kill chain models, such as the Cyber Kill Chain. However, unlike kill chain models, the tactics are not an ordered set of attacks. Instead, ATT&CK lists the TTPs within a matrix. Additionally, attackers are constantly modifying their attack methods, so the ATT&CK Matrix is a living document that is updated at least twice a year. The matrix includes the following tactics:
     - Reconnaissance
     - Resource development
     - Initial access
     - Execution
     - Persistence
     - Privilege escalation
     - Defense evasion
     - Credential access
     - Discovery
     - Lateral movement
     - Collection
     - Command and control
     - Exfiltration
     - Impact
  
- 7.2.7 User and Entity Behavior Analytics (UEBA)
    - ✈️**UEBA (aka UBA)**: focuses on the analysis of user and entity behavior as a way of detecting inappropriate or unauthorized activity (e.g. fraud, malware, insider attacks etc); analysis engines are typically included with SIEM solutions or may be added via subscription.
        - User and Entity Behavior Analytics (UEBA) refers to a security capability that utilizes 📝machine learning to establish a baseline of expected user and device activity in order to detect deviations that might indicate a compromise. 
    - ✈️**Behavior-based detection**: (AKA statistical intrusion, anomaly, and heuristics-based detection), starts by creating a baseline of normal activities and events; once enough baseline data has been accumulated to determine normal activity, it can detect abnormal activity (that may indicate a malicious intrusion or event)
    - Behavior-based IDSs use the baseline, profiles, activity statistics, and heuristic evaluation techniques to compare current activity against previous activity to detect potentially malicious events
    - ✈️**Static code scanning techniques**: the scanner scans code in files, similar to white box testing
    - ✈️**Dynamic techniques**: the scanner runs executable files in a sandbox to observe their behavior.

[7.3](#7.3) Perform Configuration Management (CM) (e.g. provisioning, baselining, automation) (OSG-9 Chpt 16)
- 📁**Configuration Management (CM)**: collection of activities focused on establishing and maintaining the integrity of IT products and info systems, via the control of processes for initializing, changing, and monitoring the configurations of those products/systems through their lifecycle; the process of identifying, controlling, and verifying the configuration of systems and components throughout their lifecycle
    - CM is an integral part of secure provisioning and relates to the proper configuration of a device at the time of deployment
    - CM helps ensure that systems are deployed in a secure, consistent state and that they stay in a secure, consistent state throughout their lifecycle
    - Configuration Management is used to ensure secure 📝baselines on systems are adequately maintained, and any deviations are authorized and documented. Configuration management seeks to establish safe, reliable configurations for systems.
- 🚡**Provisioning**: taking a particular config baseline, making additional or modified copies, and placing those copies into the environment in which they belong; refers to installing and configuring the operating system and needed apps on new systems
    - new systems should be configured to reduce vulnerabilities introduced via default configurations; the key is to harden a system based on intended useage
- 🚡**System Hardening**: process of applying security configurations, and locking down various hardware, communications systems, software (e.g. OS, web/app server, apps etc); normally performed based on industry guidelines and benchmarks like the Center for Internet Security ⚖️(CIS); The CIS benchmarks provide a useful security standard and baseline to assess systems against or to configure them to. Organizations can adapt and modify the baseline to meet their specific needs while speeding up deployment by using an accepted industry standard. 
    - makes it more secure than the default configuration and includes the following:
        - 🎈disable all unused services
        - 🎈close all unused logical ports
        - 🎈remove all unused apps
        - 🎈change default passwords
    - The goal of system hardening is to reduce the attack surface available for an attacker to try to access.
    - Once a system is hardened, that state should be the baseline. Once a secure baseline has been established on a particular system, administrators can capture the system's state, called an 📝"image", and deploy it to other systems. This process is known as imaging.
- 🚡**Baseline**: in the context of configuration management, it is the starting point or starting config for a system
    - an easy way to think of a baseline is as a list of services; an OS baseline identifies all the settings to harden specific systems
    - many organizations use images to deploy baselines; baseline images improve the security of systems by ensuring that desired security settings are always configured correctly
    - baseline images improve the security of systems by ensuring that desired security settings are always configured correctly; they also reduce the amount of time required to deploy and maintain systems, reducing overall maintenance costs
    - Images are an effective provisioning method that ensures systems receive an initial, known baseline configuration. 
- Automation: it's typical to create a baseline, and then use automated methods to add additional apps, features, or settings for specific groups of computers
    - note that admins can use create/modify group policy settings to create domain-level standardization or to make security-related Windows registry changes

[7.4](#7.4) Apply foundational security operations concepts (OSG-9 Chpt 16)
- Security operations encompasses the day-to-day tasks, practices, and processes involved in securing and maintaining the operational integrity of an organization's information systems and assets; it includes security monitoring, incident response, and security awareness and training
- The primary purpose of security operations practices is to safeguard assets such as information, systems, devices, facilities, and apps, and helping organizations to detect, prevent, and respond to security threats
- Implementing common security operations concepts, along with performing periodic security audits and reviews, demonstrates a level of due care and due diligence. Note📝 Security audits and reviews help ensure that an organization is following its policies but wouldn’t directly check systems for vulnerabilities.

- 7.4.1 🔴Need-to-know/least privilege: Least privilege is based upon limiting access and ability as determined by a subject's security clearance or permission group, while need-to-know is more 📝selective and determined by a task that needs to be completed by a specific subject, even within a security clearance level. For example, a user with a secret clearance may be granted access to all general military intelligence. Certain users within that security clearance may be given specific tasks and, therefore, provided with information related to their specific task that is inaccessible to others who were not assigned that same task.
    - 💥**Need-to-know principle**: imposes the requirement to grant users 📝access only to data or resources they need to perform assigned work tasks. The need-to-know policy operates on the basis that any given system user should be granted access only to portions of sensitive information or materials necessary to perform some task.
        - The Need-To-Know (NTK) principle is used to determine if a user’s access to certain information is necessary to perform their job role sufficiently. If a user does not need read access to data to carry out their job role, they should not be granted access. The need-to-know principle differs from least privilege because the need-to-know principle is only concerned with read access.
        - need to know is about the actual data 
    - 💥**Least privilege principle**: states that subjects are granted only the privileges necessary to perform assigned work tasks and no more.  The principle of least privilege ensures that personnel are granted only the 📝permissions they need to perform their job and no more.
        - privilege in this context includes both permissions to data and rights to perform systems tasks
        - limiting and controlling privileges based on this concept protects confidentiality and data integrity
        - principle relies on the assumption that all users have a well-defined job description that personnel understand
        - least privilege is typically focused on ensuring that user privileges are restricted, but it also applies to apps or processes (e.g. if an app or service is compromised, the attacker can assume the service account’s privileges)
        - New user accounts should have no access permissions by default and then give each user the necessary permissions to perform their job responsibilities.
        -  The principle of least privilege includes both rights and permissions
        -  Least privilege is about permissions
        -  If a user has an NTK then the question is what level of permission do they need. Do they need read, write, full control, or another level? The best idea is to give them as little permission as possible, just enough to do their job. This is called the principle of least privilege. 
- 7.4.2 Separation of Duties (SoD) and responsibilities
    - 💥**Separation of Duties (SoD)**: ensures that no single person has total control over a critical function or system. Separation of duties allows for two or more people to 📝play separate 📝roles in the completion of a critical process. 
        - SoD policies help reduce fraud by requiring collusion between two or more people to perform unauthorized activity
        - A separation of duties (SoD) policy prevents a single person from controlling all elements of a process. When applied to security settings, it can prevent a person from making major security 📝changes without assistance.
        - It adds additional oversight by involving multiple individuals.
        - example of how SoD can be enforced, is by dividing the security or admin capabilities and functions among multiple trusted individuals
        - example Separation of duties applied to a situation where the same person may not have both the ability to initiate a request and the ability to approve a request. 
    - 💥**Two-person control**: (AKA two-man rule) requires the approval of two individuals for critical tasks. Dual control is a security principle that requires two or more individuals to 📝work together to perform critical tasks, such as authorizing sensitive transactions or accessing high-security areas. Ensuring multiple people are involved reduces the risk of unauthorized activities, fraud, or errors, providing an extra layer of protection and accountability in security-sensitive operations.
        - using two-person controls within an org ensures peer review and reduces the likelihood of collusion and fraud
        - a process that requires the concurrence of two people to perform a sensitive action
        - the principle of two-person control by requiring simultaneous action by two separate authorized individuals to gain access is designed for highly sensitive operations
        - ex: privilege access management (PAM) solutions that create special admin accounts for emergency use only; perhaps a password is split in half so that two people need to enter the password to log on
    - 💥**Split knowledge**: combines the concepts of separation of duties and two-person control into a single solution; the info or privilege required to perform an operation is divided among two or more users, ensuring that no single person has sufficient privileges to compromise the security of the environment
    - Principles such as least privilege and separation of duties help prevent security policy violations, and monitoring helps to deter and detect any violations that occur despite the use of preventive controls. Before granting access, you should verify that the user has a valid security clearance and a business need to know the information. 
    - Segregation of duties matrix: is used to ensure that one person does not obtain two privileges that would create a potential conflict.
    - 💥**Entitlement** refers to the privileges granted to new users when an account is first provisioned. Entitlement refers to the level of privileges granted to a user. If a user has been granted access to an object, they are entitled to it.
- 7.4.3 Privilege account management
    - **Privileged Account Management (PAM)**: solutions that restrict access to privileged accounts or detect when accounts use any elevated privileges (e.g. admin accounts)
        - Microsoft domains, this includes local admin accounts, Domain and Enterprise Admins groups
        - Microsoft domains include a privileged account management solution that grants administrators elevated privileges when they need them but restrict the access using a time-limited ticket.
        - Linux includes root or sudo accounts
    - PAM solutions should monitor actions taken by privileged accounts, new user accounts, new routes to a router table, altering config of a firewall, accessing system log and audit files, backup restore, purging log entries etc
    - Privileged account management ensures that personnel do not have more privileges than they need and do not misuse their privileges. It can identify whether users have excessive privileges violating the least privilege principle.
    - **Privileged Access Reviews**: Privileged access reviews are one of the most critical components of an organization’s security program because they ensure that only authorized users have access to perform the most sensitive operations. They should take place whenever a user with privileged access leaves the organization or changes roles as well as on a regular, recurring basis
        - A periodic account access review can discover when users have more privileges than they need and could be used to  discover privilege creep. 
    
- 7.4.4 Job rotation
    - 💥**Job rotation**: (AKA rotation of duties) means that employees rotate through jobs or rotate job responsibilities with other employees
        - using job rotation as a security control provides peer review, reduces fraud, and enables cross-training
        - prevemts employees from developing familiarity with security controls over time and finding ways to bypass them.
        - job rotation policy can act as both a deterrent and a detection mechanism
        - Job rotation and separation of duties policies help prevent fraud.
        - job rotation also Provides peer review, Reduces fraud, Enables cross-training
        - The purpose of job rotation is to act as a deterrent and a detection tool. If one knows that someone will be taking over their job functions soon, they are less likely to participate in fraudulent activities. If someone does do something fraudulent, job rotation increases the likelihood it will be discovered through a peer review when someone rotates into that job and is cross-trained on responsibilities. A peer review is performed, effectively, when someone rotates into a job. It is necessary to figure out where the last person left off in order to get to work. This peer review could either detect fraud or, knowing it will happen, reduce the likelihood of it ever happening.
        - Job rotation helps ensure that multiple people can do the same job and can help prevent the organization from losing information when a single person leaves.
        -  Job rotation of duties is effective in preventing security breach where employee has found ways of bypassing intended safeguards., as it would help prevent from having sufficient time in thier role to discover and exploit weaknesses in the system.
- 7.4.5 Service Level Agreements (SLA)
    - 💢**Service Level Agreement (SLA)**: an agreement between an organization and an outside entity, such as a vendor, where the SLA stipulates performance expectations and often includes penalties if the vendor doesn’t meet these expectations. A service level agreement, or SLA, contains details about how the service will be provided, what level of outages or downtime is acceptable, and what remedies may exist in the case of outages or other issues. 
    - 💢**Memoradum of Understanding (MOU)**: documents the intention of two entities to work together toward a common goal
    - 💢**Operational Level Agreement (OLA)**: is between internal service organizations and does not involve customers.
    - 💢**Statement of Work (SOW)**: SOW is an addendum to a contract describing work to be performed.
    - 💢**Software Escrow Agreements** place a copy of the source code for a software package in the hands of an independent third party who will turn the code over to the customer if the vendor ceases business operations. Software escrow agreements place the application source code in the hands of an independent third party, thus providing firms with a “safety net” in the event a developer goes out of business or fails to honor the terms of a service agreement.
    - 💢**Resource capacity agreement** specifically addresses the availability of resources in a disaster scenario. This type of agreement ensures that the cloud provider has sufficient resources to meet the needs of their clients, even in the event of multiple simultaneous disasters. It directly tackles the issue of resource allocation and availability. Resource capacity agreements are used to ensure that appropriate resources will be available in a recovery scenario.
    - 💢**Mutual assistance agreement** typically involves agreements between organizations for support during emergencies. 

[7.5](#7.5) Apply resource protection (OSG-9 Chpt 16)
- Media management should consider all types of media as well as short- and long-term needs and evaluate:
    - Confidentiality
    - Access speeds
    - Portability
    - Durability
    - Media format
    - Data format
- For the test, data storage media should include any of the following:
    - Paper
    - Microforms (microfilm and microfiche)
    - Magnetic (HD, disks, and tapes)
    - Flash memory (SSD and memory cards)
    - Optical (CD and DVD)
 - Mean Time Between Failure (MTBF) is an important criterion when evaluating storage media, especially where valuable or sensitive information is concerned
 - Media management includes the protection of the media itself, which typically involves policies and procedures, access control mechanisms, labeling and marking, storage, transport, sanitization, use, and end-of-life
- 7.5.1 Media management
    - **Media management**: refers to the steps taken to protect media (i.e. anything that can hold data) and the data stored on that media; includes mostt portable devices (e.g. smart phones, memory/flash cards etc)
    - As above, OSG-9 also refers to tape media, as well as “hard-copy data”
- 7.5.2 Media protection techniques
    - If media includes sensitive info, it should be stored in a secure location with strict access controls to prevent loss due to unauthorized access
        - any location used to store media should have temperature and humidity controls to prevent losses due to corruption
    - Media management can also include technical controls to restrict device access from computer systems
    - When media is marked, handled, and stored properly, it helps prevent unauthorized disclosure (loss of confidentiality), unauthorized modification (loss of integrity), and unauthorized destruction (loss of availability)

[7.6](#7.6) Conduct incident management (OSG-9 Chpt 17)
- 🔴**Incident response**: the mitigation of violations of security policies and recommended practices; the process to detect and respond to incidents and to reduce the impact when incidents occur; it attempts to keep a business operating or restore operations as quickly as possible in the wake of an incident
- Incident management is usually conducted by an Incident Response Team (IRT), which comprises individuals with the required expertise and experience to manage security incidents; the IRT is accountable for implementing the incident response plan, which is a written record that defines the processes to be followed during each stage of the incident response cycle

- The main goals of incident response:
    - Provide an effective and efficient response to reduce impact to the organization
    - Maintain or restore business continuity
    - Defend against future attacks

- An important distinction needs to be made to know when an incident response process should be initiated: events take place continually, and the vast majority are insignificant; however, events that lead to some type of adversity can be deemed incidents, and those incidents should trigger an org's incident response process steps: 🟡PACERL
    - 💢**Preparation**: includes developing the IR process, assigning IR team members, and everything related to what happens when an incident is identified; preparation is critical, and will anticipate the steps to follow
    - 💢**Analysis:** Gathering and analyzing information about the incident to determine its scope, impact, and root cause (e.g., by interviewing witnesses, collecting and analyzing evidence, and reviewing system logs)
    - 💢**Containment:** Limiting the impact of the incident and preventing further damage (e.g., by isolating affected systems, changing passwords, and implementing security controls)
    - 💢**Eradication:** Removing the cause of the incident from the environment (e.g., by removing malware, patching vulnerabilities, and disabling compromised accounts)
    - 💢**Recovery:** Restoring systems and data to their normal state (e.g., by restoring from backups, rebuilding systems, and re-enabling compromised accounts)
    - 💢**Lessons Learned:** Documenting the incident and learning from it to improve future responses (e.g., by identifying areas where the incident response process can be improved and by sharing lessons learned with other organizations). Advisable to recruit an independent moderator or external consultant to facilitate the session.

- The following steps (🔥Detection, 🔥Response, 🔥Mitigation, 🔥Reporting, 🔥Recovery, 🔥Remediation, and 🔥Lessons Learned) are on the exam 🟡DRMRRRL 
- 7.6.1 Detection
    - ❄️**Detection:** the 📝identification of potential security incidents via monitoring and analyzing security logs, threat intelligence, or incident reports; as above, understanding the distinction between an event and an incident, the goal of detection is to identify an adverse event (an incident) and begin dealing with it
    - Common methods to detect incidents:
        - intrusion detection and prevention systems
        - antimalware
        - automated tools that scan audit logs looking for predefined events
        - end users sometimes detect irregular activity and contact support
    - Note: receiving an alert or complaint doesn’t always mean an incident has occurred
- 7.6.2 ❄️**Response**
    - After detecting and verifying an incident, the next step is 📝activate an Incident Response (IR) or CSIRT team
    - CSIRT representation normally includes at least representatives of senior management, information security professionals, legal representatives, public affairs staff, and engineering/technical staff.
    - An IR team is AKA computer incident response team (CIRT) or computer security incident response team (CSIRT)
    - Among the first steps taken by the IR Team will be an impact assessment to determine the scale of the incident, how long the impact might be experienced, who else might need to be involved etc.
    - The IR team typicall investigate the incident, assess the damage, collect evidence, report the incident, perform recovery procedures, and participate in the remediation and lessons learned stages, helping with root cause analysis
    - its important to protect all data as evidence during an investigation, and computers should not be turned off
    - The response phase includes steps taken to assemble a team and triage the incident.

- 7.6.3 Mitigation
    - ❄️**Migitation**: attempt to contain an incident; in addition to conducting an impact assessment, the IR Team will attempt to minimize or contain the damage or impact from the incident
    - The IR Team's job at this point is not to fix the problem; it's simply to try and prevent further damage
    - responders attempt to contain the incident in the mitigation step. 
    - Note this may involve disconnecting a computer from the network; sometimes responders take steps to mitigate the incident, but without letting the attacker know that the attack has been detected
    - The mitigation phase of incident response focuses on actions that can contain the damage incurred during an incident. This includes 📝limiting the scope and or effectiveness of the incident.
    - Containing the compromise is part of the mitigation step of the incident response. the IoC must be contained before any further mitigations or recovery attempts are made.

- 7.6.4 ❄️**Reporting**
    - Reporting occurs throughout the incident response process
    - During the Reporting phase, incident responders assess their obligations under laws and regulations to report the incident to government agencies and other regulators.
    - Once an incident is mitigated, formal reporting occurs because numerous stakeholders often need to understand what has happened
    - Jurisdictions may have specific laws governing the protection of personally identifiable information (PII), and must report if it's been exposed
    - Additionally, some third-party standards, such as the Payment Card Industry Data Security Standard (PCI DSS), require orgs to report certain security incidents to law enforcement

- 7.6.5 ❄️**Recovery**
    - At this point, the goal is to start returning to normal
    - Recovery is the next step, 📝returning a system to a fully functioning state
    - The most secure method of restoring a system after an incident is completely rebuilding the system from scratch, including restoring all data from the most recent backup
    - Initiating a BCP is always part of a recovery activity. Recovery controls bring non-functional equipment/servers/data centers/etc. back to a normal state. 
        - effective configuration and change management will provide the necessary documentation to ensure the rebuilt systems are configured properly
    - According to the OGS, you should check these areas as part of recovery:
        - access control lists (ACLs), including firewall or router rules
        - services and protocols, ensuring the unneeded services and protocols are disabled or removed
        - patches
        - user accounts, ensuring they have changed from default configs
        - known compromises have been reversed

- 7.6.6 Remediation
    - ❄️**Remdiation**: changes to a system's config to immediately limit or reduce the change of reoccurance of an incident;
    - Remediation stage: personnel look at the incident, identify what allowed it to occur, and then implement methods to 📝prevent it from happening again
        - Remediation includes performing a 📝root cause analysis (which examines the incident to determine what allowed it to happen), and if the root cause analysis identifies a vulnerability that can be mitigated, this stage will recommend a change
        - The root-cause analysis examines the incident to determine what allowed it to happen and provides critical information for repairing systems so that the incident does not recur. This is a component of the remediation step of the incident response process because the root-cause analysis output is necessary to fully remediate affected systems and processes.

- 7.6.7 ❄️**Lessons Learned**
    - Lessons learned stage: an all-encompassing view of the situation related to an incident, where personnel, including the IR team and other key stakeholders, examine the incident and the response to see if there are any lessons to be learned
        - the output of this stage can be fed back to the detection stage of incident management
    - It's common for the IR team to create a report when they complete a lessons learned review
        - based on the findings, the team may recommend changes to procedures, the addition of security controls, or even changes to policies
        - management will decide what recommendations to implement and is responsible for the remaining risk for any recommendations they reject
        - The remediation and lessons learned stages include root cause analysis to determine the cause and recommend solutions to prevent a reoccurrence
        - performs a retrospective to identify potential inefficiencies or errors in their response that could be corrected for future incidents.
- NOTE📝 Incident management DOES NOT include a counterattack against the attacker

- Incident Response Summary:

    | Step | Stage | Action/Goal |  
    |--------|---------------| -----------|
    | Preparation|||
    | Detection  | Triage||
    | Response | Triage| activate IR team |
    | Mitigation  | Investigate| containment|
    | Reporting  | Investigate||
    | Recovery | Recovery | return to normal|
    | Remediation  | Recovery| prevention|
    | Lessons Learned | Recovery| improve process|


[7.7](#7.7) Operate and maintain detective and preventative measures (OSG-9 Chpts 11,17)

- As noted in [Domain 1](https://github.com/jefferywmoore/CISSP-Study-Resources/blob/main/CISSP-Domain-1-Objectives.md#1.10), a preventive or preventative control is deployed to thwart or stop unwanted or unauthorized activity from occurring
    - Examples:
        - fences
        - locks
        - biometrics
        - separation of duties policies
        - job rotation policies
        - data classification
        - access control methods
        - encryption
        - smart cards
        - callback procedures
        - security policies
        - security awareness training
        - antivirus software
        - firewalls
        - intrusion prevention systems
- 📁**A detective control** is deployed to discover or detect unwanted or unauthorized activity; detective controls operate after the fact
    - Examples:
        - security guards
        - motion detectors
        - recording and reviewing of events captured by security cameras
        - job rotation policies
        - mandatory vacation policies (At least 1 week)
        - 🔮audit trails (Audit trails provide documentation on what happened, when it happened, and who did it. IT personnel create audit trails by examining logs.)
        - A SIEM system is a detective control. It is centralized application that monitors multiple systems and can detect attacks after they occur, not prevent them.
        - honeypots or honeynets (A honeypot is a system that typically has pseudo flaws and fake data to lure intruders. A honeynet is two or more honeypots in a network.)
        - Darknet: A monitored network without any hosts. It is a segment of unused network address space that should have no network activity and, therefore, may be easily used to monitor for illicit activity.
        - intrusion detection systems
        - violation reports
        - supervision and reviews of users
        - incident investigations
- 📁**A preventive control** is  designed to stop or prevent security incidents or breaches from occurring. Some preventative measures:
    - Keep systems and applications up to date
    - Remove or disable unneeded services and protocols
    - Use intrusion detection and prevention systems
    - Use up-to-date antimalware software
    - Use firewalls
    - Implement configuration and system management processes
- 📁**Corrective control** Security Orchestration, Automation, and Response (SOAR) technologies are a combination of detective and corrective controls. They can detect attacks and apply automated actions to mitigate the attacks.
    - Corrective controls are designed to take a non-functional state back to functional. For example, a Disaster Recovery (DR) site. They allow a company to function after a disaster such as a fire in the data center.
- 📁**Compensatory control**  A compensating control is used to compensate for the anticipated failure of another control. It is with compensating controls that defense in depth is created.

- 🔴**NIST SP 800-137**: organizations should use the following factors to ­determine assessment and monitoring frequency:
    - security control volatility: refers to how frequently a security control might need to be changed or updated over time
    - system ­categorizations/impact levels
    - security controls or specific assessment objects providing ­critical functions
    - security controls with identified weaknesses
    - organizational risk tolerance
    - threat information
    - vulnerability information
    - risk assessment results
    - the output of ­monitoring strategy reviews
    - reporting requirements.

- 🔴**Software-Defined Security (SDS)**: Software-defined security (SDS) is an increasingly common approach to security that involves using software solutions and policies to secure environments, rather than traditional hardware-based approaches. This strategy allows for flexible and dynamic security configurations, particularly suited to the cloud's scalable nature.
    - Policy engines contribute to decision-making in SDS and other frameworks by enforcing security policies.
        - ⚒️Centralized Security Control:   SDS allows security policies to be managed centrally through software, often using a security management platform or controller. This centralization makes it easier to apply consistent security policies across diverse environments, such as data centers, cloud services, and on-premises infrastructure.
        - ⚒️Dynamic and Scalable Security: Unlike traditional security measures that rely on static configurations, SDS can dynamically adjust security policies based on real-time data, changes in the environment, or specific conditions. This flexibility is particularly important in environments that require rapid scaling or frequent changes, such as cloud computing.
        - ⚒️Integration with Software-Defined Networks (SDN): SDS is often closely integrated with Software-Defined Networking (SDN), where network control is decoupled from the hardware and managed by software. SDS can leverage the SDN infrastructure to apply security policies at the network layer, making it possible to enforce security at the point where the network is defined by software.
        - ⚒️Automation and Orchestration: SDS enables automation of security tasks, such as deploying firewalls, configuring security rules, or responding to threats. This automation can help reduce human error, increase efficiency, and improve the overall security posture by ensuring that security policies are consistently applied and updated as needed.
        - ⚒️Policy-Driven Approach: SDS uses a policy-driven approach, where security policies are defined in software and can be automatically enforced across the entire IT environment. Policies can be based on a variety of factors, including user roles, data sensitivity, or network conditions, ensuring that security is tailored to the specific needs of the organization.
        - ⚒️Agility and Adaptability: SDS provides the ability to quickly adapt to new threats or changes in the environment. For instance, if a new vulnerability is discovered, SDS can quickly propagate updates or new rules across the network without needing to manually reconfigure hardware-based devices.
        - ⚒️Enhanced Visibility and Monitoring: SDS typically includes comprehensive monitoring and reporting tools, giving security teams better visibility into the security posture of the entire network. This visibility helps in detecting and responding to security incidents more effectively.
    
- 7.7.1 🔴Firewalls (e.g. next generation, web application, network)
    - Firewalls are preventive and technical controls: Most appliance (i.e., hardware) firewalls offer extensive logging, auditing, and monitoring capabilities as well as alarms/alerts and even basic IDS functions. It is also true that firewalls are unable to prevent internal attacks that do not cross the firewall. Firewalls are unable to block new phishing scams, unless phishing scam's URL is already on a block list.
    - Types of firewalls:
        - 📗TCP Wrapper is an application that can serve as a basic firewall by restricting access based on user IDs or system IDs
        - 📗Packet Filter: Static packet filtering firewalls are known as first-generation firewalls and do not track connection state. Static packet-filtering firewalls filter traffic by examining data from a message header. Usually, the rules are concerned with source and destination IP address (layer 3) and port numbers (layer 4).
            - Static packet-filtering firewalls are stateless and do not consider context of the communications.  
        - 📗Application gateway firewall: filters traffic based on specific application requirements
        - 📗Circuit-level gateway firewall: designed to provide connection security to internal and external computers in a network's 📝session layer, they filter traffic based on the communications circuit; they do not engage in packet filtering based on packet contents. Circuit-level firewalls are able to make permit and deny decisions in regard to circuit establishment either based on simple rules for IP and port, using captive portals, requiring port authentication via 802.1X, or more complex elements such as context- or attribute-based access control.
        - 📗Third-generation firewalls: (AKA stateful inspection firewalls and 📝dynamic packet filtering firewalls) filter traffic based on its state within a stream of traffic. Stateful protocol analysis uses blacklists, whitelists, thresholds, and program code viewing to provide various security capabilities. Stateful inspection firewalls (aka dynamic packet-filtering firewall) enable the real-time modification of the filtering rules based on traffic content and context.
            -  Stateful inspection firewalls make access control decisions based on the 📝content and 📝context of communications, but are not typically limited to a single application-layer protocol.
            -  Stateful inspection firewalls are able to grant a broader range of access for authorized users and activities and actively watch for and block unauthorized users and activities.
        - 📗App firewalls: control traffic going to or from a specific app or service
            - An application-level firewall is able to make access control decisions based on the content of communications as well as the parameters of the associated 📝protocol and software.
            - Application-level firewalls operate at the Application layer (layer 7) and therefore do not evaluate source and destination address; however, they may perform stateful inspection of the packet payload specific to the Application-layer protocol it is focused on (such as HTTP for a WAF)
            - e.g. a web application firewall (WAF) inspects traffic going to a web server and can block malicious traffic such as SQL injection attacks and cross-site scripting (XSS) attacks
        - 📗Next-generation firewall (NGFW): functions as a unified threat management (UTM) device and combines several capabilities, including traditional functions such as packet filtering and stateful inspection, and can also perform packet inspection allowing identification and blocking of malicious traffic

- 7.7.2 Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS): Network-­based intrusion detection systems (NIDSs) and intrusion protection systems (IPSs) primarily monitor ✏️incoming traffic for threats.
    - Intrusion Detection Systems (IDSs) and Intrusion Prevention Systems (IPSs) are two methods organizations typically implement to detect and prevent attacks
    - ❄️**Intrusion detection**: a specific form of monitoring events, usually in real time, to detect abnormal activity indicating a potential incident or intrusion
        - Intrusion Detection System (IDS) automates the inspection of logs and real-time system events to detect intrusion attempts and system failures
        - IDSs are an effective method of detecting many DoS and DDoS attacks
        - an IDS actively watches for suspicious activity by monitoring network traffic and inspecting logs
        - 📝An IDS is most likely to connect to a switch port configured as a mirrored port. 
        - an IDS is intended as part of a defense-in-depth security plan
        - Active IDS, in addition to logging, can actively by changing the environmentidentifies the response after a threat is detected. Note📝 Active IDS is not the same as IPS becuase it is not deployed inline.
        - Passive IDS respond passively by logging and sending notifications.
        - A network-­based IDS can be both signature based and anomaly based.
        - **🔥knowledge-based detection**: AKA 🎆signature-based or 🎆pattern-matching detection, the most common method used by an IDS. It uses known signatures to detect attacks. It can only detect attacks from known threats.
        - **🔥behavior-based detection**: AKA 🎆statistical intrusion, 🎆anomaly-based, and 🎆heuristics-based detection; behavior-based IDSs use baseline, activity stats, and heuristic eval techniques to compare current activity against previous activity to detect potentially malicious events. It requires a baseline, and it then monitors traffic for any anomalies or changes when compared to the baseline.  The baseline can be outdated if the network is modified, so it must be updated when the environment changes. It can detect new security threats.
    - ❄️**Intrusion prevention**: An IPS includes detection capabilities, you’ll see them referred to as intrusion detection and prevention systems (IDPSs)
        - an IPS includes all the capabilities of an IDS but can also take additional steps to stop or prevent intrusions
    - IDS/IPS should be deployed at strategic network locations to monitor traffic, such as at the perimeters, or between network segments, and should be configured to alert for specific types of scans and traffic patterns
    - 📝IPS is usually placed in line with the network traffic, so it is placed before the switch.
    - See NIST SP 800-94

- 7.7.3 🔴**Whitelisting/blacklisting**
    - Method used to control which applications run and which applications can’t is allow list and deny list (AKA whitelists and blacklists)
    - 🎱**Allow list (Whitelist)**:: identifies a list of apps authorized to run on a system and blocks all other apps. A whitelist of allowed applications will ensure that users can run only the applications that you preapproves.
    - 🎱**Deny list (Blacklist)**: identifies a list of apps that are not authorized to run on a system. The blacklisting approach to application control allows users to install any software they wish except for packages specifically identified by the administrator as prohibited. This would be an appropriate approach in a scenario where users should be able to install any non malicious software they wish to use. Blacklists would require you to maintain a list of every application that you don't want to allow, which is an almost impossible task.
    - Allow and deny lists use for applications help to prevent malware infections
    - Important to note: a system would only use one list, either allow or deny
    - Apple iOS running on iPhones/iPads is an example of an extreme version of an allow list; users are only able to install apps from the App Store
- 7.7.4 Third-party provided security services
    - Some orgs outsource security services such as auditing and penetration testing to third party security services
    - Some outside compliance entities (e.g. PCI DSS) require orgs to ensure that service providers comply
    - OSG also mentions that some SaaS vendors provide security services via the cloud (e.g. next-gen firewalls, UTM devices, and email gateways for spam and malware filtering)
- 7.7.5 Sandboxing
    - 💥**Sandboxing**: refers to a security technique where a separate, secure environment is created to run and analyze untested or untrusted programs or code without risking harm to the host device or network; this isolated environment, known as a sandbox, effectively contains the execution of the code, allowing it to run and behave as if it were in a normal computing environment, but without the ability to affect the host system or access critical resources and data 
    - Sandboxing provides a security boundary for applications and prevents the app from interacting with other apps; can be used as part of development, integration, or acceptance testing, as part of malware screening, or as part of a honeynet
    - Using a sandbox is an example of confinement, where the system restricts the access of a particular process to limit its ability to affect other processes running on the same system.
    - Sandboxing is commonly utilized by next-generation antimalware solutions to evaluate suspect executables in a secure, isolated environment.
    - Sandboxed environments are often enabled through virtualization technology, which prevents software executed within the sandboxed environment from interacting with the environment outside of the sandbox, except in controlled ways.
    - Sandboxing executes potential malware in an isolated environment that includes monitoring capabilities. This helps to identify potential malware before it is delivered to its intended destination.
    - Sandboxing is also used in mobile devices to isolate apps from one another.
- 7.7.6 Honeypots/honeynets
    - 🍏**Honeypots**: individual computers created as a trap or a decoy for intruders or insider threats. A pseudoflaw is a false vulnerability in a system that may distract an attacker. These are often used in honeypot system to consume an attacker's time while alerting administrators. Administrators of honeypots need to be careful not to solicit attackers into attacking a honeypot, as this is considered entrapment and is illegal. They should not be made too weak as more knowledgeable attackers will figure out that it is a honeypot much quicker and move on, possibly to critical business systems.
    - 🍏**Honeynet**: two or more networked honeypots used together to simulate a network
    - They look and act like legit systems, but they do not host data of any real value for an attacker; admins often configure honeypots with vulnerabilities to tempt intruders into attacking them
    - In addition to keeping the attacker away from a production environment, the honeypot allows administrators to observe an attacker’s activity without compromising the live environment
    - 🍏**pseudoflaw** is a false vulnerability in a system that may distract an attacker. This can also be added to honeypots
- 7.7.7 Anti-malware
    - 💥**Malware**: program inserted into a system with the intent of compromising the CIA of the victim's data, applications, or  OS; malicious software that negatively impacts a system
    - The most important protection against malicious code is the use of antimalware software with up-to-date signature files and heuristic capabilities
        - multi-pronged approach with antimalware software on each system in addition to filtering internet content helps protect systems from infections
        - following the principle of least privilege, ensuring users do not have admin permissions on systems won’t be able to install apps that may be malicious
    
    - These are the characteristics of each malware type:
        - 🔥virus: software written with the intent/capability to copy and disperse itself without direct owner knowledge/cooperation; the defining characteristic is that it's a piece of malware that has to be triggered in some way by the user; program that modifies other programs to contain a possibly altered version of itself. It is a type of malware that delivers its payload after a user launches an application.
        - 🔥worm: software written with the intent/capability to copy and disperse without owner knowledge/cooperation, but without needing to modify other programs to contain compies of itself; malware that can self-propagate and spread through a network or a series of systems on its own by exploiting a vulnerability in those systems
        - 🔥companion: helper software -- it's not malicious on its own; it could be something like a wrapper that accompanies the actual malware
        - 🔥macro: associated with Microsoft Office products, and is created using a straightforward programming language to automate tasks; macros can be programmed to be malicious and harmful
        - 🔥multipartite: means the malware spreads in different ways; (e.g. Stuxnet)
        - 🔥polymorphic: malware that can change aspects of itself as it replicates to evade detection (e.g. file name, file size, code structure etc)
        - 🔥trojan: A Trojan horse is malware that looks harmless or desirable but contains malicious code; trojans are often found in easily downloadable software; a trojan inserts backdoors or trapdoors into other programs or systems. A Trojan horse is a form of malware that uses social engineering tactics to trick a victim into installing it—the trick is to make the victim believe that the only thing they have downloaded or obtained is the host file, when in fact it has a malicious hidden payload. 
        - 🔥bot: an emerging class of mobile code; employing limited machine learning capabilities to assist with user requests for help or assistance, automation of or assistance with workflows, data input quality validation etc. Also, computer that is part of a botnet is called a zombie or bot
        - 🔥botnet: many infected systems that have been harnessed together and act in unison. Botnets are used for a wide variety of malicious purposes, including scanning the network for vulnerable systems, conducting brute-force attacks against other systems, mining cryptocurrency, and sending out spam messages.
        - 🔥bot herder: A botnet is a collection of compromised computing devices (often called bots or zombies) organized in a network controlled by a criminal known as a bot herder. Bot herders use a command-­and-­control server to remotely control the zombies and often use the botnet to launch attacks on other systems or send spam or phishing emails. Bot herders also rent botnet access out to other criminals. Data transmissions between the bot-herder and the individual zombies are called 📝Command and Control (C&C) traffic. 
        - 🔥boot sector infectors: pieces of malware that can install themselves in the boot sector of a drive
        - 🔥hoaxes/pranks: not actually software, they're usually part of social engineering—via email or other means—that intends harm (hoaxes) or a joke (pranks). A hoax is a social engineering attack that is attempting to trick a user into taking actions that will harm them through the use of fear that not taking action would actually cause harm. If you contact the claimed sender and they confirm they did not send the message, then it could be a hoax or at least a spoofed message. Hoaxes often use the threat of damage or harm to encourage the victim to take action, and those actions are often provided steps that will actually cause the victim harm
        - 🔥logic bomb: malware inserted into a program which will activate and perform functions suiting the attacker at some later date/conditions are met; code that will execute based on some triggering event
        - 🔥stealth: malware that uses various active techniques to avoid detection
        - 🔥ransome attack: any form of attack which threatens the destruction, denial or unauthorized public release/remarketing of private infomation assets; usually involves encrypting assets and withholding the decryption key until a ransom is paid
        - 🔥ransomware: type of malware that typically encrypts a system or a network of systems, effectively locking users out, and then demands a ransom payment (usually in the form of a digital currency) to gain access to the decryption key
        - 🔥rootkit: Similar to stealth malware, a rootkit attempts to mask its presence on a system; typically includes a collection of malware tools that an attacker can utilize according to specific goals
        - 🔥zero-day: is any type of malware that's never been seen in the wild before, and the vendor of the impacted product is unaware (or hasn't issued a patch), as are security companies that create anti-malware software intended to protect systems; previously unreported vuln which can be potentially exploited without risk of detection or prevention until system owner/developer detects and corrects vuln; gets name from the "zero time" being the time at which the exploit or vuln is first identified by the systems' owners or builders; AKA zero-hour exploit, zero-day attack. A zero-­day exploit is an attack that exploits a vulnerability that doesn’t have a patch or fix. In many cases when an Zero-day exploit is initially reported, there are no prebuilt signatures or detections for vulnerability scanners, and the CVE database may not immediately have information about the attack. The best option is to quickly gather information and review potentially vulnerable servers based on current configuration.  📝Advanced persistent threat (APT) are most closely associated with zero-day attacks due to the cost and complexity of the research required to discover or purchase them. 
        - 🔥covert channel is a method used to transmit information in a way that is not intended or designed for communication, often bypassing normal security controls. Covert channels exploit unintended or hidden pathways in a system to communicate or transfer data in ways that are difficult to detect. These channels can be used to exfiltrate data, bypass security measures, or execute unauthorized commands. Covert channels use surreptitious communications' paths. Covert timing channels alter the use of a resource in a measurable fashion to exfiltrate information. If a user types using a specific rhythm of Morse code, this is an example of a covert timing channel. Someone watching or listening to the keystrokes could receive a secret message with no trace of the message left in logs. There are two types of covert channels:
            - 🔨Storage: exchange of information by writing data to a common storage medium that another process can read. Covert storage channels work by writing data to storage, allowing unauthorized subjects to read it.
            - 🔨Timing: exchange of information through the use of timing. An example of this could be that a shared resource is used or not used. Covert timing channels work by encoding messages by utilizing or predictably altering the performance of system resources. A timing channel could also allow a bad actor to leak information to someone by the time that something occurs.
        - 🔥Spyware is software designed to gather information about a user or system without their consent.
        - 🔥Adware delivers advertisements.
        - 🔥Advanced Persistent Threat (APT) is a sophisticated and prolonged cyberattack where an adversary gains unauthorized access to a network and remains undetected for an extended period. APTs are characterized by their use of advanced techniques and tools. Attackers employ sophisticated methods to bypass security measures and remain undetected. APTs are designed to stay in the target environment for a long time, often months or even years. The attackers aim to maintain a long-term presence to achieve their objectives. APTs are typically targeted at specific organizations, sectors, or individuals. The attackers often conduct extensive reconnaissance to tailor their attack to the specific vulnerabilities of their target.
        - An Advanced Persistent Threat (APT) is usually carried out by a group of attackers who work together to target a specific organization. When their skill sets are put together they are very skilled. APTs are skilled and exercise patience to reduce the likelihood of being discovered and increase the possibility of a successful attack. Attacks carried out by an APT are generally not random, and the attackers have a motive to attack a specific organization. They are often even state-sponsored. 
- 🍮**Saboage**: An attack committed against an organization by an insider, such as an employee, is known as sabotage. Sabotage refers to deliberate actions intended to disrupt, damage, destroy, or compromise an organization's operations, assets, infrastructure, or reputation.
- 🍮**Espionage**: This involve the theft of sensitive information and unauthorized access or collection of sensitive or confidential information for the purpose of gaining a competitive advantage, political advantage, or other nefarious motives.

- 7.7.8 Machine learning and Artificial Intelligence (AI) based tools
    - 📘**AI**: gives machines the ability to do things that a human can do better or allows a machine to perform tasks that we previously thought required human intelligence
    - 📘**Machine Learning**: a subset of AI and refers to a system that can improve automatically through experience
        - a ML system starts with a set of rules or guidelines
        - Machine learning uses 📝mathematical approaches to analyze data, searching for patterns that predict future activity. 
        - an AI system starts with nothing and progressively learns the rules, creating its own algorithms as it learns the rules and applies ML techniques based on these rules
            - 🛠️supervised learning: uses of labeled datasets.
            -  🛠️unsupervised learning: uses unlabeled datasets.  
    - Behavior-based detection is one way ML and AI can apply to cybersecurity
        - an admin relates a baseline of normal activities and traffic on a network; the baseline in this case is similar to a set of rules given to a ML system
        - during normal operations, it detects anomalies and reports them; if the detection is a **🔥false positive** (incorrectly classifying a benign activity, system state, or configuration as malicious or vulnerable), the ML system learns
    - An AI system starts without a baseline, monitors traffic and slowly creates its own baseline based on the traffic it observes
        - as it creates the baseline it also looks for anomalies
        - an AI system also relies on feedback from admins to learn if alarms are valid or false positives
    - 📘**Neural networks** attempt to use complex computational techniques to model the behavior of the human mind. Through the use of the Delta rule or Learning rule, neural networks are able to learn from experience.
    - 📘**Knowledge banks** are a component of expert systems, which are designed to capture and reapply human knowledge.
    - 📘**Decision support systems** are designed to provide advice to those carrying out standard procedures and are often driven by expert systems.
    - 📘**Expert Systems** uses details about the current situation, and the inference engine uses a combination of logical reasoning and fuzzy logic techniques to draw a conclusion based on past experience. Expert systems use a knowledge base consisting of a series of 🧠“if/then” statements to form decisions based on the previous experience of human experts. Expert systems contain business logic.


[7.8](#7.8) Implement and support patch and vulnerability management (OSG-9 Chpt 16)
- Patch and vulnerability management processes 📝work together to help protect an org against emerging threats; 📝patch management ensures that appropriate patches are applied, and 📝vuln management helps verify that systems are not vulnerable to known threats
- 📁**Patch Management**: systematic notification, identification, deployment, installation and verification of OS and app code revisions known as patches, hot fixes, and service packs  
    - an effective patch management program ensures that systems are kept up to date with current patches
    - An effective patch management program evaluates and tests patches before deploying them.
    - A patch management system ensures that systems have required patches. In addition to deploying patches, it would also check the systems to verify they accepted the patches.
    - 📝Vulnerability scanning can help discover systems that still require patching. Vulnerability scans should be performed periodically on production environments.
    - Some of the key steps in a patch management program include:
        - 🆎Evaluate Patches: Determine whether the organization needs to deploy the patch
        - 🆎Test Patches: Test patches on a non-production system to ensure that they do their job and don’t create other issues
        - 🆎Approve Patches: Approve tested patches for deployment in production, potentially via a change management process
        - 🆎Deploy Patches: Deploy patches to production systems, potentially using automation
        - 🆎Verify Patch Deployment: Test patches to ensure that they were applied correctly and fix the vulnerability
    - **Patch**: (AKA updates, quick or hot fixes) a blanket term for any type of code written to correct bug or vulnerability or to improve existing software performance; when installed, a patch directly modifies files or device settings without changing the version number or release details of the related software comonent
    - in the context of security, admins are primarily concerned with security patches, which are patches that affect a system’s vulns
    - When patches are introduced to fix a security weakness in the system, they help mitigate risk
    - **Patch Tuesday**: several big-tech orgs (e.g. Microsoft, Adobe, Oracle etc) regularly release patches on the second Tuesday of every month
    - A patch management process includes evaluating patches, testing patches, and auditing patches.
        - 🎤Evaluating patches determines what patches should be deployed.
        - 🎤Testing helps discover unintended problems before they are deployed.
        - 🎤Auditing ensures required patches have been deployed.
- There are three methods for determining patch levels:
    - 🚸Agent: update software (agent) installed on devices
    - 🚸Agentless: remotely connect to each device
    - 🚸Passive: monitor traffic to infer patch levels
- Deploying patches can be done manually or automatically
- Common steps within an effective program:
    - evaluate patches: determine if they apply to your systems
    - test patches: test patches on an isolated, non-production system to determine if the patch causes any unwanted side effects
    - approve the patches: after successful testing, patches are approved for deployment; it’s common to use 📝Change Management as part of the approval process
    - deploy the patches: after testing and approval, deploy the patches; many orgs use automated methods to deploy patches, via third-party or the software vendor
    - verify that patches are deployed: regularly test and audit systems to ensure they remain patched
- 📁**Vulnerability Management**: regularly identifying vulns, evaluating them, and taking steps to mitigate risks associated with them. Vulnerability Management are activities necessary to identify, assess, prioritize, and remediate information systems weaknesses
    - Vulnerability management is a systematic approach to identifying, evaluating, and mitigating security vulnerabilities in computer systems, networks, applications, and other digital assets.
    - The primary objective of vulnerability management is to 📝proactively reduce the risk of potential security breaches and data compromises by addressing weaknesses in an organization's IT infrastructure before they can be exploited by malicious actors. 
    - it isn’t possible to eliminate risks, and it isn’t possible to eliminate all vulnerabilities
    - a vuln managment program helps ensure that an org is regularly evaluating vulns and mitigating those that represent the greatest risk
    - one of the most common vulnerabilities within an org is an unpatched system, and so a vuln management program will often work in conjunction with a patch management program
    - Vulnerability scanners are used to check systems for known issues and are part of an overall vulnerability management program.
    - A 📝vulnerability scan will list or enumerate all security risks within a system.

[7.9](#7.9) Understand and participate in change management processes (OSG-9 Chpt 16)
- 📁**Configuration management**: Configuration Management is used to ensure secure 📝baselines on systems are adequately maintained, and any deviations are authorized and documented. Configuration management seeks to establish safe, reliable configurations for systems. Configuration management is typically used to manage the configurable system settings.
    - It is the management process for 📝baselines to ensure any deviations are authorized and documented.
    - Configuration management is used to ensure secure baselines on systems are adequately maintained, and any deviations are authorized and documented.
    - Configuration management seeks to establish safe, reliable configurations for systems.
    - Configuration management is part of ITIL (formerly Information Technology Infrastructure Library), not a government regulation. ITIL was invented by the United Kingdom (UK) government and it is not managed by Axelos in conjunction with the UK government.
- 📁**Change management**: formal process an org uses to transition from the current state to a future state; typically includes mechanisms to request, evaluate, approve, implement, verify, and learn the change; ensures that the costs and benefits of changes are analyzed and changes are made in a controlled manner to reduce risks
    - Change management is a structured and systematic approach used to manage the process of introducing modifications, updates, or alterations to an organization's systems, processes, technology, or infrastructure. The primary goal of change management is to minimize disruption, reduce risks, and ensure that changes are implemented smoothly and effectively while maximizing benefits and minimizing adverse impacts.
    - Change management requires documentation and signatures from authorized personnel to avoid rogue changes that could affect an organization's infrastructure. Any changes made to critical systems that could impact organizational performance require change management controls. The process greatly reduces the risk associated with updates and upgrades.
    - Security analysts should always 📝report findings to management and 📝gain approval to implement changes once the total risk to the business can be properly weighed. This is the process of change management and patch management.
    - Change management processes allow various IT experts to review proposed changes for unintended consequences before implementing
    - 💥**Request For Change (RFC)**: documentation of a proposed change in support of change management activities. Each change should be the result of a reviewed and approved request for change (RFC). These RFCs may be approved by the change advisory board (CAB). 
    - Change management aims to ensure that any change does not result in unintended outages or reduce security. 
    - 💢The request control process provides an organized framework within which users can request modifications, managers can conduct cost/benefit analyses, and developers can prioritize tasks.
    - 💢Change control provides an organized framework within which multiple developers can create and test solutions prior to rolling them out into a production environment.
    - 💢Release control includes acceptance testing to ensure that any alterations to end-user work tasks are understood and functional.
        - 🔨Release management (found within ITIL) is the process of managing, coordinating, controlling, and making new and changed services available. This includes software features being made available to users.
        - 🔨Deployment management is moving new hardware into production or new software to a live environment. 
    - 💢Configuration control ensures that changes to software versions are made in accordance with the change control and configuration management process. Updates can be made only from authorized distributions in accordance with those policies.
        - 🎈Versioning: refers to version control used in software configuration management. A labeling or numbering system is used to differentiate between different software sets and configurations across multiple machines or at different points in time on a single machine. Versioning also helps with rollback procedures when deployment fails.
    - Change management controls provide a process to control, document, track, and audit all system changes.
    - The typical steps for the change management process are as follows:
        - 🆎Request the change: The requester submits a request to alter something. This could be a firewall change, a software code change, etc.
        - 🆎Review the change: A committee has been established to evaluate the effect of this change on the business, the business unit(s), the policy, legal compliance, and so on. This is looking at the benefits and the disadvantages of making this change.
        - 🆎Approve/reject the change:  Based on the assessment in step two, the request is either accepted or denied.
        - 🆎Test the change: If the request is accepted the requesters can proceed to test this change in a test environment.
        - 🆎Schedule and implement the change: If the change operated as expected in the test environment, it can be scheduled for deployment to the production environment. Then the deployment can proceed as scheduled.
        - 🆎Document the change: Formal documentation needs to be created that documents all that occurred in the steps above.
    - The change management process includes multiple steps that build upon each other:
        - 🔐Change request: a change request can come from any part of an org and pertain to almost any topic; companies typically use some type of change management software. 
        - 🔐Assess impact: after a change request is made, however small the request might be, the impact of the potential change must be assessed
        - 🔐Approval/reject: based on the requested change and related impact assessment, common sense plays a big part in the approval process
        - 🔐Build and test: after approval, any change should be developed and tested, ideally in a test environment. ✏️UAT: Before implementing any change, tests should be performed, preferably in a closed environment such as an operational system taken off-line or replicated, which mainly depends on the business capacities.
        - 🔐Schedule/notification: prior to implementing any change, key stakeholders should be notified
        - 🔐Implement: after testing and notification of stakeholders, the change should be implemented; it's important to have a roll-back plan, allowing personnel to undo the change
        - 🔐Validation: once implemented, senior management and stakeholders should again be notified to validate the change
        - 🔐Document the change: documentation should take place at each step; it's critical to ensure all documentation is complete and to identify the version and baseline related to a given change
- When a change management process is enforced, it creates documentation for all changes to a system, providing a trail of info if personnel need to reverse the change, or make the same change on other systems
- Change management control is a mandatory element for some security assurance requirements 🔥(SARs) in the ISO 🔥Common Criteria. Security Assurance Requirements (SARs) document the measures taken during the product's development and evaluation to ensure compliance with the claimed security functionality. It is common for SARs to require change management for the system being developed.
- 📁Change Management roles:
     - 🔮**Program Sponsor** is the role that provides overall direction, support, and oversight for a project or change initiative. They are typically a senior executive or manager who is responsible for ensuring that the project aligns with organizational goals and has the resources and support needed for success. The program sponsor plays a critical role in endorsing and championing the change throughout the organization, addressing high-level issues, and facilitating necessary resources and support.
     - 🔮**Project Manager**: The project manager is responsible for planning, executing, and closing the project. They focus on managing the project’s scope, schedule, and resources but may not have the same level of executive support and overall responsibility as the program sponsor.
     - 🔮**Change Implementer**: The change implementer is responsible for executing the change according to the plan but does not typically handle the overarching support and strategic alignment that the program sponsor provides.
     - 🔮**Change Driver**: This term is less commonly used but generally refers to the person or entity that initiates or advocates for the change.

[7.10](#7.10) Implement recovery strategies (OSG-9 Chpt 18)
- **Recovery strategy**: a plan for restoring critical business components, systems, and operations following a disruption
- ❄️**Disaster recovery (DR)**: set of practices that enables an organization to minimize loss of, and restore, mission-critical technology infrastructure after a catastrophic incident. A disaster is any event that can disrupt normal IT operations and can be either natural or manmade. Hacking and terrorism are examples of manmade disasters, while flooding and fire are examples of natural disasters. Tip❗The term 100-­year flood plain is used to describe an area where flooding is expected once every 100 years. It is, however, more mathematically correct to say that this label indicates a 1 percent probability of flooding in any given year❗
     - 👖Natural disasters: earthquakes, floods, storms, fires, tsunamis, and volcanic eruptions.
     - 👖Human-­made disasters: Explosions, electrical fires, terrorist acts, power outages, other utility failures, infrastructure failures, hardware/software failures, labor difficulties, theft, hacking incident and vandalism, power outage. 📝Most general business insurance and homeowner's insurance policies do not provide any protection against the risk of flooding or flash floods. If floods pose a risk to your organization, you should consider purchasing supplemental flood insurance under FEMA's National Flood Insurance Program.
- ❄️**Business continuity (BC)**: set of practices that enables an organization to continue performing its critical functions through and after any disruptive event
- 7.10.1 Backup storage strategies
    - Backup strategies are driven by org goals and objectives and usually focus on backup and restore time as well as storage needs
    - **Archive bit**: technical detail (metadata) that indicates the status of a backup relative to a given backup strategy
        - 0 = no changes to the file or no backup required
        - 1 = file has been modified or backup required
    - Different backup strategies deal with the archive bit differently; Incremental and differential backup strategies don't treat the archive bit in the same manner
        - once a full backup is complete, the archive bit on every file is reset, turned off, or set to 0
    - Three types of backups:
        - **📁Full backup**: store a complete copy of the data contained on the protected device or backup media; full backups duplicate every file on the system regardless of the setting of the archive bit
        - **📁Incremental backup**: captures changes since the last  full or incremental backup
            -  Incremental backup is used in conjunction with full backups. The incremental backups copy the files that have been changed or created since the last backup, either incremental or full.
            - only files that have the archive bit turned on, enabled, or set to 1 are duplicated
            - once an incremental backup is complete, the archive bit on all duplicated files is reset, turned off, or set to 0
            - Incremental backups provide the option that includes the 📝smallest amount of data. In this case, that would be only the data modified since the most recent incremental backup.
            -  Incremental backups are created faster than differential backups because of the number of files it is necessary to back up each time.
        - **📁Differential backup**: captures changes since the last full backup
            - only files that have the archive bit turned on, enabled, or set to 1 are duplicated
            - unlike full and incremental backups, the differential backup process does not change the archive bit
            - Differential backups involve always storing copies of all files modified since the most recent full backup, regardless of any incremental or differential backups created during the intervening time period.
        - the most important difference between incremental and differential backups is the time needed to restore data in the event of an emergency
        - A differential backup would back up all data modified since the last full backup, which could be a substantial amount.
            - 🔥a combination of full and differential backups will require only two backups to be restored: the most recent full backup and the most recent differential backup.
            - 🔥a combination of full backups with incremental backups will require restoration of the most recent full backups as well as all incremental backups performed since that full backup, which can make the number of backups and the number of required restorations large.
            - differential backups don’t take as long to restore, but they take longer to create than incremental
    - Backup storage best practices include keeping copies of the media in at least one offsite location to provide redundancy should the primary location be unavailable, incapacitated, or destroyed; common strategy is to store backups in a cloud service that is itself geographically redundant
    - Two commmon backup strategies:
        1) full backup on Monday night, then run differential backups every other night of the week
            - if a failure occurs Saturday morning, restore Monday’s full backup and then restore only Friday’s differential backup
        2) full backup on Monday night, then run incremental backups every other night of the week
            - if a failure occurs Saturday morning, restore Monday’s full backup and then restore each incremental backup in the original chronological order

- **Transaction log** backups are specifically designed to support database servers and would not be effective on a file server.

    | Feature    | Full Backup  | Incremental Backup | Differential Backup |
    |---------------|------------------|-----------------------|--------------|
    | **Description**  | A complete copy of all selected data  | Only backs up data that has changed since the last backup (regardless of type) | Backs up all changes made since the last full backup |
    | **Storage Space**     | Requires the most storage space                     | Requires the least storage space                    | Requires more space than incremental but less than full |
    | **Backup Speed**      | Slowest, as it copies all data                      | Fastest, as it only copies changed data since the last backup | Faster than full but slower than incremental, as it copies all changes since the last full backup |
    | **Recovery Speed**    | Fastest, as all data is in one place                | Slowest, as it may require multiple incremental backups to restore to a specific point | Faster than incremental since it requires the last full backup and the last differential backup |
    | **Complexity**        | Simplest, with no dependency on previous backups    | Complex, as it depends on a chain of backups from the last full backup to the most recent incremental backup | Less complex than incremental, requires the last full backup and the last differential backup for restoration |
    | **Best Use Case**     | When backup time and storage space are not issues Ideal for less frequent backups | Suitable for environments where daily changes are minimal and quick backups are necessary | Ideal for environments where storage space is a concern but restoration time needs to be relatively quick |

    - Three main techniques used to create offsite copies of DB content: electronic vaulting, remote journaling, and remote mirroring
        - **🔥electronic vaulting**: where database backups are moved to a remote site using bulk transfers, but it does not do so in real time. Electronic vaulting is an online bulk transfer of data offsite.
        - **🔥remote journaling**: data transfers are performed in a more expeditious manner; remote journaling is similar to electronic vaulting in that transaction logs transferred to the remote site are not applied to a live database server but are maintained in a backup device. Remote journaling options send logs, rather than full data replicas, to the remote location.
            - Remote journaling is a type of backup system where the transfer of data happens closer to real-time. Remote journaling only transmits file deltas to keep systems synchronized. The Recovery Point Objective (RPO) is determined by the frequency of how often the deltas are synchronized.
        - **🔥remote mirroring**: the most advanced db backup solution, and the most expensive, with remote mirroring, a live db server is maintained at the backup site; the remote server receives copies of the db modifications at the same time they are applied to the production server at the primary site. When you use remote mirroring, an exact copy of the database is maintained at an alternative location. You keep the remote copy up to date by executing all transactions on both the primary and remote sites at the 📝same time. Remote mirroring is the only backup option in which a live backup server at a remote site maintains a bit-for-bit copy of the contents of the primary server, synchronized as closely as the latency in the link between primary and remote systems will allow.

- **Backup Tape Rotation** There are several commonly used tape rotation strategies for backups:
	- ⚒️the Grandfather-­Father-­Son (GFS) strategy: This is a tiered backup scheme that uses daily (son), weekly (father), and monthly (Grandfather) backup tapes.
 	- ⚒️the Tower of Hanoi strategy: Uses a series of tapes labeled with numbers that follow the recursive pattern of the Tower of Hanoi puzzle. The nth backup tape is used at intervals of 2^𝑛 days
	- ⚒️the Six Cartridge Weekly Backup strategy: A simple weekly backup rotation strategy that uses Six tapes labeled Monday through Saturday.
 - These strategies can be fairly complex, especially with large tape sets. They can be implemented manually using a pencil and a calendar or automatically by using either commercial backup software or a fully automated hierarchical storage management (HSM) system.
     - **HSM system** is an automated robotic backup 📝jukebox consisting of 32 or 64 optical or tape backup devices. All the drive elements within an HSM system are configured as a single drive array (a bit like RAID). A Hierarchical Storage Management (HSM) system 📝dynamically manages data across different storage media to optimize for access speed or media cost. This technology is frequently used in backup solutions. For example, a HSM will transfer last week’s backup from Solid-State Drives (SSDs) to spindle drives and, after a month, it may transfer it to tape.
- 🍪Online Storage: This is for data that is frequently accessed and actively used. It’s the most accessible and typically the most expensive form of storage.
- 🍪Nearline Storage: This sits between online and offline storage in terms of accessibility and cost. It’s suitable for data that doesn’t need to be immediately accessible but still needs to be retrieved quickly when needed.
- 🍪Offline Storage: This is for data that is rarely accessed and can be stored on less expensive, slower media. Examples include tape drives or optical media.

- 7.10.2 Recovery site strategies
    - Non-disaster: service disruption with significant but limited impact
    - Disaster: event that causes an entire site to be unusable for a day or longer (usually requires alternate processing facility)
    - Catastrophe: major disruption that destroys the facility altogether
    - For disasters and catastrophes, an org has 3 basic options:
        - use a dedicated site that the org owns/operates
        - lease a commercial facility (hot, warm, cold site)
        - enter into a formal agreement with another facility/org
        - A Service bureau: is an organization that can provide on-site or off-site IT services in the event of a disaster
    - When a disaster interrupts a business, a disaster recovery plan should kick in nearly automatically and begin providing support for recovery operations
        -in addition to improving your response capabilities, purchasing insurance can reduce the impact of financial losses
    - Recovery site strategies consider multiple elements of an organization, such as people, data, infrastructure, and cost, as well as factors like availability and location
    - When designing a disaster recovery plan, it’s important to keep your goal in mind — the restoration of workgroups to the point that they can resume their activities in their usual work locations
        - sometimes it's best to develop separate recovery facilities for different work groups
    - To recover your business operations with the greatest possible efficiency, you should engineer the disaster recovery plan so that those business units with the highest priority are recovered first
    - 👽**Mutual assistance agreements (reciprocal)**: (MAAs) provide an inexpensive alternative to disaster recovery sites, but they are not commonly used because they are difficult to enforce. Organizations participating in an MAA may also be shut down by the same disaster, and MAAs raise confidentiality concerns. A reciprocal agreement, also called a Mutual Assistance Agreement (MAA), is an agreement or a memorandum of understanding where two companies pledge the availability of their organization's data center during a disaster. This allows company A to utilize company B's data center or vice versa. They are rarely used in the real world but are quite often discussed in disaster recovery literature. They have been used by companies such as newspaper companies, although the companies very carefully constructed the contracts to ensure success.
    - 👽**Service bureau (vendor hot site)**: is a type of company that it is possible to subscribe to as a service. When a company has a disaster, they only need to show up to the service bureau with their people and their data. 
      
- 7.10.3 Multiple processing sites
    - One of the most important elements of the disaster recovery plan is the selection of alternate processing sites to be used when the primary sites are unavailable
        - 📁**cold sites**: standby facilities large enough to handle the processing load of an organization and equipped with appropriate electrical and environmental support systems
            - a cold site has NO COMPUTING FACILITIES (hardware or software) preinstalled. Installation can take weeks!
            - a cold site has no active broadband comm links
            - A rented space with power, cooling, and connectivity that can accept equipment as part of a recovery effort.
            - 🎈advantages:
                - a cold site is the LEAST EXPENSIVE OPTION and perhaps the most practical
            - 🎈disadvantages:
                - tremendous lag to activate the site, often measured in weeks, which can yield a false sense of security
                - difficult to test
                - Cold sites may take a week or more to become live since the equipment must be purchased and installed. 
                - A rented space with power, cooling, and connectivity that can accept equipment as part of a recovery effort
        - 📁**warm sites**: a warm site is better than a cold site because, in addition to the shell of a building, basic equipment is installed
            - a warm site contains the data links and preconfigured equipment necessary to begin restoring operations, but no usable data for information
            - It provides a good 📝balance between cost and recovery time. It is less expensive than a hot site but facilitates faster recovery than a cold site. 
            - Warm site relies on shared storage and backups for recovery.
            - it balances cost and recovery time, allowing activation in about one week after a disaster is declared
            - ✏️contain workstations, servers, and the communications circuits necessary to achieve operational status but require the 🎆restoration of data from backup.
            - unlike hot sites, however, warm sites do not typically contain copies of the client’s data
            - activation of a warm site typically takes at least 12 hours from the time a disaster is declared
            - It relies on shared storage and backups for recovery
        - 📁**hot sites**: a fully operational offsite data processing facility equipped with hardware and software; a backup facility that is maintained in constant working order, with a full complement of servers, workstations, and comm links
            - a hot site is usually a subscription service
            - A site with dedicated storage and real-time data replication, often with shared equipment that allows restoration of service in a very short time.
            - A hot site provides rapid recovery capabilities since it has all the resources required for instant function and connectivity of IT systems minus people and the most recent data.
            - the data on the primary site servers is periodically or continuously replicated to corresponding servers at the hot site, ensuring that the hot site has up-to-date data
            - They can become live in a 📝few hours. 
            - ✏️contain workstations, servers, and the communications circuits necessary to achieve operational status as well as 🎆current data or near-­real-­time copies of the operational data
            - advantages:
                - unsurpassed level of disaster recovery protection
            - disadvanages:
                - extremely costly, likely doubling an org’s budget for hardware, software and services, and requires the use of additional employees to maintain the site
                - has (by definition) copies of all production data, and therefore increases your attack surface
                - A site with dedicated storage and real-time data replication, often with shared equipment that allows restoration of service in a very short time
        - 📁**Mobile sites**: non-mainstream alternatives to traditional recovery sites; usually configured as cold or warm sites, if your DR plan depends on a workgroup recovery strategy, mobile sites are an excellent way to implement that approach
            -  Non-mainstream alternatives to traditional recovery sites. They typically consist of self-contained trailers or other easily relocated units.
            -  Mobile sites are usually configured as cold sites or warm sites, depending on the disaster recovery plan they are designed to support. 
        - 📁**Cloud Sites**: many orgs now turn to cloud computing as their preferred disaster recovery option
            - some companies that maintain their own datacenters may choose to use these IaaS options as backup service providers
            - Cloud service providers offer Infrastructure-as-a-Service options that are ideal backup sites.
            - Cloud sites are alternate processing facility that take advantage of Infrastructure-as-a-Service (IaaS) providers
    - Note: A hot site is a subscription service, while a redundant site, in contrast, is a site owned and maintained by the org (and a redudant site may be "hot" in terms of capabilities)
        - the exam differentiates between a hot site (a subscription service) and a redundant site (owned by the organization)

- 7.10.4 System resilience, High Availability (HA), Quality of Service (QoS), and fault tolerance
    - **System resilience**: the ability of a system to maintain an acceptable level of service during an adverse event
     - **High Availability (HA)**: the use of redundant technology components to allow a system to quickly recover from a failure after experiencing a brief disruption
        - ✴️**Clustering**: refers to a group of systems working together to handle workloads; often seen in the context of web servers that use a load balancer to manage incoming traffic, and distributes requests to multiple web servers (the cluster). Clustering servers adds a degree of fault tolerance, protecting against the impact of a single server failure.
            - ⚒️Active-active HA cluster: each member actively processes data in advance of a failure. This is commonly referred to as load balancing. Having systems in an active-active or load-balancing configuration is typically more costly
            - ⚒️Active-Passive or hot standby configuration, where the backup systems only begin processing when a failure is detected. An example is the Windows failover cluster, where DB1 and DB2 are both configured as database servers. At any given time, only one will function as the active database server, while the other remains ready to assume responsibility if the first one fails.
        - ✴️**Redundancy**: unlike a cluster, where all members work together, redundancy typically involves a primary and secondary system; the primary system does all the work, and the secondary system is in standby mode unless the primary system fails, at which time activity can fail over to the secondary
        - Both clustering and redundancy include high availability as a by-product of their configuration
     - **Quality of Service (QoS)**: controls protect the availability of data networks under load
        - many factors contribute to the quality of the end-user experience and QoS attempts to manage all of these factors to create an experience that meets business requirements
        - factors contributing to QoS:
            - bandwidth: the network capacity available to carry communications
            - latency: the time it takes a packet to travel from source to destination
            - packet loss: some packets may be lost between source and destination, requiring re-transmission
            - interference: electrical noise, faulty equipment, and other factors may corrupt the contents of packets
    - **Fault tolerance**: the ability of a system to suffer a fault but continue to operate
    - **Redundant array of independent disks (RAID)**: refers to multiple drives being used in unison in a system to achieve greater speed or availability; the most well-known RAID levels are:
        - 🔥RAID 0—📝Striping: provides significant speed, writing and reading advantages. Requires minimum of 📝2 drives. offers no 🚫redundancy. RAID level 0 is also known as disk striping. When implementing RAID 0 (striping) with two 50 GB drives, the effective capacity of your disks is simply the sum of both drives since RAID 0 doesn’t use any space for redundancy. It distributes (stripes) data evenly across both drives, maximizing storage capacity.
        - 🔥RAID 1—📝Mirroring: uses redundancy to provide reliable availability of data. Requires minimum of 📝2 drives. sacrifices 🚫performance. RAID 1 is called disk mirroring. When implementing RAID 1 (mirroring) with two 50 GB drives, the effective capacity is equal to the capacity of just one of the drives because the second drive is used to duplicate (mirror) the data for redundancy.
        - 🔥RAID 3: This level uses byte-level striping with a dedicated parity disk. It provides fault tolerance but can become a bottleneck because all parity calculations are handled by a single disk. It is less common compared to RAID 5 and RAID 6. RAID 3 requires a minimum of 📝3 drives because it uses dedicated parity for fault tolerance. So, the effective capacity of your disks in RAID 3 with 3 drives of 50 GB each is 100 GB.
        - 🔥RAID 4 provides fault tolerance and improved performance by striping data across multiple drives, similar to RAID-3.
        - 🔥RAID 10—📝Mirroring and Striping: requires a minimum of 📝four drives and provides the benefits of striping (speed) and mirroring (availability) in one solution; this type of RAID is typically one of the most expensive. RAID 10 is known as a stripe of mirrors. When implementing RAID 10 (a combination of RAID 1 + RAID 0) with 4 drives of 50 GB each, half of the drives are used for mirroring (RAID 1), and the other half are used for striping (RAID 0). So, the effective capacity of your disks in RAID 10 is 100 GB. RAID 10 may protect against two drive failures, depending on which drives fail.
        - 🔥RAID 5—📝Parity Protection: requires a minimum of three drives and provides a cost-effective balance between RAID 0 and RAID 1; RAID 5 utilizes a parity bit, computed from an XOR operation, for purposes of storing and restoring data. RAID level 5 offers both redundancy and fault tolerance by using striping with distributed parity. It uses 📝three or more disks, with 📝one disk containing parity information used to restore data to another disk in the event of failure. When used with three disks, RAID 5 is able to withstand the loss of a single disk. RAID 5 is called disk striping with parity.
            - When implementing RAID 5 with 3 drives of 50 GB each, one drive's worth of capacity is used for parity, which provides fault tolerance. The remaining capacity is used for data storage. So, the effective capacity of your disks in RAID 5 with 3 drives of 50 GB each is 100 GB.
        - 🔥RAID 6 - Similar to RAID-5, however, 📝two sets of parity are written to each drive. This allows for two drives to fail without causing the RAID to fail. This provides redundancy, but the usable storage is reduced by two drive's worth of storage. RAID 6 requires at least 📝4 drives, as it uses two drives for parity, allowing for better fault tolerance (two simultaneous drive failures).  If you had 4 drives of 50 GB each, the effective capacity would be Total capacity - Size of two drives (used for parity) i.e 100 GB. RAID 6 protects against two drive failures.
        - 🧯If four drives at 100GB each are used, you get the following results for each RAID level:
            - 🧠RAID-0 – 400GB of usable space with no fault-tolerance.
            - 🧠RAID-1 – 200GB of usable space with 2x fault-tolerance (1 from each mirrored pair). usable storage is reduced by 50% of your total storage
            - 🧠RAID-10 – 200GB of usable space with ~2x fault-tolerance.
            - 🧠RAID-5 – 300GB of usable space with 1x fault-tolerance.
            - 🧠RAID-6 – 200GB of usable space with 2x fault-tolerance.

    | Backup Method | Cost Implications                                                          | Time Implications for RPO                      |
    |---------------|----------------------------------------------------------------------------|-----------------------------------------------|
    | Incremental   | Lower cost due to reduced storage requirements as only changes are backed up | Longer recovery time as it requires the last full backup plus all subsequent incremental backups until the RPO |
    | Differential  | Moderate cost; more storage is needed than incremental, but less than full, as it stores all changes since the last full backup | Faster recovery than incremental as it requires the last full backup and the last differential backup up to the RPO |
    | Replication   | Higher cost due to the need for a duplicate environment ready to take over at any time; continuous data replication can also increase bandwidth costs | Minimal recovery time as the data is continuously updated, allowing for near-instant recovery up to the latest point before failure |
    | Clustering    | Highest cost because it involves multiple servers (cluster) working together to provide high availability and redundancy | Minimal recovery time as the system is designed for immediate failover without data loss, ensuring the RPO can be met instantaneously |


    | Site Recovery Method | Cost Implications                                                                 | Time Implications for RTO                         |
    |----------------------|-----------------------------------------------------------------------------------|---------------------------------------------------|
    | Cold Site            | Lowest cost option; facilities and infrastructure are available, but equipment and data need to be set up post-disaster | Longest recovery time as systems and data must be configured and restored from backups Suitable for non-critical applications with more flexible RTOs |
    | Warm Site            | Moderate cost; a compromise between cold and hot sites, includes some pre-installed hardware and connectivity that can be quickly activated | Faster recovery than a cold site as the infrastructure is partially ready, but data and systems might still need updates to be fully operational |
    | Hot Site             | High cost; a duplicate of the original site with full computer systems and near-real-time replication of data and ready to take over operations immediately | Minimal recovery time, designed for seamless takeover with data and systems up-to-date, allowing for critical operations to continue with little to no downtime |
    | Redundant Site       | Highest cost; essentially operates as an active-active configuration where both sites are running simultaneously, fully mirroring each other | Instantaneous recovery, as the redundant site is already running in parallel with the primary site, ensuring no interruption in service |
 
[7.11](#7.11) Implement Disaster Recovery (DR) processes (OSG-9 Chpt 18)

- 🔴**Business Continuity Management (BCM)**: the process and function by which an organization is responsible for creating, maintaining, and testing BCP and DRP plans
    - 🧮**Business Continuity Planning (BCP)**: focuses on the survival of the business processes when something unexpected impacts it. Business continuity planning is focused on 📝keeping business functions uninterrupted when a disaster strikes. Preventing business interruption is the goal of business continuity
    - 🧮**Disaster Recovery Planning (DRP)**: focuses on the recovery of vital technology infrastructure and systems. disaster recovery planning picks up where business continuity planning leaves off. Disaster recovery planning guides an organization through recovery of normal operations at the primary facility. The goal of DRP is to 📝restore regular business activity as quickly as possible
- Organizations can choose whether to develop business continuity planning or disaster recovery planning plans, although it is highly recommended that they do both.
- Tip❗People should always be your highest priority in business continuity planning. Any life safety systems, e.g fire suppression systems, should always receive high prioritization❗ Employee safety should always be the top priority when designing a Business Continuity Plan (BCP) or Disaster Recovery Plan (DRP).
    - BCM, BCP, and DRP are ultimately used to achieve the same goal: the continuity of the business and its critical and essential functions, processes, and services
- The key BCP/DRP steps are:
    - 🚡Develop contingency planning policy
    - 🚡Conduct BIA (business Impact Analysis): During the business impact analysis phase, you must identify the business priorities of your organization to assist with the allocation of BCP resources. You can use this same information to drive the disaster recovery planning business unit prioritization.
        - 🎈BIA (business Impact Analysis): Performing a Business Impact Analysis (BIA) is critical in developing a Business Continuity Plan (BCP) and a Disaster Recovery Plan (DRP). The BIA identifies all critical functions and processes so the organization can prioritize them based on criticality. The BIA also involves calculating risk for the identified business functions so that vulnerabilities are prioritized appropriately.
        - In business continuity planning, qualitative and quantitative evaluations are performed and relative priorities are established primarily during the Business Impact Analysis (BIA) stage.
            - ⚒️Qualitative evaluations might consider impacts on intangibles such as customer reputation
            - ⚒️Quantitative evaluations typically rely on calculations such as Single Loss Expectancy (SLE) and Annualized Loss Expectancy (ALE) to measure the impact on tangible assets.
    - 🚡Identify controls
    - 🚡Create contingency strategies
    - 🚡Develop contingency plan
    - 🚡Ensure testing, training, and exercises (Conducting regular business continuity exercises, such as practical continuity work, keeps knowledge fresh)
    - 🚡Maintenance
- 🍏Four key measurements for BCP and DRP procedures:
    - ⚒️RPO (recovery point objective): max tolerable data loss measured in time and should be used to guide backup strategies
    - ⚒️RTO (recovery time objective): max tolerable time to recover systems to a defined service level.  It should never exceed the Maximum Tolerable Downtime (MTD)
    - ⚒️WRT (work recovery time): max time available to verify system and data integrity as part of the resumption of normal ops
    - ⚒️MTD (max tollerable downtime): max time-critical system, function, or process can be disrupted before unacceptable/irrecoverable consequences to the business

- 7.11.1 ❄️Response
    - A disaster recovery plan should contain simple yet comprehensive instructions for essential personnel to follow immediately upon recognizing that a disaster is in progress or imminent
    - Emergency-response plans are often put together in a form of checklists provided to responders; arrange the checklist tasks in order of priority, with the most important task first!
    - The response plan should include clear criteria for activation of the disaster recovery plan, define who has the authority to declare a disaster, and then discuss notification procedures
    - The emergency response guidelines should include the immediate steps an organization should follow in response to an emergency situation. These include immediate response procedures, a list of individuals who should be notified of the emergency, and secondary response procedures for first responders. 
- 7.11.2 ❄️Personnel
    - A disaster recovery plan should contain a list of 📝contact information of personnel to contact in the event of a disaster
        - usually includes key members of the DRP team as well as critical personnel
        - An organization's employee contact information should be contained in the Disaster Recovery Plan (DRP) for potential emergencies. This is also referred to as a 📝Crisis Communications Plan. The plan also provides a priority list to establish a chain of command.
    - Businesses need to make sure employees are trained on DR procedures and that they have the necessary resources to implement the DR plan
	- Key activities involved in preparing people and procedures for DR include:
		- develop DR training programs
		- conduct regular DR drills
		- provid eemployees with necessary resources and tools to implement the DR plan
		- communicate the DR plan to all employees
- 7.11.3 ❄️Communications
    - Ensure that response checklists provide first responders with a clear plan to protect life and property and ensure the continuity of operations
        - the notification checklist should be supplied to all personnel who might respond to a disaster
- 7.11.4 ❄️Assessment
    - When the DR team arrives on site, one of their first tasks is to assess the situation
        - this normally occurs in a rolling fashion, with the first responders performing a simple assessment to triage the situation and get the disaster response under way
        - as the incident progresses more detailed assessments will take place to gauge effectiveness, and prioritize the assignment of resources
    
- 7.11.5 ❄️Restoration
    - Note that recovery and restoration are separate concepts
    - **Restoration**: bringing a business facility and environment back to a workable state
    - **Recovery**: bringing business operations and processes back to a working state
    - System recovery includes the restoration of all affected files and services actively in use on the system at the time of the failure or crash
    - When designing a disaster recovery plan, it’s important to keep your goal in mind — the restoration of workgroups to the point that they can resume their activities in their usual work locations

- 7.11.6 ❄️Training and awareness
    - As with a business continuity plan, it is essential that you provide training to all personnel who will be involved in the disaster recovery effort
    - When designing a training plan consider the following:
        - orientation training for all new employees
        - initial training for employees taking on a new DR role for the first time
        - detailed refresher training for DR team members
        - brief awareness refreshers for all other employees
- 7.11.7 ❄️Lessons learned
    - A lessons learned session should be conducted at the conclusion of any disaster recovery operation or other security incident.
    - A lessons learned document is often created and distributed to involved parties after a postmortem review to ensure that those who were involved in the incident and others who may benefit from the knowledge are aware of what they can do to prevent future issues and to improve response in the event that one occurs.
    - The lessons learned process is designed to provide everyone involved with the incident response effort an opportunity to reflect on their individual roles and the teams overall response
    - Time is of the essence in conducting a lesson learned, before memories fade
    - Usually a lessons learned session is led by 📝trained facilitators or 📝External Consultants
    - 📂NIST SP 800-61 offers a series of questions to use in the lessons learned process:
        - exactly what happened and at what times?
        - how well did staff and management perform in dealing with the incident?
        - were documented procedures followed?
        - were the procedures adequate?
        - were any steps or actions taken that might have inhibited the recovery?
        - what would the staff and management do differently the next time a similar incident occurs?
        - how could information sharing with other organizations have been improved?
        - what corrective actions can prevent similar incidents in the future?
        - what precursors or indicators should be watched for in the future to detect similar incidents?
        - what additional tools or resources are needed to detect, analyze, and mitigate future incidents?
    - The team leader to document the lessons learned in a report that includes suggested process improvement actions

[7.12](#7.12) Test Disaster Recovery Plans (DRP) (OSG-9 Chpt 18)
- Every DR plan must be tested on a periodic basis to ensure that the plan’s provisions are viable and that it meets an org’s changing needs
- Five main test types:
    - checklist\Read-through tests
    - structured walk-throughs/tabletop
    - simulation tests
    - parallel tests
    - full-interruption tests
- 7.12.0 📙**Checklist tests:**
- The checklist review is the 📝least disruptive type of disaster recovery test. During a checklist review, team members each review the contents of their disaster recovery checklists on their own and suggest any necessary changes. It also has a very minimal 📝time commitment.
    - The checklist review is the least disruptive type of disaster recovery test. During a checklist review, team members each review the contents of their disaster recovery checklists on their own and 📝suggest any necessary changes.
    - A checklist exercise is to walk through the DRP using a checklist of items/sections that should be in the plan.
    - It is possible to use a list such as commonly forgotten items to see if they have been included in the plan. A desk check, also known as a checklist, reviews the plan to ensure the expected items have been included in the disaster recovery plan document itself.
- 7.12.1 
    - 📙**Read-through test**: one of the 📝simplest to conduct, but also one of the most critical; copies of a DR plan are distributed to the members of the DR team for review, accomplishing three goals:
        - ensure that key personnel are aware of their responsibilities and have that knowledge refreshed periodically
        - In the read-through test, you distribute copies of the disaster recovery plan to key personnel for review but do not actually meet or perform live testing
        - provide individuals with an opportunity to review and update plans, remvoving obsolete info
        - helps identify situations in which key personnel have left the company and the DR responsibility needs to be re-assigned (note that DR responsibilities should be included in job descriptions)
- 7.12.2 Walkthrough
    - 📙**Structured walk-through/Tabletop**: AKA tabletop exercise, takes testing one step further, where members of the DR team gather in a large conference room and role-play a disaster 📝scenario
        - the team refers to their copies of the DR plan and discuss the appropriate responses to that particular type of disaster
        - Tabletop exercises are designed to walk teams through a scenario, role-playing a disaster recovery scenario to test their readiness to respond. A moderator knows the full scope of the disaster, and each team member provides suggestions as they discuss the process of responding to this scenario. 
        - During a tabletop exercise, team members come together and walk through a scenario 📝without making any changes to information systems.
        - A structured walk-through test occurs in the 📝conference room where team members talk through a scenario using their plan. The completeness of the plan is assessed before they leave the conference room and attempt any actions on any systems.
        - The idea is to talk through the steps that have been documented to see if it has all of the correct steps and that they appear to be in the Disaster Recovery Plan (DRP). It is called a tabletop exercise because the team sits around a conference table to talk through the steps
        - It's important to have a structured walk-through before a live disaster test to adequately prepare and complete all necessary implementations.
        - tabletop plans prioritizes cost and minimal disruption over realism
- 7.12.3 Simulation
    - 📙**Simulation tests**: similar to the structured walk-throughs, where team members are presented with a scenario and asked to develop an appropriate response
        - unlike read-throughs and walk-throughs, some of these response measures are then tested
        - In simulation tests, disaster recovery team members are presented with a scenario and asked to develop an 📝appropriate response. 
        -  ❗Simulation tests may shut down noncritical business units. 
        - this may involve the interruption of noncritical business activities and the use of some operational personnel
        - A simulation test involves a 📝role-play of a prepared scenario overseen by a moderator. Responses are assessed to help improve the organization's response process. 
        - Simulations are the 📝most complete test that can be conducted without the risk that a full failover test creates.
        - In a simulation, the systems are not brought to a functional level at the alternate site. The plan is analyzed possibly 📝in and 📝out of the conference room. A simulation could involve stepping through some of the procedural documents to see if the instructions match what is visible in a system, without taking any actual action.
- Checklist tests, structured walk-­throughs, and simulations are all test types that do not involve actually activating the alternate site.
- 7.12.4 Parallel
    - 📙**Parallel tests**: represent the next level, and involve 📝relocating personnel to the alternate recovery site and implementing site activation procedures
        - the relocated employees perform their DR responsibilities just as they would for an actual disaster
        - In a parallel test, production processing is not interrupted
        - operations at the main facility are not interrupted
        - During a parallel test, the team actually activates the disaster recovery site for testing, but the primary site remains operational in the original location during the test. 
        -  it fully evaluates operations at the backup facility but does not shift primary operations responsibility from the main site
        -  During a parallel test, the team activates the disaster recovery site for testing, but the primary site remains operational. 

- 7.12.5 Full interruption
    - 📙**Full-interruption tests**: operate like parallel tests, but involve actually shutting down operations at the primary site and shifting them to the recovery site
        - 📝most disruptive
        - these tests involve a significant risk (📝shutting down the primary site, transfer recovery ops, followed by the reverse) and therefore are extremely difficult to arrange (management resistance to these tests are likely)
        - During a full interruption test, the team takes down the primary site and confirms that the disaster recovery site is capable of handling regular operations. The full interruption test is the most thorough test but also the most disruptive.
     
- 7.12.6 Communication
    - ❄️**DR Comunication**: DR testing communication encompasses various internal and external stakeholders with different needs and communication methods/preferences
        - 🔥Internal Stakeholders:
            - Disaster Recovery Team: Reliable real-time channels for real-time cordination, problem-solving and decision making e.g Chats, Teams, Zoom
            - Managment: Needs periodic updates on progress and potential impact on normal operations. e.g emails
            - Wide Company: General awareness and ahead of scheduled tests minimizes confusion. Pre-test and post-test announements.
            - The executive summary provides a high-­level view of the entire organization’s disaster recovery efforts. This document is useful for the managers and leaders of the firm as well as public relations personnel who need a nontechnical perspective on this complex effort.
        - 🔥External Stakeholders:
            - Customer/Clients: In large scale tests or real life scenarios, there should be proactive controlled communications about potential service disruptions. When there are impacts, ETAs for resolution are important for preserving trust and managing expectations.
            - Partners: if testing may impact join systems or join operations, inform partners of relevant timelines and agree on cordination channels.
            - Regulators: Certain industries have reporting requirements. For testing that triggers downtime of regulated systems, advanced notification of post-test results may be mandatory. Always include compliance experts in plan creation and testing.

[7.13](#7.13) Particpate in Business Continuity (BC) planning and exercises (OSG-9 Chpt 3)

- Business continuity planning addresses how to keep an org in business after a major disruption takes place
    - It's important to note that the scope is much broader than that of DR
    - A security leader will likely be involved, but not necessarily lead the BCP effort
- The BCP life cycle includes:
    - Developing the BC concept
    - Assessing the current environment
    - Implementing continuity strategies, plans, and solutions
    - Training the staff
    - Testing, exercising, and maintaining the plans and solutions
-  Plan-Do-Check-Act model:
    - ✏️⚒️PLAN: Ensure that business continuity policy, objectives, targets, controls, processes and procedures relevant to improving business continuity have been established.
    - ✏️⚒️DO: Ensure the business continuity policy, controls, processes, and procedures have been implemented.
    - ✏️⚒️ACT: Maintain and improve the Business Continuity Management (BCM) system by taking corrective action, based on the results of management review.
    - ✏️⚒️CHECK: Monitor and review performance against business continuity policy and objectives, report the results to management for review, and determine and authorize actions for remediation and improvement.


[7.14](#7.14) Implement and manage physical security (OSG-9 Chpt 10)
- Physical access control mechanisms deployed to control, monitor and manage access to a facility
    - Sections, divisions, or areas within a site should be clearly designated as public, private, or restricted with appropriate sinage
- 7.14.1 Perimeter security controls
    - 🍮**Fence** is a perimeter-defining (outdoor) device. Fences should be at least 📝eight feet/two to three meters tall with 📝barbed wire at the top for adequate deterrence. The standard fencing height recommendations are:
        - ⚒️3-4 feet/1-1.5 meters deter casual trespassers
        - ⚒️6-7 feet/1.8- 2.1 meters deter most intruders, but not determined ones
        - ⚒️8 or more feet/2.4 meters or more deter determined intruders
    - Fences can consist of:
        - stripes painted on the ground
        - chain link fences
        - barbed wire
        - concrete walls
        - invisible perimeters using laser, motion, or heat detection
    - 🍮**Perimeter intrusion detection and assessment system (PIDAS)**: an advanced form of fencing that has two or three fences used in concert to optimize security. A PIDAS (perimeter intrusion detection and assessment system) is a 📝fence system that has two or three fences used in concert to optimize security. The space between the fences can serve as a corridor for guard patrols or wandering guard dogs. One or more fences can support touch detection technologies. An exterior fence is used to keep animals and casual trespassers from accessing the main fence. This reduces the nuisance alarm rate (NAR) or false positives.
    - 🍮**Sensitive compartmented information facility (SCIF)** is often used by government and military agencies, divisions, and contractors to provide a secure environment for highly sensitive data storage and computation. An SCIF is typically located within a structure, although an entire structure can be implemented as an SCIF. 
    - 🍮**Gate**: controlled exit and entry point in a fence or wall
    - 🍮**Turnstile**: form of gate that prevents more than one person at a time from gaining entry and often restricts movement in one direction. Turnstiles allow only one person to enter at a time. As one person swipes a badge and enters the data center, the turnstile restricts the following person. It's a method of preventing 📝tailgating/📝piggybacking, which happens when an unauthorized person follows an authorized person through an entrance.
    - 🍮**Access control vestibule\Mantrap**: (AKA mantrap) a double set of doors that is often protected by a guard or other physical layout preventing piggybacking and can trap individuals at the discretion of security personnel
        - A mantrap uses two sets of doors, only one of which can open at a time. A mantrap is a type of 📝preventive access control, although its implementation is a physical control.
            - Double door systems, or mantraps, force individuals into a small room with an ingress and egress door. Before the person can exit through the egress door, the ingress door must be closed and locked. If the individual is authorized, the egress door will unlock and they can proceed. If they are not authorized, both doors remain locked until a security guard or police officer arrives and escorts them off the property or arrests them for trespassing. It is common for mantraps to have a weight scale across the floor to ensure only one person is in the room at a time.
            - It can also be used preventing 📝tailgating/📝piggybacking
    - 🍮**Security bollards**: a key element of physical security, which prevent vehicles from ramming access points and entrances
    - 🍮**Barricades**: in addition to fencing, are used to control both foot traffic and vehicles
    - 🍮**Lighting** is the 📝most commonly used form of perimeter security control providing the security benefit of deterrence (primary purpose is to discourage casual intruders, trespassers etc). A foot-candle or lumen is a measure of the visible light that a light produces. The accepted standard for lighting or MINIMUM light level used in parking lots or perimeters is a minimum of 📝two foot-candles or 📝two lumen.
    - All physical security controls ultimately rely on personnel to intervene and stop actual intrusions and attacks
- 7.14.2 Internal security controls
    - In all circumstances and under all conditions, the most important aspect of security is protecting people
    - If a facility is designed with restricted areas to control physical security, a mechanism to handle visitors is required
    - 🏀**Visitor logs**: manual (or automated) list of nonemployee entries or access to a facility/location
        - physical access logs can establish context for interpretation of logical logs
    - 🏀**Locks**: designed to prevent access without proper authorization; a lock is a crude form of an identification and authorization mechanism. Key locks are the most common and inexpensive form of physical access control device for both interior and exterior use. 

[7.15](#7.15) Address personnel safety and security concerns (OSG-9 Chpt 16)
- 7.15.1 🔴Travel
    - Training personnel on safe practices while traveling can increase their safety and prevent security incidents:
        - sensitive data: devices traveling with the employee shouldn’t contain sensitive data
        - malware and monitoring devices: possibilities include physical devices being installed in a hotel room of a foreign country
        - free wi-fi: sounds appealing but can be a used to capture a user's traffic
        - VPNs: employers should have access to VPNs that they can use to create secure connections

- 7.15.2 🔴Security training and awareness
    - Orgs should add personnel saftey and security topics to their training and awareness program and help ensure that personnel are aware of duress systems, travel best practices, emergency management plans, and general safety and security best practices
    - security training and awareness programs usually focus on broader and more universally applicable topics like insider threats, which address risks from employees or contractors; social media impact, which covers the risks of sharing information online; and 2FA fatigue, which relates to the weariness or complacency in using two-factor authentication. These are relevant to a wider audience and are crucial for overall organizational security awareness. 
    - Training programs should stress the importance of protecting people
- 7.15.3 Emergency management
    - Emergency management plans and practices help an organization address personnel safety and security after a disaster
    - Safety of personnel should be a primary consideration during any disaster
- 7.15.4 💥Duress
    - An example of a duress system is a button that sends a distress call
    - Duress systems are useful when personnel are working alone
    - If a duress system is activated accidentally code word(s) can be used to assure responding personnel it was an accident, or omit the word(s) keying an actual response
    - Cipher lock: Some cipher locks can be programmed with more than one code. One code can open the door, and another code can open the door and also raise a silent duress alarm. 

- 🔴**Trusted Computing Base (TCB)**: This encompasses all the hardware, software, and firmware that are critical to the system's security. It includes the Security Kernel but also includes other components involved in the security of the system. The TCB includes the reference monitor and the security kernel as well as the other security mechanisms necessary to be able to trust the computer. It can be considered the totality of all security mechanisms.
    - It is the collection of all hardware, software, and firmware components within an architecture that is specifically responsible for security and the isolation of objects;
    - It is the combination of hardware, software, and controls that work together to enforce a security policy.
    - TCB is a term that is usually associated with 📝security kernels and the 📝reference monitor
    - TCB has a component known as the reference monitor in theory, which becomes the security kernel in implementation.
    - The 📝reference monitor validates access to every resource prior to granting the requested access. The reference monitor is much like the bouncer at a club because it stands between each subject and object. Its role is to verify the subject meets the minimum requirements for access
- 📘**Security Kernel**: The actual implementation within the OS that enforces the security policies and manages the interfaces between the hardware, OS, and other parts of the system. It is part of an operating system (OS) responsible for providing security interfaces among the hardware, OS, and other parts of the computing system
    - **Ring Protection**:Ring protection is a feature of CPU architectures that provides a way to enforce different levels of privilege or protection for different parts of the system. It uses a hierarchy of privilege levels, typically referred to as "rings," where Ring 0 (the innermost ring) has the highest level of privilege and direct access to hardware, while Ring 3 (the outermost ring) has the lowest level of privilege and is used for user applications.
        - 🎈Ring 0 The kernel lies within the central ring. contains the operating system's kernel
            - The kernel mode of a CPU, also known as supervisor mode or privileged mode, is a distinct execution state in the Central Processing Unit (CPU) of a computer. It is a higher privilege level than the user mode and allows the operating system's kernel (core) to have full control and access to hardware resources and system-wide operations. In kernel mode, the CPU can perform the following essential tasks:
                - 🥑Direct Hardware Access: The kernel mode allows direct access to hardware resources, such as memory, CPU registers, and I/O ports. This level of access is necessary for managing hardware devices, setting up memory protection, and handling interrupts.
                - 🥑Critical System Operations: The operating system's core functions, such as memory management, process scheduling, and input/output operations, require the ability to execute critical system instructions that are only permitted in kernel mode.
                - 🥑Exception Handling: When the CPU encounters exceptions, such as system calls, hardware interrupts, or page faults, the kernel mode provides the necessary privileges to handle these events effectively.
                - 🥑Memory Protection: The kernel mode enforces memory protection, ensuring that user-level processes cannot access restricted memory areas or interfere with the operating system's critical data structures.
                - 🥑Privileged Instructions: Certain CPU instructions are considered privileged, meaning they can only be executed in kernel mode. These instructions are essential for controlling system-level operations and managing hardware resources.
            - It is crucial to have a clear distinction between the kernel mode and the user mode to ensure the security and stability of the operating system. 
        - 🎈Ring 1 Conceptually contains other operating system components.
        - 🎈Ring 2 is used for drivers and protocols.
        - 🎈Ring 3 User-level programs and applications: In the user mode (also known as program mode and problem mode), applications and user-level processes run with limited privileges, preventing them from interfering with critical system operations or accessing sensitive resources directly. Any attempt by a user-level process to execute privileged instructions or access restricted resources results in a hardware exception, which the operating system can handle in the kernel mode to maintain system integrity. 
    - Note📝 Rings 0 through 2 run in privileged mode while Ring 3 runs in user mode. Layers 1 and 2 contain device drivers but are not normally implemented in practice, since they are often collapsed into layer 0. It is important to note that many modern operating systems do not fully ­implement this model.
    - 🔖**Reference Monitor**: This concept refers to an abstract model that 🔥enforces access control policies in a system. It's a theoretical component that must be implemented by the Security Kernel. The reference monitor enforces access control between all subjects and all objects.
        - The reference monitor is an abstract machine concept that mediates all access from every subject before granting access requests to objects.
        - It is described as an abstract machine which means that it needs to be built using as few lines of code as possible to be able to validate that it works exactly as it is supposed to.
        - The reference monitor should meet four conditions:
            - 1) it must always be turned on,
              2) it must be verifiable,
              3) it must be tamper-proof, and
              4) it cannot be bypassed.
    - 🔖**Process States**: Ready, running, and stopped each refer to operating system process states.
        - 🏓Ready describes a process queued for execution (in the order of any priority set) as soon as the Central Processing Unit (CPU) becomes available.
        - 🏓Running describes a process that is in the midst of execution by the CPU.
        - 🏓Stopped: A process that is no longer running because it has been completed (or errored out) is described as stopped.
- 📘**Security Perimeters**:
   - the imaginary boundary that separates the TCB from the rest of the system is known as a security perimeter
   - A border firewall could be considered a security perimeter protection device
   - the boundary of the physically secure area surrounding your system, is also a security perimeter
- 🔴**DDOS Attacks**: Performing a load or stress test to validate how applications/systems performs under both expected and extreme loads to know what a denial-of-service attack based on load will look like is a good practice. Not all instances of DoS are the result of a malicious attack. Errors in coding OSs, services, and applications have resulted in DoS conditions. Some examples of this include a process failing to release control of the CPU or a service consuming system resources out of proportion to the service requests it is handling.  A 📝DoS attack typically originates from a single source. A 📝DDoS attack is more complex and involves multiple sources. The attacker uses a network of compromised computers (often referred to as a botnet) to launch a coordinated attack on the target. 
    - 🔥SYN Flood: In a SYN flood attack, the attacker sends a large number of SYN packets to a system but does not respond to the SYN/ACK packets, attempting to overwhelm the attacked system’s connection state table with half-open connections. Many firewalls have a built-in anti–SYN flood defense (SYN-ACK Spoofing) that responds to SYNs on behalf of protected systems. It is a TCP attack.
    - 🔥Smurf: A Smurf attack is a type of DDoS (Distributed Denial of Service) attack where an attacker sends a large number of ICMP echo request packets to a 📝broadcast address, with the source address spoofed to that of the target. In a Smurf attack, the attacker spoofs the victim's IP address as the source and sends Internet Control Message Protocol (ICMP) Echo Requests to the network's broadcast address. The echo request is usually referred to as a PING. Some define PING as the Packet InterNet Groper. When all of the devices on the network reply with an ICMP Echo Response, they will flood the intended victim. With a lot of devices on the network, a flood of packets would hit the victim's device, causing a Denial-of-Service (DoS) attack.
    - 🔥Ping Flood (or ICMP Flood): attack involves overwhelming the target with a large number of ICMP echo request (ping) packets. Unlike a Smurf attack, it doesn't rely on amplification via broadcast addresses
    - 🔥Fraggle attack: is similar to a Smurf attack but uses 📝UDP echo packets instead of ICMP.
    - 🔥TCP Reset Attack: Another type of attack that manipulates the TCP session is the TCP reset attack. Sessions are normally terminated with either the FIN (finish) or the RST (reset) packet. Attackers can spoof the source IP address in a RST packet and disconnect active sessions. The two systems then need to reestablish the session. This is primarily a threat for systems that need persistent sessions to maintain data with other systems. When the session is reestablished, they need to re-­create the data, so it’s much more involved than just sending three packets back and forth to establish the session.
    - 🔥Time Synchronization Attack or Time-Reset Attack: manipulates the system clock or NTP to set the operating system time to zero, disrupting time-sensitive protocols, logs, and scheduled tasks
    - 🔥Ping of Death: A ping-­of-­death attack used oversized ping packets. Some operating systems couldn’t handle them. In some cases, the systems crashed, and in other cases, the attack caused a buffer overflow error (legacy).
    - 🔥Teardrop: A teardrop attack 📝fragments data packets, making them difficult or impossible to be put back together by the receiving system. This often caused systems to crash (legacy). A teardrop attack exploits a flaw in a system’s ability to reassemble oversized fragmented packets. Attackers intentionally send oversized fragmented packets with an offset and size that are incorrect, causing the victim system to crash when they are reassembled. This attack breaks traffic into malformed-sized pieces before transmitting, so it goes undetected. It is designed to cause a Denial of Service (DoS).
    - 🔥Land: In a land attack, the attack sends spoofed SYN packets to a victim using the victim’s IP address as both the source and destination IP address. It occurs when a packet is sent to a device and the source and destination Internet Protocol (IP) addresses are the same value. Again the employee/user would not know of the attacker's presence until the machine fails.
        - Banana Attack: A variant is a banana attack, which redirects outgoing messages from a system back to the system, shutting down all external communication (legacy).
    - 🔥DNS Amplification Attack: where an attacker sends false requests to third-party DNS servers with a forged source IP address belonging to the targeted system. Because the attack uses UDP requests, there is no three-way handshake. The attack packets are carefully crafted to elicit a lengthy response from a short query. The purpose of these queries is to generate responses headed to the target system that are sufficiently large and numerous enough to overwhelm the targeted network or system.

- 🔴**Man-­in-­the-­middle attack**: (sometimes called an on-­path attack) occurs when a malicious user is able to gain a logical position between the two endpoints of a communications link. Although it takes a significant amount of sophistication on the part of an attacker to complete a man-­in-­the middle attack, the amount of data obtained from the attack can be significant.
    - 🔥IP Spoofing: In this attack, an attacker sends IP packets from a false (spoofed) source address to impersonate a trusted host. The attacker intercepts and modifies traffic between two parties who believe they are communicating directly with each other.
    - 🔥DNS Spoofing (DNS Cache Poisoning): This involves corrupting the DNS resolution process to redirect users to malicious websites. The attacker manipulates the DNS server to cache incorrect IP address information, leading users to unintended destinations.
    - 🔥ARP Spoofing (ARP Cache Poisoning): Address Resolution Protocol (ARP) spoofing occurs when an attacker sends fake ARP messages over a local area network, associating their MAC address with the IP address of a legitimate network device. This enables the attacker to intercept data intended for the targeted device.
        - Address Resolution Protocol (ARP) is used to map Media Access Control (MAC) addresses to Internet Protocol (IP) addresses. Attackers can spoof an IP address by sending a gratuitous ARP update to a client claiming an IP address. The client will update its ARP cache with the attacker’s MAC address that maps the IP address. If an attacker spoofs a client’s default gateway, the client will send all traffic it would typically send to the gateway to the attacker instead. 
    - 🔥SSL Stripping: This attack downgrades HTTPS connections to HTTP, allowing the attacker to intercept and read plaintext communication. The attacker exploits weaknesses in how websites implement HTTPS to force users onto unencrypted HTTP connections.
    - 🔥Session Hijacking: Also known as TCP session hijacking or cookie hijacking, this attack involves intercepting a session token or session ID to gain unauthorized access to a web server or application. The attacker can use captured session information to impersonate a legitimate user.
    - 🔥Wi-Fi Eavesdropping: Attackers can monitor Wi-Fi networks to intercept data packets transmitted between devices and access points. This method can expose sensitive information such as login credentials, financial details, and personal data.
    - 🔥Email Hijacking: In this attack, the attacker intercepts and reads email messages between parties, potentially altering the content or forwarding messages to unintended recipients. Email hijacking can lead to data leakage and compromise of sensitive information.
    - 🔥HTTPS Spoofing: Similar to SSL stripping, HTTPS spoofing involves intercepting and decrypting HTTPS traffic by fooling the victim into thinking they are communicating with a secure server. This can be achieved through techniques like issuing fake SSL certificates or exploiting weaknesses in the TLS handshake process.
    - 🔥Man-in-the-Browser (MitB): This attack involves injecting malicious code into a victim's web browser. The code intercepts and manipulates communication between the user and the websites they visit, potentially capturing sensitive information such as login credentials or financial data before it is encrypted and sent to the server.
    - 🔥DNS Hijacking: In DNS hijacking, an attacker redirects DNS queries to malicious DNS servers under their control. This allows the attacker to redirect users to malicious websites or alter DNS records to direct traffic to unauthorized IP addresses.
    - 🔥SSL/TLS Interception (TLS MitM): This attack involves intercepting SSL/TLS encrypted traffic by compromising the trusted relationship between the client and server. Attackers can use techniques like SSL stripping, where they downgrade HTTPS connections to HTTP, or employ compromised or fake SSL certificates to intercept and decrypt secure communications.
    - 🔥Bluetooth MitM Attacks: In Bluetooth MitM attacks, attackers exploit vulnerabilities in the Bluetooth protocol to intercept and manipulate data exchanged between Bluetooth-enabled devices. This can lead to unauthorized access to sensitive information or control over Bluetooth devices.
    - 🔥HTTP Session Hijacking: In this attack, the attacker steals a user's session ID or token to impersonate the user and gain unauthorized access to web applications. This can be achieved through various means, such as intercepting cookies transmitted over insecure HTTP connections or exploiting vulnerabilities in session management.
    - 🔥The temporary internet files cache if accessed by an adversary could result in a split-response attack, cache poisoning, and/or DOM XSS. 🧨Split-response attacks can cause the client to download content and store it in the cache that was not an intended element of a requested web page. Once files have been poisoned in the cache, then even when a legitimate web document calls on a cached item, the malicious item will be activated. 🧨DOM XSS may be able to access and use locally cached files to execute malicious code or exfiltrate data. 

- **Basic Security Controls**: Basic preventive measures can prevent many incidents from occurring. These include
    - keeping systems  and applications up to date
    - removing or disabling unneeded protocols and services
    - using intrusion detection and prevention systems
    - using antimalware software with up-­to-­date signatures
    - enabling both host-­based and network-­based firewalls
    - Implement configuration and system management processes. 
