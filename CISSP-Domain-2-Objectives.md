[Domain 2](#domain2-top) **Asset Security**

- Domain 2 of the CISSP exam covers asset security making up ~10% of the test 
- Asset security includes the concepts, principles, and standards of monitoring and securing any asset important to the organization. Asset: anything of value owned by the organization
- Anything that removes a vulnerability or protects against one or more specific threats is considered a safeguard or a countermeasure. The annual costs of safeguards should not exceed the expected annual cost of asset value loss.
- The Asset Security domain focuses on collecting, handling, and protecting information throughout its lifecycle; the first step is classifying information based on its value to the organization. Classification should be conducted based on the ✏️value of the data to the organization, its ✏️sensitivity, and the amount of ✏️harm that could result from exposure of the data. When data is stored in a mixed classification environment, it is typically classified based on the highest classification of data included.
- **Asset lifecycle**: phases an asset goes through, from creation (or collection) to destruction
- 🔴**Risk** is the _possibility, probability, chance or likelihood_ that a threat will exploit a vulnerability to cause harm to an asset and the severity of damage that could result. e.g damage to equipment.
     - Risk = Threat ✖️ Vulnerability
     - 🍋**Threat** is any _potential occurrence_ that may cause an undesirable or unwanted outcome for an organization or for a specific asset. e.g threat/likelihood of fire
     - 🍋**Vulnerability** is the _weakness_ in an asset, or the absence or the weakness of a safeguard or countermeasure. e.g lack of fire extinguishers
     - 🍋**Exposure** is being _susceptible_ to asset loss because of a threat; there is the possibility that a vulnerability can or will be exploited. It is the presence of a vulnerability when a related threat exists. - Every instance of exposure is a risk.
     - 🍋**Risk Analysis**: is simply the evaluation of threats against assets in regard to rate of occurrence and levels of consequence.
     - 🍋**Risk Assessment**: A risk assessment would help identify which controls are needed to protect the assets

[2.1](#2.1) Identify and classify information assets (OSG-9 Chpt 5)

- 2.1.1 ❄️**Data classification**
  - Managing the data lifecycle refers to protecting it from cradle to grave -- steps need to be taken to protect data when it's first created until it's destroyed
  - One of the first steps in the lifecycle is identifying and classifying information and assets, often within a security policy
  - Data classifications provide strong protection against the loss of confidentiality
  - Identifying the security classification for data and defining the requirements to protect the data is the primary purpose of data classification. It defines how to 📝protect data at rest and in transit, and how to back it up.
  - In this context, assets include sensitive data, the hardware used to process that data, and the media used to store/hold it
      - 🎄**Data categorization**: process of grouping sets of data, info or knowledge that have comparable sensativities (e.g. impact or loss rating), and have similar law/contract/compliance security needs. In the 🍎NIST SP 800-60 diagram, the process determines appropriate categorization levels resulting in security categorization and then uses that as an input to determine controls.
      - 🎄**Data Aggregation**: When discussing classification labels, data aggregation means that data classified at a higher level can be inferred by combining data at a lower classification level.
      - 🎄**Sensitive data**: any information that isn't public or unclassified, and can include anything an org needs to protect due to its value, or to comply with existing laws and regulations
      - 🎄**Personally Identifiable Information (PII)**: any information that can identify an individual, more specifically, info about an individual including
              - 🔨(1) any info that can be used to distinguish or trace an individual‘s identity, such as name, social security number, date and place of birth, mother‘s maiden name, or biometric records; and
              - 🔨(2) any other information that is linked or linkable to an individual, such as medical, educational, financial, and employment information. This would also include any other unique identifier (including a student ID number). ❗ZIP/Post code, by itself, does not uniquely identify an individual. ([NIST SP 800-122](https://csrc.nist.gov/publications/detail/sp/800-122/final)) NIST Special Publication 🍎NIST SP 800-122
      - 🎄**Protected Health Information (PHI)**: any health-related information that can be related to a specific person
      - 🎄**Proprietary data**: any data that helps an organization maintain a competitive edge
  - Organizations classify data using labels
  - 🔴**Governmental data classification**The impact to national security is more typically associated with government classification schemes. government classification labels include:  🟡TSCSU
      - ❄️**Top Secret**: if disclosed, could cause massive damage to national security, such as the disclosure of spy satellite information. It is the highest level of classification used by the government.
      - ❄️**Secret**: if disclosed, can adversely affect national security
      - ❄️**Confidential**: Confidential data is usually data that is exempt from disclosure under laws such as the Freedom of Information Act but is not classified as national security data.
      - ❄️**Sensitive but Unclassified (SBU)**: SBU data is data that is not considered vital to national security, but its disclosure would do some harm. Many agencies classify data they collect from citizens as SBU. In Canada, the SBU classification is referred to as protected (A, B, C).
      - ❄️**Unclassified**: not sensitive
  - 🔴**Commercial/Non-Governmental data classification** 🟡CPSP often takes into account the value of the data, any regulatory or legal requirements that may apply to the data, and how long the data is useful—its lifespan. non-government organizations use labels such as:
      - 🍮**For official use only**:
      - 🍮**Confidential/Proprietary**: only used within the org and, in the case of unauthorized disclosure, it could suffer serious consequences. It is the most sensitive data. Proprietary data is information that helps organizations maintain a competitive edge or that they want to keep to their own organization or a controlled set of individuals. Highest level of classified data that could cause exceptional damage to the organization if exposed. An organization’s IP or “secret sauce” could be considered proprietary data.
      - 🍮**Private**: may include personal information, such as credit card data,  patient X-ray data, and bank accounts; unauthorized disclosure can be disastrous. Private data is internal business data that shouldn't be exposed but that doesn't meet the threshold for confidential or proprietary data.  Next-highest level of classified data and shouldn’t be shared outside the organization and could cause significant damage if exposed. PII and PHI may be classified as private data.
      - 🍮**Sensitive**: needs extraordinary precautions to ensure confidentiality and integrity. Sensitive data may help attackers or otherwise create risk. Financial information, essential business contacts, and board meeting minutes would likely be classified as sensitive. Sensitive data should remain within the organization but it isn't as critical as confidential or proprietary data. For instance, a breach of profit earnings and forecasts wouldn't cause issues with customers or the public, but it could cause internal problems in the organization. Data that could cause damage to the organization if made public. The organization’s network layout and the devices it uses may be sensitive data.
      - 🍮**Public**: can be viewed by the general public and, therefore, the disclosure of this data would not cause damage. This includes the contents of websites, social media, brochures, etc.
  - There are six standard data type classifications used in either a government/military or a private sector organization in this list of options: public, private, sensitive, proprietary, critical, and confidential. Some organisations used 🍮critical and 🍮internal.
  -  It is important to protect data in all states: at rest, in transit, or in use
  -  The best way to protect data confidentiality is via use of strong encryption
- **Declassification** is the process of moving an object into a lower level of classification once it is determined that it no longer justifies being placed at a higher level. In a Bell-LaPadula implementation, for example, Only a trusted subject can perform declassification because this action is a violation of the verbiage of the star property of Bell–LaPadula, but not the spirit or intent, which is to prevent unauthorized disclosure. ✏️Ensuring that data cannot be recovered is difficult, and the time and effort required to securely and completely wipe media as part of declassification can exceed the cost of new media. ✏️Sanitization, purging, and clearing may be considered part of declassification.
   - If an organization allows media to be downgraded, the purging process should be followed, and then the media should be relabeled.

- 2.1.2 Asset Classification
  - It's important to identify and classify assets, such as systems, mobile devices etc.
  - **Classification**: derived from compliance mandates, the process of recognizing organizational impacts if information suffers any security compromise (whether to confidentiality, integrity, availability, non-repudiation, authenticity, privacy, or safety)
  - 🔥Assets are classified based on the data that they hold or process.
  - 🔥Data is classified based on its value to the organization. 
      - Data Classification identifies the value of data to an organization
      - Asset classification identifies the classification of assets based on the classification of data the assets hold or process. Assets are protected using various security controls, and these controls are selected based on the classification of the data
      - 📝Asset classifications should match data classification, i.e. if a computer is processing top secret data, the computer should be classified as a top secret asset
      - Handling requirements and tools include visual indicators like a distinctive screen background and can help employees remember what level of classification they are dealing with and thus the handling requirements that they are expected to follow.
      - In a single-level security environment, systems should be assigned the classification level of the highest classification of information they are ever expected to process.
      - Information should be classified based upon its sensitivity. This may be due to the value of the information to the organization, the damage caused if lost or compromised, or other factors.
      - **Reclassifying Data**: When the value of data changes due to legal, compliance, or business reasons, reviewing classifications and reclassifying the data is an appropriate response. Once the review is complete, data can be reclassified and handled according to its classification level. 
      - **Clearance**: relates to access of certain classfication of data or equipment, and who has access to that level or classification
      - A **Formal Access Approval Process** should be used to change user access; the process should involve approval from the data/asset owner, and the user should be informed about rules and limits
      - before a user is granted access they should be educated on working with that level of classification
      - Classification levels can be used by businesses during acquisitions, ensuring only personnel who need to know are involved in the assessment or transition
      - In general, classification labels help users use data and assets properly, for instance by restricting dissemination or use of assets by their classification
      - **Sensitive Data Scanning Tools:** To identify data that should be classified that already exists in an environment, Sensitive data scanning tools can be used. They are designed to scan for and flag sensitive data types using known formatting and structure. Social Security numbers, credit card numbers, and other regularly structured data that follows known rules can be identified and then addressed as needed. examples include Microsoft Azure Information Protection (AIP), Varonis Data Security Platform, Vormetric Data Security Platform. Some DLP solutions also offer this tool/capability as part of thier broader solution.

[2.2](#2.2) Establish information and asset handling requirements (OSG-9 Chpt 5)

- **Asset handling**: refers to secure transport of media through its lifetime
- The data and asset handling key goal is to prevent data breaches, by using:
  - **Data Maintenance**: on-going efforts to organize and care for data through its life cycle
  - 🏷️**Data Loss Prevention (DLP)**: systems that detect and block data exfiltration attempts; DLP systems can use labels on data to determine the appropriate controls to apply to the data. DLP system or software is designed to identify labeled data or data that fits specific patterns and descriptions to help prevent it from leaving the organization.
      - ⛅Inventory: Inventorying sensitive data is the first step to take prior to selecting a means of data loss prevention. By inventorying assets in terms of sensitive data, you can then figure out what you need to protect and the best way to do so.
      - ⛅Classification:  A data loss prevention (DLP) system can tag, monitor, and limit where files are transferred to. Data must be correctly 📝classified before the Data Loss Prevention (DLP) appliance can prevent it from being leaked. If data is incorrectly classified, the DLP appliance may allow sensitive information to leave the organization.
      -  There are two primary types:
     - 🩰Network-based DLP: Network-based DLP would not detect stored information unless the user transmits it over the network. Network DLP solutions inspect the content of network traffic, looking for signs of attempted data exfiltration and preventing it from leaving the network.
         - Network DLP is used to prevent sensitive information from being transmitted over the network. For example, network DLP can catch when a user places sensitive information such as credit card numbers in an email.  
     - 🩰Endpoint-based DLP: identify the presence of information on endpoint devices. Endpoint-based DLP involves preventing data loss at each endpoint by recognizing patterns or keywords. Additionally, endpoint-based DLP can be used to conduct an initial scan to recognize potentially sensitive information on a device. Endpoint-based DLP will simply prevent certain data from leaving a device, potentially warning the user of their action to prevent future misbehavior. For example, an endpoint DLP solution could prevent printing sensitive data or saving it to a USB drive.
         - Endpoint Data Loss Prevention (DLP) is an agent/program installed on an endpoint. It applies protection for data at rest and in use. It can block a user from saving sensitive data to a removable media device or printing sensitive information on an attached printer. 
     - 🩰Cloud-Based DLP: Cloud DLP solutions are network DLP solutions designed to operate in cloud environments.
  - 🏷️**Marking (AKA labeling)**: sensitive information/assets ensures proper handling (both physically and electronically).
     - Security labeling identifies the classification of data such as sensitive, secret, and so on. Media holding sensitive data should be labeled. Similarly, systems that hold or process sensitive data should also be marked
     -  Labeling assets would allow all employees to know the importance of the information or items and handle them appropriately.
     -  Labeling all assets would allow for their classification and an organized manner in which to determine the values of items and data within a company.
     - Media is typically labeled with the highest classification level of data it contains. This prevents the data from being handled or accessed at a lower classification level.
     - Systems and media should be labeled with the highest level of sensitivity that they store or handle.
     - labels can be as granular and custom as required by the org
     - Data labeling can help ensure that controls are applied to the right systems and data.
     - Data labels are crucial to identify the classification level of information contained on the media, and labeling data at creation helps to ensure that it is properly handled throughout its lifecycle.
     - Requiring all media to have a label means that when unlabeled media is found, it should immediately be considered suspicious. This helps to prevent mistakes that might leave sensitive data unlabeled.
     - Tagging: Tags that include information about the lifespan of the data and when it has expired can help with lifecycle management processes. Tags can be as simple as timestamps, or they can include additional metadata like the data type, creator, or purpose that can help inform the retention and disposal process. Metadata tagging allow organisations to use technical tools like DLP and DRM to handle and track data, based on its type and content. Asset tagging is used to make sure that individuals working with assets can determine the security level or practices they require.
  - 🏷️**Data Collection Limitation**: Providing ✏️consent, or agreeing to data collection and use, is important in many data collection scenarios and may be required by law. ✏️Only required data is collected, that individuals are made aware of the data collection, and that they consent to the collection. Similarly, data should only be collected ✏️lawfully and via fair methods. Prevent loss by not collecting unnecessary sensitive data. 🥇Sometimes it is better to collect limited information🥇
  - 🏷️**Data Location**: keep duplicate copies of backups, on- and off-site. Data location, particularly at rest, may drive compliance requirements based on local or national laws. This concern drives the majority of data location concerns
  - 🏷️**Storage**: define storage locations and procedures by storage type; use physical locks for paper-based media, and encrypt electronic data. It is cost effective to purchase high-quality media to contain sensitive data becuase the value of the data often far exceeds the cost of the media. This makes more expensive media that may have a longer life span or additional capabilities like encryption support a good choice for sensitive data. Backup media should be protected with the same level of protection afforded the data it contains e.g using a secure offsite storage facility. Backup tapes should be removed from service when they begin to generate errors. 
- 📕**sanitization** is a series of processes that removes data from a system or media while ensuring that the data is unrecoverable by any means. Sanitization methods (such as clearing, purging, and destroying) help ensure that data cannot be recovered. 📝Note: Downgrading systems and media is rare due to the difficulty of ensuring that sanitization is complete. The need to completely wipe (or destroy) the media that systems use means that the ✏️cost of reuse is often significant and may exceed the cost of purchasing a new system or media i.e sanitazation cost ▶ cost of new media.
  - ✴️**Destruction**: destroy data no longer needed by the organization; policy should define acceptable destruction methods by type and classification ([see NIST SP-800-88 for details](https://csrc.nist.gov/publications/detail/sp/800-88/rev-1/final)) ✏️Physical destruction: used for SSD/electronic components, or in combination with other less-secure methods. Due to problems with remnant data, the U.S. National Security Agency requires physical destruction of SSDs. Incineration, pulverizing, crushing, shredding, and disintegration all describe data destruction. Physical destruction is the most secure method of deleting data on optical media such asa DVD. 
      - 🍎**SSD**: Spare sectors, bad sectors, and space provided for wear leveling on SSDs (over provisioned space) may all contain data that was written to the space that will not be cleared when the drive is wiped. This is a form of data remanence and is a concern for organizations that do not want data to potentially be accessible. Many wiping utilities only deal with currently addressable space on the drive.
          - To prevent accidental data disclosure due to wear leveling on an SSD device before reusing the drive, encrypting data on SSD drives.
          - SSDs cannot be degaussed, and wear leveling space cannot be reliably used to hide data. These spaces are still addressable by the drive, although they may not be seen by the operating system.
          - The two valid options for destroying data on SSD drives are ATA Secure Erase and destruction.
          - Destruction is the best method for SSD drives. 
      - 🍎**shredding**: is a type of physical destruction. Though this term is sometimes used in relation to overwriting of data, here shredding  refers to the process of making unrecoverable any data printed on hard copy or on smaller objects, such as credit cards, floppy or optical disks. There are industrial shredders capable of shredding larger devices like servers and hard disks. Shredding is the most thorough way to ensure data cannot be recovered since it leaves the media and data unreadable and unrecoverable.
  - ✴️**Clearing**: removal of sensitive data from a storage device such that there is assurance data may not be reconstructed using normal functions or software recovery or software recovery utilities; over-writing existing data or scrubbing un-needed data. Clearing describes preparing media for 📝reuse in same-security/sensitivity level. When media is cleared, unclassified data is written over all addressable locations on the media. Once that's completed, the media can be reused. Clearing (sometimes called overwriting) overwrites the disk drive with different bits in three separate passes.
      - Clearing is defined in 🍎NIST SP 800-88 as the application of logical techniques to sanitize data in all user-addressable storage locations for protection against simple non-invasive data recovery techniques, typically applied through the standard Read and Write commands to the storage device, such as by
rewriting with a new value or using a menu option to reset the device to the factory state.  
  - ✴️**Purging (Sanitization)**: removal of sensitive data from a system or device with the intent that data cannot be reconstructed by any known technique; usually refers to mutliple clearing passes combined with other tools-- not considered acceptable for top secret data.
      - Purging overwrites the media with random bits multiple times and includes additional steps to ensure that data is removed. It ensures there isn’t any data remanence.
      - When done properly, purged data is not recoverable using any known methods. 
      - Purged media can then be reused in less secure environments. 
      - Purging is a more intensive form of clearing for 📝reuse in lower-security areas
      - The 🍎NIST SP 800-88 process for sanitization and disposition shows that media that will be reused and was classified at a moderate level should be purged and then that purge should be validated. Finally, it should be documented. ✏️Validation processes are conducted to ensure that the sanitization process was completed, avoiding data remanence. A validation form  helps to ensure that each device has been checked and that it was properly wiped, purged, or sanitized.
          - Purging is defined in NIST SP 800-88 as using either physical or logical techniques that render target data recovery infeasible using state-of-the-art laboratory techniques.. If a physical technique is used, then the drive can not be used again. However, the drive could be used again if a logical technique is used. When a drive is purged, the data on that drive is infeasible even if using state-of-the-art laboratory techniques. So the drive could be used again in an unclassified capacity if logically purged
  - ✴️**Overwriting**: Same as Clearing and also known as 🔥Purging. Overwriting the disks multiple times will remove all existing data. This is called purging, and purged media can then be 📝re-used again.
       - Single-pass wipe: involves overwriting the entire hard drive with random data once (e.g replacing with 0s and 1s). 
       - Multi-pass wipes: involves overwriting the drive multiple times, and is more secure against advanced recovery techniques.
       - Zero fill wipes a drive by replacing data with zeros
  - ✴️**Degaussing**: used on ✏️magentic media including Tape media, floppy disks, Magnetic Stripe Cards, Zip drives, VHS and audio cassette tapes, and hard disk drives. Degaussing the disks often damages the electronics but doesn’t reliably remove the data. Tapes can be erased by degaussing, but degaussing is not always fully effective because it could allow for the platters to be installed on a new drive and data to be read. Although they are only effective on magnetic media, so they can't be used on media such as Digital Video Disks (DVDs), Compact Discs (CDs), or Solid State Drives (SSDs).
  - ✴️**Erasing**: usually refers to a delete operation on media, leaving data remanence. It rarely remove the data from media but instead mark it for deletion. Erasing is the deletion of files or media and may not include all of the data on the device or media, making it the 📝worst choice here. Erasing, which describes a typical deletion process in many operating systems, typically removes only the link to the file and leaves the data that makes up the file itself. The data will remain in place but not indexed until the space is needed and it is overwritten. 
  - ✴️**Cryptographic Erasure**: AKA cryptoshedding, basically destroying encryption key; may be only secure method for 🧠cloud storage. Cryptoshredding, or cryptographic erasure, destroys the cryptographic keys used to protect encrypted data. This runs the risk that the encryption algorithm may be broken in the future or a backup key might be discovered. However, it is often the only available option in cloud environments.

[2.3](#2.3) Provision resources securely (OSG-9 Chpt 16)

- The primary purpose of security operations practices is to safeguard assets such as information, systems, devices, facilities, and apps; these practices help to identify threats, vulnerabilities, and implement controls to reduce the risk to these asssets
- Implementing common security operations concepts, along with performing periodic security audits and reviews demonstrates a level of due care
- **Need-to-know**: a principle that imposes the requirement to grant users access only to data or resources they need to perform assigned work tasks
- **Least privilege**: a principle stating that subjects are granted only the privileges necessary to perform assigned work tasks and no more

- 2.3.1 Information and asset ownership
  - ☪️**Data owner**: the person who has ultimate organizational responsibility for data; usually sr. manager (CEO,president, dept. head); data owners typically delegate data protection tasks to others in the org 
  - ☪️**Asset Owner**: identifies the individual(s) responsible for protecting the asset or for delegating the task of protecting the asset. Asset owners are responsible for assets in an organization, including their procurement, management, and life cycle.

- 2.3.2 Asset inventory (e.g., tangible, intangible)
  - **Inventory**: complete list of items.  In most organizations, changing processes so that new systems and devices are added to inventory before they are deployed is the first step in making sure asset inventories are current. A system inventory is most frequently used to associate individuals with systems or devices. This can help when tracking their support history and aids in provisioning the proper tools, permissions, and data to a system. While it can be a lot of work, the most complete inventory of active systems and devices can be created by determining what is connected to the network by looking at logs, and then finding those assets. Barcodes & RFID tags are a common solution for tracking hardware assets and equipment. RFID tags can be queried wirelessly at varying ranges depending on the tags and may be built-in to hand-held readers or even included in doorways or arches to track items as they enter or leave a facility. Visual inventory relies on staff checking items, MAC addresses are hardware addresses for networked devices.
  - 🍏**Tangible assets**: include hardware and software assets, cables, and buildings owned by the company. Tangible asset inventories include physical items owned by the organization. 
  - 🍏**Intangible assets**: things like Patents, databases, and formulas, copyrights, a company’s reputation, Intellectual property, files stored on a server, and other assets representing potential revenue
    - an org should keep track of intangible assets, like intellectual property, patents, trademarks, and company’s reputation, and copyrights to protect them
    - To protect intangible inventories (like intellectual property, patents, trademarks, and company’s reputation, and copyrights), they need to be tracked
    - note: patents in the US are valid for 20 years
  - 🍏**Personnel Assets**: Employees

- 2.3.3 Asset management
  - Asset management refers to managing both tangible and intangible assets; this starts with inventories of assets, tracking the assets, and taking additional steps to protect them throughout their lifetime
  - An effective asset management program helps prevent losses by tracking and protecting assets throughout their lifetime. 
  - **Accountability**: ensures that account management has assurance that only authorized users are accessing a system and using it properly
  - 🍏**Hardware assets**: IT resources such as computers, servers, routers, switches and peripherals
    - use an automated configuration management system (CMS) to help with hardware asset management
    - An automated configuration management system CMS connects to systems over the network and can assist with hardware asset management by verifying systems are in the network and operational. 📝 A CMS can verify changes are implemented, but it doesn't include the entire change management process.
    - use barcodes, RFID tags to track hardware assets
        - Proximity Devices
            - 🎈Field-powered proximity device: RFID (radio-frequency identification) is effectively a field-powered proximity device. It has an antenna to generate current from a magnet field provided by an external source.
            - 🎈Passive proximity device is often a magnet;
            - 🎈Self-powered proximity device has its own battery
            - 🎈The use of an electromagnetic coil inside the card
  - 🍏**Software assets**: operating systems and applications 
    - important to monitor license compliance to avoid legal issues
    - software licensing also refers to ensuring that systems do not have unauthorized software installed
- 📁**IT Asset Management (ITAM) Tiers**
     -  🎈Tier 0 Assets: Tier zero is reserved for 📝essential assets. These are assets that handle sensitive data and are always required to be available. An example would be a router or file server that an entire organization relies on.
         - Tier 0: Core Infrastructure Assets includes the most critical and foundational IT assets that the organization depends on for all operations. If these assets fail, they could cause a complete outage of services. Examples: Data centers, Primary network infrastructure (e.g., core routers and switches), Critical servers (e.g., DNS, authentication servers), Enterprise storage systems. Management Focus is High availability, redundancy, continuous monitoring, and the fastest possible recovery times.
     -  🎈Tier 1 Assets:A tier one asset is important, but not equally for every department; for some departments, it may be more important than for others. File shares or local network devices used by a group of employees would be an example of this.
         - Tier 1: Critical Business Assets are essential assets that directly support critical business operations. While not as foundational as Tier 0, their failure would significantly disrupt business functions. Examples: Enterprise applications (e.g., ERP, CRM systems), Key databases, High-priority user devices (e.g., devices used by executives or essential personnel), Cloud platforms hosting critical applications. Management Focus is on Strong security controls, regular backups, disaster recovery planning, and user access management.
     -  🎈Tier 2 Assets: Tier two classifications generally describe  📝non-critical assets that could impact an individual. Workstations and phones would meet the criteria for a tier two classification.
         - Tier 2: Important but Non-Critical Assets are important for business operations but can tolerate some downtime without causing significant disruption. Examples: Secondary databases and servers, Departmental applications and tools, Standard user devices (e.g., employee workstations, mobile devices), Development and testing environments, Management Focus: Regular maintenance, security patching, and backup with less stringent recovery time objectives.  
- 📁**Understanding System Tiers in Reference Architecture**
    - 🎈Tier 1: Presentation Tier: This tier is responsible for the user interface and interaction. It includes components like web browsers, mobile apps, or any front-end system where users interact with the application. Security at this tier involves ensuring that user inputs are validated, secure authentication mechanisms are in place, and communication with back-end systems is encrypted.
    - 🎈Tier 2: Application/Logic Tier: The application tier handles the core logic, processing, and business rules of the system. It processes user inputs, interacts with the data tier, and sends results back to the presentation tier. This tier should be protected against application-level threats such as SQL injection, cross-site scripting (XSS), and logic flaws. Proper access controls and secure coding practices are essential here.
    - 🎈Tier 3: Data Tier: The data tier is responsible for storing and managing data. It includes databases, file storage systems, and other data repositories. Protecting data at this tier involves securing databases against unauthorized access, encrypting data at rest, and ensuring that proper backup and recovery mechanisms are in place.
- 📁**NIST Special Publication 1800-5**: 
     -  🎈Tier 1 Systems: Tier 1 systems collect, store, and analyze the data that they receive from the Tier 2 systems. They allow users to analyze the data and to visualize it for further analysis. e.g splunk
     -  🎈Tier 2 Systems: Tier 2 is composed of systems that each perform a unique task. Each Tier 2 system is fully capable of collecting, storing, and analyzing data pertaining to its unique task. The middle tier systems filter relevant and desired data from the raw data collected and forward this data to the analysis engine and visualization tool for further analysis e.g Fathom, Bro, Snort, OpenVAS, WSUS, BelManage, BelManage Data Analytics, Puppet, openswan, iStar/C-Cure Controller
     -  🎈Tier 3 Systems: Tier 3 systems are the assets (end points) on the enterprise network that are owned by the enterprise, such as workstations, switches, servers, users’ laptops, virtual machines, and other devices. All enterprise assets are monitored from the start of their lifecycle until disposal by the systems in the Tier 2. Device location, owner, installed software catalog, current security vulnerabilities, and abnormal traffic activity are captured to allow for better visibility by administrators e.g AD, camera, PC, laptops, mobile phones, router and firewall

[2.4](#2.4) Manage data lifecycle (OSG-9 Chpt 5)
- 2.4.1 Data roles (i.e., owners, controllers, custodians, processors, users/subjects)
  - 🔴**Business/Mission Owners**:-  Typically own processes or programs. they ensure that all operations fit within the business goals and mission.
      - This task includes ensuring that collected data is necessary for the business to function. Collecting unnecessary data wastes time and resources.
      - Because the business/mission owner is primarily concerned with the overall business
      - Conflicts between data owners, data custodians, and system owners may need to be resolved by the business/mission owner, who will need to make the best decision for the organization.
      - business owners are tasked with ensuring that systems are fulfilling their business purpose
      - Business owners are most likely to select and apply COBIT to balance the need for security controls against business requirements.
      - Business owners have to balance the need to provide value with regulatory, security, and other requirements. This makes the adoption of a common framework like COBIT attractive.
      - Business owners are typically project or system owners who are tasked with making sure systems provide value to their users or customers.
      - Mission owners are typically program or information system owners.

  - 🔴**Data Owner**: the entity that collects/creates the PII and is legally responsible and accountable for protecting it and educating others about how to protect the data through dissemination of intellectual property rights documentation, policies and regulatory requirements, specific protective measures that are expected of custodians, and compliance requirements.
       - Data owners are tasked with making decisions about data, such as who receives access to it and how it is used.
       - The data owner bears responsibility for categorizing information systems 
       - The data owner has ultimate responsibility for data belonging to an organization and is typically the CEO, president, or another senior employee
       - Data owners are more likely to ask that those responsible for control selection identify a standard to use if they are not also acting as business owners.
       - The data owner is responsible for determining who needs to know what and the data that they are responsible for.
       - The data owner then determines who should have access to that data. This would include assigning access to individuals based on their roles or attributes.
       - The data owner is also responsible for determining the data’s classification level. This would then allow the owner to assist in identifying any additional security controls that are needed to protect that data.
       - They are responsible for classifying the data that they own as well as assisting with or advising the system owners on security requirements and control selection
       - The data owner is the person responsible for classifying data, delegates selection of the required controls for each classification to system owners, and selecting baseline security standards for the organization.
       - Data owner roles is ultimately responsible for due diligence in protecting the company's data. They may be liable for negligence if they fail to perform due diligence in establishing and enforcing security policies to protect and sustain sensitive data.
       - data owner is the person respsonible for classifying, categorizing, and permitting access to the data; the data owner is the person who is best familiar with the importance of the data to the business
       - The data owner sets the rules for use and protection of data. In a DAC system, they decide who gets access.
           - 🛰️**GDPR**: Data Owner has overarching responsibility for data and compliance under the GDPR
           - A data owner is ultimately responsible for data and controls its classification, labeling, and disclosure.
           - This is often the CEO or other senior management.
   - 🔴**System owner**: controls the computer storing the data; usually includes software and hardware configurations and support services (e.g. cloud implementation)
    - system owners are responsible for the systems that process the data
    - The system owner creates the system security plan,
    - system owner is responsible for system operation and maintenance, and associated updating/patching as well as related procurement activities
    - NIST SP 800-18 describes system owner responsibilities that include helping to develop system security plans, maintaining the plan, ensuring training, and identifying, implementing, and assessing security controls. 
    - Develops a system security plan
    - Ensures that system users receive appropriate security training
    - Identifies and implements security controls
    - system ownership is an important part of making sure baselines are implemented and maintained
    - Typically, system owners, such as a department head, delegate authority to system administrators/custodians
  - 🔴**Data controller**: decide what data to process and how to process it
    - the data controller is the person or entity that controls the processing of the data - deciding what data to process, why this data should be processed, and how it is processed
    - e.g. a company that collects personal information on employees for payroll is a data controller (but, if they pass this info to a third-party to process payroll, the payroll company is the data processor, see below)
    - The data controller is the entity that makes decisions about the data they are collecting.
    - A data controller decides what data to process and directs the data processor to process the data.
    - Generally, the data controllers set policies and are the senior managers in a company. A data owner would then decide who will be given access to that data based on prior-determined policies and also maintain the data throughout the life cycle, including classifying that piece or set of data.
        - 🛰️**GDPR**: A data controller is responsible for defining how different types of data are collected and processed.
        - Data controllers are also often senior management.
  - 🔴**Data processor**: 📯🅾️an entity working on behalf (or the direction) of the data controller, that processes PII; they have a responsibility to protect the privacy of the data and not use it for any purpose other than directed by the data controller; **OR** 📯🅾️ Data processor is any system used to process data. 
    - a data controller can hire a third party to process data, and in this context, the third party is the data processor; data processors are often 🧠third-party entities that process data for an org at the direction of the data controller
    - Third-party organizations that process personal data on behalf of a data controller are known as data processors. The organization that they are contracting with would act in the role of the business or mission owners, and others within the third party organization would have the role of data administrators, granting access as needed to the data based on their operational procedures and data classification.
        - 🛰️**GDPR**: Data Process is "a natural or legal person, public authority, agency, or other body, which processes personal data soley on behalf of the data controller"
        - A data processor performs the actual processing of data, a role delegated to them by the data controller. Data processors are employees within the department who use the collected data.
        - GDPR also restricts data tranfers to countries outside EU, with fines for violations
        - many orgs have created dedicated roles to oversee GDPR data laws are followed
  - 🔴**Data custodian**: a custodian is delegated, from the system owner, day-to-day responsibilities for properly storing and protecting data;
      - responsible for the protection of data through maintenance activities, backing up and archiving, and preventing the loss or corruption and recovering data.
      - They include ✏️IT Staff in an information technology (IT) department who are delegated responsibility for day-to-day tasks.
      - Custodians are trusted to ensure the day-to-day security of the data and should do so by ensuring that the baseline is met and maintained.
      -  The custodian has the responsibility of protecting the data they have in their possession. That includes following the corporate policies, procedures, standards, and so on. They ⛔do not determine the access levels
      - Custodians are tasked with the day-to-day monitoring of the integrity and security of data. 
      - He/She is responsible for the technical environment, including things like database structures, performing data backup and the technical implementations of data policies.
      - A custodian is defined as someone entrusted with guarding and protecting something. Data custodians are responsible for maintaining the data and ensuring its availability for the data owner.
      - The data custodians are responsible for backing up data to ensure its security.
      - Controls are scoped and tailored, applied and enforced by Custodians.
      - custodians implement the controls
      - custodians are granted rights to perform day-to-day tasks when handling data
          - 🛰️**GDPR**: A data custodian handles day-to-day protection and management of data, a role delegated to them by the data owner.
          - Data custodians are usually within the IT/security department.
          -  They are DIRECTLY responsible for the security of sensitive data under GDPR
  - 🔴**Administrators** have the rights to apply the permissions to access and handle data.
      - The system administrators can act in the roles of data administrators who grant access and will also act as custodians who are tasked with the day-to-day application of security controls such as providing/granting/managing user access.
  - 🔴**Data Steward**: a newer concept related to users of the data; those who use the data for the business purpose. In many organizations, data stewards are internal roles that oversee how data is used. They can be referred to as data custodians. Data stewards provide oversight and data governance.
  - 🔴**Security administrator**: responsible for ensuring the overall security of entire infrastructure; they perform tasks that lead to the discovery of vulnerabilities, monitor network traffic and configure tools to protect the network (like firewalls and antivirus software) 
    - security admins also devise security policies, plans for business continuity and disaster recovery and train staff
  - 🔴**Supervisors**: responsible for overseeing the activities of all the above entities and all support personnel; they ensure team activities are conducted smoothly and that personnel is properly skilled for the tasks assigned
  - 🔴**Data Object**: In the subject/object model, the object is the resource being requested by a subject.
  - 🔴**Data Subject**: the person who the information is about.
  - 🔴**Users**: any person who accesses data from a computer device or system to accomplish work (think of users as employees or end users)
    - users should have access to the data they need to perform tasks; users should have access to data according to their roles and their need to access info
    - must comply with rules, mandatory policies, standards and procedures
    - users fall into the category of subjects, and a subject is any entity that accesses an object such as a file or folder 
      - note that subjects can be users, programs, processes, services, computers, or anything else that can access a resource (OSG-9 Chpts 8, 13)
- 2.4.2 Data Collection
  - One of the easiest ways of preventing the loss of data is to simply not collect it
  - **Data Collection Guideline**: if the data doesn't have a clear purpose for use, don't collect it, and don't store it; this is why many privacy regulations mention limiting data collection. Providing consent, or agreeing to data collection and use, is important in many data collection scenarios and may be required by law. A best practice when collecting customer data is to limit the amount of data collected to only what is needed. (minimization)
- 2.4.3 Data location
  - **Data Location**: in this context, refers to the location of data backups or data copies
  - If a company's system is on-prem, keeps data on-site, but regularly backups up data, best practice is to keep a backup copy on site and backup copy off-site
  - Consider distance between data/storage locations to mitigate potential mutual (primary and backup) damage risk
  - Some financial guidelines require a mirrored site to be a minimum distance away, such as at least 20 miles away. 

- 2.4.4 Data maintenance
  - 🔵**Data Maintenance**: managing data through the data lifecycle (creation, usage, retirement); data maintenance is the process (often automated) of making sure the data is available (or not available) based on where it is in the lifecycle. During the data maintenance phase of a typical data lifecycle, activities like data scrubbing occur to remove unneeded, incorrect, or out-of-date data.
  - Data lifecycle:  Data Maintenance ↪️🔄 Data Collection ➡️ Data Analysis ➡️ Data Usage ➡️ Data Retention ➡️ Data Destruction ↩️🔄 Data Maintenance 
  - In a typical data lifecycle, collection is the first stage. Once collected, data can be analyzed, used, stored, and disposed of at the end of its useful life. Policies may be created at any time, and organizations often have data before they have policies. Labels are added to data during the analysis, usage, or retention cycle.
  - Ensuring appropriate asset protection requires that sensitive data be preserved for a period of not less than what is business-required, but for no longer than necessary
  - Encrypt sensitive data
  - Safeguard assets via basic security controls to enforce appropriate levels of confidentiality, integrity and availability and act per security policies, standards, procedures and guidelines
- 2.4.5 Data retention
  - Retention requirements apply to data or records, media holding sensitive data, systems that process sensitive data, and personnel who have access to sensitive data. Record retention policies define the amount of time to keep data, and laws or regulations often drive these policies.
    - 🔵**Record Retention**: Record retention is the process of retaining and maintaining information for as long as it is needed. 
      - note: a current trend in many orgs is to reduce legal liabilities by implementing short retention policies with email
      - A data storage policy describes how and why data is stored.
      - 💥**Record retention policies** define the amount of time to keep data, and laws or regulations often drive these policies. Record retention policies define how data should be maintained within the organization. It includes what type of data to keep, the length of retention, how to maintain the data, and how to destroy the information when it is no longer needed. Careful work should be put into ensuring the policy is a match for business needs. Having 📝legal counsel in the process is a good idea as there are 📝laws and 📝regulations that have a retention requirement or a deletion requirement.
      - A data retention policy can help to ensure that outdated data is purged, removing potential additional costs for discovery, and reducing the amount of data that may need to be produced for lawsuits. Many organizations have aggressive retention policies to both reduce the cost of storage and limit the amount of data that is kept on hand and discoverable.
      - Laws and regulations that apply to an organization often dictate how long to save data. Still, all organizations should keep data as long as they are required to do so.
      - Always consult the organization's record retentions policy to determine the appropriate length of time to preserve records
          - 📝The organization should reference its user agreement documents along with government regulations. For example, they may want to delete data when it is no longer needed. This would allow them to follow the European Union (EU) General Data Protection Regulation (GDPR) Article 5, which states that data is to be retained only for the amount of time it is required. Additionally, this would allow the organization to avoid legal issues in the future if the data they stored was involved in a crime, but the data is no longer stored with them. By having this data still in storage, the organization may have to oblige court orders to provide copies of all data, which could be burdensome to the company.
  - Three fundamental retention policy questions:
    - ⚒️**How to retain**: data should be kept in a manner that makes it accessible whenever required; take taxonomy (or the scheme for data classification) into account
    - ⚒️**How long to retain data**: general guidelines for business data is 7 years (but can vary by country/region/regulation)
    - ⚒️**What data**: to retain per org requirements
- 2.4.6 Data remanence
  - 🔵**Data Remanence**: the data remaining on media after the data is supposedly erased. Data remanence refers to the residual data that can be recovered and reassembled when media is not appropriately sanitized.  It is data remaining on media after typical erasure; Remanence describes data left on media after an attempt is made to remove the data. 
    - typically refers to data on a hard drive as residual ✏️magnetic flux or ✏️slack space (unused space within a disk cluster)
      - note that many OSs store files in clusters, which are groups of sectors (the smallest storage unit on a hard disk drive)
      - Slack space is the unused space in a cluster. However, slack space can still contain data previously stored in memory. Clusters are groups of sectors. One way of eliminating data remanence is by overwriting clusters with random bits. 
    - if media includes any type of private and sensitive data, it is important to eliminate data remanence
    - note that some OSs fill slack space with data from memory, which is why personnel should never process classified data on unclassified systems
    - Remnant data is data that is left after attempts have been made to remove or erase it. itis also referred to as residual data.
    - The standard methods for clearing magnetic tapes, according to the NIST Guidelines for Media Sanitization, are overwriting the tape with nonsensitive data, degaussing, and physical destruction via shredding or incineration. Reformatting a tape does not remove remnant data.
    
- 2.4.7 Data destruction
  - Destroy sensitive data when it is no longer needed
  - An org's security or data policy should define the acceptable methods of destroying data based on the data's classification
  - a degausser can be used on a hard disk drives/magnetic media
  - the best SSD wiping method is destruction -- even when using manufacturers SSD wiping tools, data can remain, and therefore the best SSD wipe method is destruction
  - **Defensible destruction**: eliminating data using a controlled, legally defensible and regulatory compiant way

[2.5](#2.5) Ensure appropriate asset retention (e.g. End-of-Life EOL, End-of-Support (EOS)) (OSG-9 Chpt 5)
- Hardware: even if you maintain data for the appropriate retention period, it won’t do you any good if you don’t have hardware that can read the data
- Personnel: beyond retaining data for required time periods and maintaining hardware to read the data, you need personnel who know how to operate the hardware to execute restoraton processes

- 🍮**End-Of-Life (EOL)**: often identified by vendors as the time when they stop offering a product for sale. At the end of their life (EOL) for workstations, they should be destroyed. Destruction is the most complete method of ensuring that data cannot be exposed, and organizations often opt to destroy either the drive or the entire workstation or device to ensure that data cannot be recovered or exposed.
- 🍮**End-Of-Support (EOS)/End-Of-Service-Life (EOSL)**: often used to identify when support/update ends for a product. 📝Instead of vendor, a company company can intentionally end support and needs to address what happens to the devices next—secure disposal, destruction, or re-sale—depending on data security requirements and policies set by the company. For example, A company’s policy of issuing new phones every two years suggests that they have a planned schedule for replacement, which can be seen as aligning with an EOS policy, even though the old phones are still functional and receiving updates.
- EOL,EOS/EOSL can apply to either software or hardware
- 🍮**End of development** occurs when no new software updates or patches are being released
[2.6](#2.6) Determine data security controls and compliance requirements (OSG-9 Chpt 5)

- You need security controls that protect data in each possible state: at rest, in transit or in use
- Each state requires a different approach to security; note that there aren’t as many security options for data in use as there are for data at rest or data in transit

- 2.6.1 Data states (e.g., in use, in transit, at rest): **🥇Strong encryption is the only method that would protect data at all stages, in transit, rest, or even at the end of the life cycle.**
  - The three data states are at rest, in transit, and in use
    - 🔴**Data at rest**: any data stored on media such as hard drives or external media: Protecting Data at Rest: AES encryption, Access Control, redundancy/backup, Bitlocker, FileVault, symmetric encrytion e.g Serpent, IDEA.  Data breaches cause the greatest reputational damage as a result of threats to data at rest.  Data at rest with a high level of sensitivity is often encrypted to help prevent this.  Disk-level encryption and column-level encryption protects data at rest.
    - 🔴**Data in transit**: any data transmitted over a network: Protecting Data in Motion: Data in transit is data that is traversing a network or is otherwise in motion. TLS, VPNs, and IPsec tunnels are all techniques used to protect data in transit. TLS encryption, email encrytion (SMIME, PGP), IPSEC, VPN, SSH
    - Network-level encryption protects data in transit. 
        - ⭐SSH-2: Provides improved security with more robust encryption e.g AES and key exchange mechanisms. It also adds support for simultaneous shell sessions over a single SSH connection and Supports optional compression of data to improve  performance
        - ⭐IPsec: Provides strong encryption and is used in many VPNs. Operates at the Network layer. Transport Mode: Encrypts only the payload of the IP packet, leaving the header intact. Tunnel Mode: Encrypts both the payload and the header, creating a new IP header.
        - ⭐L2TP: Provides tunneling but requires IPsec for encryption. Operates at the Data link layer. perates at the data link layer and provides a framework for tunneling protocols. L2TP alone does not offer encryption or security but is typically used with IPsec to provide these features (L2TP/IPsec). often used in Point-to-Point Protocol PPP scenarios.
        - ⭐L2TPV3: L2TPv3 (Layer 2 Tunneling Protocol version 3) is an extension of L2TP (Layer 2 Tunneling Protocol) designed to provide more advanced features and improved capabilities. Extends L2TP to support tunneling of Layer 2 frames over IP networks, similar to the original L2TP but with additional features and improvements. Primarily used to carry Layer 2 traffic (e.g., Ethernet frames) over IP networks. It is useful for applications that require the transmission of Layer 2 protocols across an IP backbone.
        - ⭐PPTP: Older and less secure, generally not recommended. Operates at the Data link layer. Encapsulates PPP (Point-to-Point Protocol) frames into IP packets for transmission over the internet. PPTP itself provides encryption but is considered less secure compared to more modern protocols.
        - ⭐SSL/TLS: Strong encryption, often used in remote access VPNs. Operates at the Application link layer. Often used in remote access VPNs (SSL VPNs) and for securing web traffic (HTTPS).
        - ⭐IKEv2: Enhances IPsec with better performance and security. Operates at the Network layer (enhances IPsec).  Provides strong security and supports features like NAT traversal and seamless connection resumption.
        - ⭐OpenVPN: Flexible and secure with strong encryption, highly configurable. Operates at the Application layer (uses SSL/TLS).      
      - encryption methods protect data at rest and in transit
      - Asymmetric encryption (along with symmetric encryption) protects data in transit.
    - 🔴**Data in use**: data in 📝memory or in buffer and used by an application. Data in a 📝CPU register/cache is also in use.
      - Data in use is data that is in a temporary storage location while an application or process is using it. Thus, data in memory is best described as data in use or ephemeral data. 
      - applications should flush memory buffers to remove data after it is no longer needed: Protecting Data in Use (📝Harder to protect): RAM/memory data: The most difficult location to secure for encryption keys and similar highly sensitive information is in active memory because the data needs to be decrypted to be used. 
      - pervasive encryption,
      - prevent shoulder surfing,
      - parameter checking against buffer overflow,
      - Address space layout randomization (ASLR) is a memory-protection process for operating systems (OSes) that guards against buffer-overflow attacks by randomizing the location where system executables are loaded into memory
      - Purging memory buffers removes all remnants of data after a program has used it.
      - keeping the systems patched, maintaining a standard computer build process, and running anti-virus/malware are typically the real-world primary protections for data in use
      - Homomorphic encryption algorithms may be used to protect data in use. **Homomorphic encryption** it allows for the creation and interaction of otherwise hidden code that may be sensitive or proprietary. Homomorphic encryption is a form of encryption that allows computations to be performed on encrypted data without decrypting it first. This means that data can remain encrypted while still being processed or analyzed, and the results can be decrypted to reveal the outcome of the computation.
- 📁**Memory**: Memory is a series of on/off switches representing bits: 0s (off) and 1s (on). Memory may be chip based, disk based, or tape based.
    - RAM is random-access memory: “random” means the CPU may randomly access or jump to any location in memory.
    - Sequential memory, such as tape, must sequentially read memory, beginning at offset zero, to the desired portion of memory.
    - Volatile memory, such as RAM, loses integrity after a power loss; they  lose their contents when the computer is powered off.
    - Nonvolatile memory (such as read-only memory (ROM), disk, or tape) maintains integrity without power  
- **Different types of Memory**:
    - ❄️Cache memory: Cache memory is the fastest system memory, required to keep up with the CPU as it  fetches and executes instructions. The data most frequently used by the CPU is stored in cache memory. The fastest portion of the CPU cache is the 📝register file, which contains multiple registers. Registers are small storage locations used by the CPU to store instructions and data. The next fastest form of cache memory is 📝Level 1 cache, located on the CPU itself. Finally, 📝Level 2 cache is connected to (but outside of) the CPU. Static random access memory (SRAM) is used for cache memory.
    - ❄️RAM and ROM: RAM is volatile memory used to hold instructions and data of currently running programs. It loses integrity after loss of power. ROM is nonvolatile; data stored in ROM maintains integrity after loss of power. A computer basic input/output system (BIOS) firmware is stored in ROM. While ROM is “read only,” some types of ROM may be written to via flashing.
    - ❄️DRAM and SRAM: SRAM is fast, expensive memory that uses small latches called “flip-flops” to store bits. Dynamic random-access Memory (DRAM) stores bits in small capacitors (like small batteries), and is slower and cheaper than SRAM. The capacitors used by DRAM leak charge, and so they must be continually refreshed to maintain integrity, typically every few to a few hundred milliseconds, depending on the type of DRAM. Refreshing reads and writes the bits back to memory. SRAM does not require refreshing and maintains integrity as long as power is supplied.
    - ❄️Firmware: Firmware stores programs that do not change frequently, such as a computer’s BIOS (discussed below) or a router’s operating system and saved configuration. Various types of ROM chips may store firmware, including programmable read-only memory (PROM), erasable programmable read-only memory (EPROM), and EEPROM, defined next.
         - 🎱PROM can be written to once, typically at the factory. Data can be written to PROM chips 📝only once.
         - 🎱EPROM and EEPROM may be “flashed,” or erased and written to multiple times. EPROM/UVEPROM chips may be erased with ultraviolet light. EEPROM chips may be erased with electrical current. A programmable logic device (PLD) is a field-programmable device, which means it is programmed after it leaves the factory. EPROMs, EEPROMs, and flash memory are examples of PLDs.
              - 🎱Flash memory, such as a USB thumb drive, is a specific type of EEPROM that is used for storage. The difference is that any byte of an EEPROM may be written, while flash drives are written by larger sectors.
- 📝🧠A SSD is a combination of flash memory (EEPROM) and DRAM
    - 🍎**Real Memory**: Static RAM and dynamic RAM are types of real memory and thus are all the same concept in relation to being 📝volatile—­meaning they lose any data they were holding when power is lost or cycled. They are often classified under primary memory. Real or primary memory, such as RAM, is directly accessible by the CPU and is used to hold instructions and data for currently executing processes. 
         - ✴️Static RAM SRAM: Stores data using flip-flops. This means the data is 📝retained as long as power is supplied. 📝Faster because it doesn’t need to refresh. It can quickly read and write data. More expensive because it uses more transistors per bit of data and consumes more power. 📝volatile
         - ✴️Dynamic RAM DRAM: Stores data using capacitors and transistors. The data needs to be refreshed periodically to maintain its integrity. 📝Slower due to the need for periodic refreshing of the data stored in capacitors. cost-effective for 📝larger memory capacities and 📝consumes less power. 📝volatile
    - 🍎**Secondary Memory** is a term used to describe magnetic, optical, or flash media (i.e., typical storage devices like HDD, SSD, CD, DVD, and thumb drives). These devices will 📝retain their contents after being removed from the computer and may later be read by another user. They are therefore 📝non-volatile. Secondary memory, such as disk-based memory, is not directly accessible by the CPU. 📝The best way to ensure that data on DVDs is fully gone is to destroy them, and pulverizing DVDs is an appropriate means of destruction. DVD-ROMs are write-only media, meaning that secure erase and zero wipes won't work.

- 2.6.2 📁Scoping and tailoring: 📝Modifying baselines to better suit an organization is known as tailoring. Scoping occurs when baselines are reviewed to ensure that only controls suited to the environment or system are used. 
  - 🔴**Baseline**: documented, lowest level of security config allowed by a standard or organisation
       - After selecting a control baseline, orgs fine-tune with tailoring and scoping processes; a big part of the tailoring process is aligning controls with an org's specific security requirements
       - They provide a good starting point that can be tailored to organizational needs.
       - The Microsoft's Windows 10 security baseline, The NSA Windows 10 Secure Host Baseline, and The CIS Windows 10 baseline are all useful for building a Windows 10 security standard. Group Policy can then be used to monitor and apply settings in a security baseline. 
       - The 🌰Center for Internet Security (CIS) works with subject matter experts from a variety of industries to create lists of security controls for operating systems, mobile devices, server software, and network devices. CIS provides OS, application, and hardware security configuration guides for a wide range of products.
       - The controls implemented from a security baseline should match the data classification of the data used or stored on the system.
       - A baseline is a listing of security controls that provide a minimum level of security. Organizations can tailor a baseline to meet their needs.
       - The baseline is a starting point, and it does not ensure maximum security. A baseline provides a listing of controls an organization can apply, but it isn't necessarily a listing of applied controls.
  - 🔴**Tailoring**: refers to 📝modifying the list of 📝security controls within a baseline to align with the org's mission
    - includes the following activities:
      - Tailoring is 📝customizing a set of existing security controls to align with an organization’s mission and objectives. Examples of blueprints that can be tailored are ISO 27001 & 27002 and the NIST Cybersecurity Framework.
      - identifying and designating common controls; specificaion of organization-defined parameters in the security controls via explicit assignment and selection statements
      - Tailoring refers to modifying a list of security controls to align with the organization's mission. 
      - applying scoping guidance/considerations
      - selecting/specifying compensating controls
      - assigning control values
      - tailoring matches your organization's mission and the controls from a selected baseline.
      - Tailoring adjusts security requirements to organizational needs.
      - Tailoring ensures that assessment methods are appropriate to the systems, services, and other assets that are being validated and is the best answer here.
      - The tailoring process refers to modifying a list of controls to align with the organization's mission. One way it does so is by modifying control parameters, such as changing the account lockout threshold. While tailoring includes scoping, assigning different values for controls only apples to tailoring. Tailoring is done after selecting a baseline.
  - 🔴**Scoping**: limiting the general baseline recommendations by removing those that do not apply; part of the tailoring process and refers to reviewing a list of baseline 📝security controls and 📝selecting only those controls that apply to the systems you're trying to protect
    - Scoping is similar to tailoring but, instead of altering standards, a business would 📝completely remove the standards that are not needed. 
    - Scoping is reviewing and selecting initial security controls for a new information system. 
    - scoping processes eliminate controls that are recommended in a baseline
    - Scoping is specifically used to remove controls from a suggested baseline. 
    - Scoping is a part of the tailoring process and refers to reviewing a list of security controls and selecting the security controls that apply.
    - Scoping is the process of reviewing and selecting security controls based on the system that they will be applied to. 
    - Scoping involves selecting only the controls that are appropriate for your IT systems
    - Scoping involves setting the boundaries of security control implementations.
    - The scoping process removes controls from a list of controls from a suggested baseline.
    - Scoping is the process of determining which controls are appropriate to an organization, environment, or implementation.
- 2.6.3 Standards selection
  - Organizations need to identify the standards (e.g. PCI DSS, GDPR etc) that apply and ensure that the security controls they select fully comply with these standards
  - Even if the org doesn't have to comply with a specific standard, using a well-designed community standard can be helpful (e.g. NIST SP 800 documents)
  - **Standards selection**: the process by which organizations plan, choose and document technologies or architectures for implementation
    - e.g. you evaluate three vendors for a security control; you could use a standards selection process to help determine which solution best fits the org
  - Vendor selection is closely related to standards selection but focuses on the vendors, not the technologies or solutions
  - The overall goal is to have an objective and measurable selection process 
    - if you repeat the process with a totally different team, the alternate team should come up with the same selection

- 2.6.4 Data protection methods (e.g., Digital Rights Management (DRM), Data Loss Prevention (DLP), Cloud Access Security Broker (CASB)) 
  - **Data protection methods** include: 
    - 💻**Digital Rights Management (DRM)**: methods used in attempt to protect copyrighted materials, including intellectual property. They do not provide protection for trademarks, patents, or trade secrets. There are hardware and software based DRMs. Methods used with DRM include:
         - ✈️Persistent online authentication. 📝in the absence of internet, Product keys and cryptographic algorithms work together to ensure the product key is legitimate to the asset. If the key does not function within this proprietary algorithm, the asset is useless to the user. This requires the user to connect to the internet one time to activate the key and provide it with the ability to interact with the algorithm. If the key has been activated, a company can confirm it is stolen.
         - ✈️Automatic expiration
         - ✈️Continuous audit trail  
    - 💻**Cloud Access Security Brokers (CASBs)**: software placed logically between users and cloud-based resources ensuring that cloud resources have the same protections as resources within a network and  providing monitoring and policy enforcement capabilities
         - A cloud access security broker (CASB) is software placed logically between users and cloud-­based resources, and it can enforce security policies used in an internal network.
         - CASB can be on-site or cloud-based and are security policy enforcement points that operate between users and cloud services.
         - CASBs offer features such as
             - 📝provide data encryption
             - 📝provide access control
             - 📝provide insight into what users are doing
             - 📝enforce policies
             - 📝provide threat protection
             - 📝provide data loss prevention capabilities.
             - 📝compliance monitoring to address security concerns associated with cloud computing
             - 📝detect what services the users are utilizing.
      - note that entities must comply with the EU GDPR, and use additional data protection methods such as pseudonymization, tokenization, and anonymization
  - One of the primary methods of protecting the confidentiality of data is encryption
  - Options for protecting your data vary depending on its state:
    - 🌟**Data at rest**: consider encryption for operating system volumes and data volumes, and backups as well
      - be sure to consider all locations for data at rest, such as tapes, USB drives, external drives, RAID arrays, SAN, NAS, and optical media
      - ✏️AES is a strong modern 📝symmetric encryption algorithm that is appropriate for encrypting data at rest. Using strong encryption, like AES-256, can help ensure that loss of removable media like tapes doesn't result in a data breach.
      - ✏️Full disk encryption like 📝Bitlocker can protect data at rest.
      - Tapes may be vulnerable to theft or loss in transit. That means that tapes that are leaving their normal storage facility should be handled according to the organization's classification schemes and handling requirements.
      - ✏️DRM is useful for data at rest because DRM 📝"travels with the data" regardless of the data state
      - ✴️Watermarks can detect the unauthorized copying of documents, including photos. For example, A 🧠watermark is used to digitally label data and can be used to indicate ownership, as well as to assist a digital rights management (DRM) system in identifying data that should be protected. Digital watermarking places labels or marking in files (digital data). Other methods, such as data loss prevention (DLP) and digital rights management (DRM), can detect the labels. Digital watermarks allow owners to track and identify assets, ⛔not prevent their use. This may also allow for an audit trail and the prosecution of users who may have stolen the product.
      - ✴️Steganography is the art of using cryptographic techniques to embed secret messages within other content. Some steganographic algorithms work by making alterations to the least significant bits of the many bits that make up image files.  Other methods, such as data loss prevention (DLP) and digital rights management (DRM), can detect these embedded information.
      - DRM is especially useful when you can’t encrypt data volumes. 
    - 🌟**Data in transit**: think of data in transit wholistically -- moving data from anywhere to anywhere; use ✏️encryption for data in transit 
      - TLS is frequently used to secure data when it is in transit.
      - e.g. a web server uses a certificate to encrypt data being viewed by a user, or IPsec encrypting a communication session 
      - most important point is to use encryption whenever possible, including for internal-only web apps
      - ✏️DLP solutions are useful for data in transit, scanning data on the wire, and stopping the transmission/transfer, based on the DLP rules set (e.g. outbound data that contains numbers matching a social security number pattern, a DLP rule can be used to block that traffic)
    - 🌟**Data in use**: 
      - ✏️CASB solution often combines DLP, a web application firewall with some type of authentication and authorization, and a network firewall in a single solution; A CASB solution is helpful for protecting data in use (and data in transit)
  - 🟢**Pseudonymization**: refers to the process of using pseudonyms or alias to represent other data. An external dataset holds the original data along with the pseudonym so that the original dataset can be re-created.
       - A pseudonym is an alias, and pseudonymization can prevent data from directly identifying an entity (i.e. person).
       -  Pseudonymization is the process of replacing some data with an identifier, such as a pseudonym.
       -  Pseudonymization does not remove any data and instead provides false names for subjects.
       -  While it may seem like an effective method to protect data, those with knowledge of the fact that the data is using pseudonyms can narrow down the actual identities of subjects based on the categories of traits. 
       - However, if applying pseudonymization techniques, the GDPR still applies
       - 📝the process can be reversed
  - 🟢**Tokenization**: use of a token, typically a random string of characters that ✏️remains the same for each instance of that data, to replace other data. Tokenization replaces other data with a random string of characters. These tokens are then matched to the actual values for secure lookups as needed. 
       - Tokenization involves replacing sensitive data with a token that has no actual relation to the data it is replacing.
       - note that tokenization is similar to pseudonymization in that they are both used to represent other data, and the token or pseudonym have no meaning or value outside the process that creates and links them to that data
       - If company are reselling products to the same customers, they can use tokenization to save tokens that match the credit card data, instead of saving and storing credit card data.
       - example of tokenization used in Credit Card transactions:
       - registration: app on user's smart phone securely sends CC info to the credit card processor (CCP)
       - The CCP sends the CC info to a tokenization vault, creating a token and associating it with the user's phone
       - usage: when the user makes a purchase, the POS system sends the token to the CCP for authorization
       - validation: the CCP sends the token to the tokenization vault; the vault replies with the CC info, the charge is processed
       - completing the sale: the CCP sends a reply to the POS indicating the charge is approved
       - this system prevents CC theft at the POS system
  - 🟢**Anonymization** removes all personally identifiable data to ensure that the original subject cannot be identified. Unlike peudonymization, removing personal data without using an identifier is closer to anonymization. Anonymization techniques remove all data so that it is difficult to identify the original identities. When done correctly, the GDPR no longer applies. 
       - Anonymization is the process of removing data to the point that it is impossible to identify the subject(s). This is most effective with large data sets with many categories.
       - Anonymization also cannot be reversed, making it impossible to use a secondary data set to retrace steps associated with the anonymization process. What sets it apart is anonymization can't be reversed because the technique requires the shuffling of a large data set and has no database to link the random data back to the original subject(s).
       - Anonymization techniques remove all personal data and make the data unusable for reuse on the website. Techniques of Data Anonymization
       - 1. Data masking: obscures some, but not all data. Data masking is a method used to anonymize data and protect the privacy of individuals in a dataset. It replaces privacy data (such as names) with incorrect data, but the dataset still retains usable data for instance when needed for reserach
         2. Pseudonymization: Pseudonymization involves replacing identifiable information with pseudonyms or identifiers that do not directly reveal the identity of individuals. The original data can only be restored if additional information, kept separate, is used.
         3. Generalization
         4. Data swapping: Swapping data involves exchanging data between different records, which can still potentially allow for some degree of identification, especially if the data structure or patterns are recognizable.
         5. Data perturbation: is the use of false or misleading data in a database management system in order to redirect or thwart information confidentiality attacks.
         6. Synthetic data
         7. Randomized masking: is one of many anonymization methods. When done correctly, it cannot be reversed to discover the original data. 
  - 🟢**Randomizing data** Randomizing data is a common approach to anonymization. It involves replacing PII with artificial but realistic-looking data. This technique ensures that the data in a test environment does not correspond to any actual individuals, thus protecting privacy while maintaining the data’s utility for testing purposes.

- **Policy ▶️ Standard ▶️ Baseline ▶️ Guideline ▶️Procedure**
    - 📩**Policy**: Policy is High level from Management. A security policy is a high-level description of an organization's security requirements. Policies are high-level documents that align security objectives with business objectives. 
    - 📩**Standard** mandatory, must meet EXACTLY, no more, no less e.g DoD 8570, AR 25-2, NIST SP , 800 53. Standards selection refers to adding security controls based on external standards. Standards define required activities that are designed to help an organization fulfill its policy goals. Standards document, 📝in detail, the security requirements for a subset of technology. Standards are generally referenced by and enforced in a separate security policy. A standard documents, in detail, the security requirements for a subset of technology e.g encryption type. Standards are generally referenced by and enforced in a separate security policy.
    - 📩**Baseline** mandatory, must meet AT LEAST, can do more than it requires e.g CIS Benchmarks, or imaging. Imaging is done to deploy an identical configuration to multiple systems, but this is typically done after identifying security controls. Baselines are starting points, generally using lists, and they typically require modifications. Baselines are often used when creating images of OS. A baseline is the minimum level of acceptable security applied to a system. Baselines are used to standardize security levels across multiple systems. An example of a baseline is a Microsoft GPO that enables the host-based firewall on all systems.
    - 📩**Guideline** suggested practices, not mandatory e.g DoD STIGs, Microsoft NSA, PCI DSS, NIST 800-88. Guidelines provide recommendations about how to do something if no specific standard exists. Guidelines are recommended actions or behaviors if a standard does not apply.
    - 📩**Procedures** are step-by-step descriptions of how a member of an organization should achieve a security goal. Procedures are step-by-step instructions to accomplish a task.
- **Salami Attack** is a type of cyberattack or fraud where a perpetrator takes small, seemingly insignificant actions that individually appear harmless but collectively result in a substantial impact. The term "salami" is used metaphorically, implying that the attack slices off small pieces, like slicing a salami, which are not noticeable individually but accumulate to cause significant harm.
- **Bitrot** describes the slow loss of data on aging media
