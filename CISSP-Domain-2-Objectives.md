[Domain 2](#domain2-top) **Asset Security**

- Domain 2 of the CISSP exam covers asset security making up ~10% of the test 
- Asset security includes the concepts, principles, and standards of monitoring and securing any asset important to the organization. Asset: anything of value owned by the organization
- Anything that removes a vulnerability or protects against one or more specific threats is considered a safeguard or a countermeasure. The annual costs of safeguards should not exceed the expected annual cost of asset value loss.
- The Asset Security domain focuses on collecting, handling, and protecting information throughout its lifecycle; the first step is classifying information based on its value to the organization. Classification should be conducted based on the ‚úèÔ∏èvalue of the data to the organization, its ‚úèÔ∏èsensitivity, and the amount of ‚úèÔ∏èharm that could result from exposure of the data. When data is stored in a mixed classification environment, it is typically classified based on the highest classification of data included.
- **Asset lifecycle**: phases an asset goes through, from creation (or collection) to destruction
- üî¥**Risk** is the _possibility, probability, chance or likelihood_ that a threat will exploit a vulnerability to cause harm to an asset and the severity of damage that could result. e.g damage to equipment.
     - Risk = Threat ‚úñÔ∏è Vulnerability
     - üçã**Threat** is any _potential occurrence_ that may cause an undesirable or unwanted outcome for an organization or for a specific asset. e.g threat/likelihood of fire
     - üçã**Vulnerability** is the _weakness_ in an asset, or the absence or the weakness of a safeguard or countermeasure. e.g lack of fire extinguishers
     - üçã**Exposure** is being _susceptible_ to asset loss because of a threat; there is the possibility that a vulnerability can or will be exploited. It is the presence of a vulnerability when a related threat exists. - Every instance of exposure is a risk.
     - üçã**Risk Analysis**: is simply the evaluation of threats against assets in regard to rate of occurrence and levels of consequence.
     - üçã**Risk Assessment**: A risk assessment would help identify which controls are needed to protect the assets

[2.1](#2.1) Identify and classify information assets (OSG-9 Chpt 5)

- 2.1.1 Data classification
  - Managing the data lifecycle refers to protecting it from cradle to grave -- steps need to be taken to protect data when it's first created until it's destroyed
  - One of the first steps in the lifecycle is identifying and classifying information and assets, often within a security policy
  - Data classifications provide strong protection against the loss of confidentiality
  - In this context, assets include sensitive data, the hardware used to process that data, and the media used to store/hold it
  - **Data categorization**: process of grouping sets of data, info or knowledge that have comparable sensativities (e.g. impact or loss rating), and have similar law/contract/compliance security needs. In the üçéNIST SP 800-60 diagram, the process determines appropriate categorization levels resulting in security categorization and then uses that as an input to determine controls. 
  - **Sensitive data**: any information that isn't public or unclassified, and can include anything an org needs to protect due to its value, or to comply with existing laws and regulations
  - **Personally Identifiable Information (PII)**: any information that can identify an individual
    - more specifically, info about an individual including (1) any info that can be used to distinguish or trace an individual‚Äòs identity, such as name, social security number, date and place of birth, mother‚Äòs maiden name, or biometric records; and (2) any other information that is linked or linkable to an individual, such as medical, educational, financial, and employment information. This would also include any other unique identifier (including a student ID number). ‚ùóZIP/Post code, by itself, does not uniquely identify an individual. ([NIST SP 800-122](https://csrc.nist.gov/publications/detail/sp/800-122/final)) NIST Special Publication ‚ú¥Ô∏è800-122
  - **Protected Health Information (PHI)**: any health-related information that can be related to a specific person
  - **Proprietary data**: any data that helps an organization maintain a competitive edge
  - Organizations classify data using labels
  - üî¥**Governmental data classification**The impact to national security is more typically associated with government classification schemes. government classification labels include:  üü°TSCSU
      - ‚ùÑÔ∏è**Top Secret**: if disclosed, could cause massive damage to national security, such as the disclosure of spy satellite information
      - ‚ùÑÔ∏è**Secret**: if disclosed, can adversely affect national security
      - ‚ùÑÔ∏è**Confidential**: Confidential data is usually data that is exempt from disclosure under laws such as the Freedom of Information Act but is not classified as national security data.
      - ‚ùÑÔ∏è**Sensitive**: But Unclassified (SBU): SBU data is data that is not considered vital to national security, but its disclosure would do some harm. Many agencies classify data they collect from citizens as SBU. In Canada, the SBU classification is referred to as protected (A, B, C).
      - ‚ùÑÔ∏è**Unclassified**: not sensitive
  - üî¥**Commercial/Non-Governmental data classification** üü°CPSP often takes into account the value of the data, any regulatory or legal requirements that may apply to the data, and how long the data is useful‚Äîits lifespan. non-government organizations use labels such as:
      - üçÆ**Confidential/Proprietary**: only used within the org and, in the case of unauthorized disclosure, it could suffer serious consequences. It is the most sensitive data. Proprietary data is information that helps organizations maintain a competitive edge or that they want to keep to their own organization or a controlled set of individuals.
      - üçÆ**Private**: may include personal information, such as credit card data,  patient X-ray data, and bank accounts; unauthorized disclosure can be disastrous. Private data is internal business data that shouldn't be exposed but that doesn't meet the threshold for confidential or proprietary data. 
      - üçÆ**Sensitive**: needs extraordinary precautions to ensure confidentiality and integrity. Sensitive data may help attackers or otherwise create risk.
      - üçÆ**Public**: can be viewed by the general public and, therefore, the disclosure of this data would not cause damage
  - There are six standard data type classifications used in either a government/military or a private sector organization in this list of options: public, private, sensitive, proprietary, critical, and confidential. Some organisations used üçÆcritical and üçÆinternal.
  -  It is important to protect data in all states: at rest, in transit, or in use
  -  The best way to protect data confidentiality is via use of strong encryption
- **Declassification** is the process of moving an object into a lower level of classification once it is determined that it no longer justifies being placed at a higher level. In a Bell-LaPadula implementation, for example, Only a trusted subject can perform declassification because this action is a violation of the verbiage of the star property of Bell‚ÄìLaPadula, but not the spirit or intent, which is to prevent unauthorized disclosure. ‚úèÔ∏èEnsuring that data cannot be recovered is difficult, and the time and effort required to securely and completely wipe media as part of declassification can exceed the cost of new media. ‚úèÔ∏èSanitization, purging, and clearing may be considered part of declassification.
   - If an organization allows media to be downgraded, the purging process should be followed, and then the media should be relabeled.

- 2.1.2 Asset Classification
  - It's important to identify and classify assets, such as systems, mobile devices etc.
  - **Classification**: derived from compliance mandates, the process of recognizing organizational impacts if information suffers any security compromise (whether to confidentiality, integrity, availability, non-repudiation, authenticity, privacy, or safety)
      - Classification identifies the value of data to an organization
      - üìùAsset classifications should match data classification, i.e. if a computer is processing top secret data, the computer should be classified as a top secret asset
      - Handling requirements and tools include visual indicators like a distinctive screen background and can help employees remember what level of classification they are dealing with and thus the handling requirements that they are expected to follow.
      - In a single-level security environment, systems should be assigned the classification level of the highest classification of information they are ever expected to process.
      - Information should be classified based upon its sensitivity. This may be due to the value of the information to the organization, the damage caused if lost or compromised, or other factors.
      - **Reclassifying Data**: When the value of data changes due to legal, compliance, or business reasons, reviewing classifications and reclassifying the data is an appropriate response. Once the review is complete, data can be reclassified and handled according to its classification level. 
      - **Clearance**: relates to access of certain classfication of data or equipment, and who has access to that level or classification
      - A **Formal Access Approval Process** should be used to change user access; the process should involve approval from the data/asset owner, and the user should be informed about rules and limits
      - before a user is granted access they should be educated on working with that level of classification
      - Classification levels can be used by businesses during acquisitions, ensuring only personnel who need to know are involved in the assessment or transition
      - In general, classification labels help users use data and assets properly, for instance by restricting dissemination or use of assets by their classification
      - **Sensitive Data Scanning Tools:** To identify data that should be classified that already exists in an environment, Sensitive data scanning tools can be used. They are designed to scan for and flag sensitive data types using known formatting and structure. Social Security numbers, credit card numbers, and other regularly structured data that follows known rules can be identified and then addressed as needed. examples include Microsoft Azure Information Protection (AIP), Varonis Data Security Platform, Vormetric Data Security Platform. Some DLP solutions also offer this tool/capability as part of thier broader solution.

[2.2](#2.2) Establish information and asset handling requirements (OSG-9 Chpt 5)

- **Asset handling**: refers to secure transport of media through its lifetime
- The data and asset handling key goal is to prevent data breaches, by using:
  - **Data Maintenance**: on-going efforts to organize and care for data through its life cycle
  - üè∑Ô∏è**Data Loss Prevention (DLP)**: systems that detect and block data exfiltration attempts; DLP systems can use labels on data to determine the appropriate controls to apply to the data. DLP system or software is designed to identify labeled data or data that fits specific patterns and descriptions to help prevent it from leaving the organization. A data loss prevention (DLP) system can tag, monitor, and limit where files are transferred to. There are two primary types:
     - ü©∞Network-based DLP: Network-based DLP would not detect stored information unless the user transmits it over the network. 
     - ü©∞Endpoint-based DLP: identify the presence of information on endpoint devices
  - üè∑Ô∏è**Marking (AKA labeling)**: sensitive information/assets ensures proper handling (both physically and electronically).
     - Security labeling identifies the classification of data such as sensitive, secret, and so on. Media holding sensitive data should be labeled. Similarly, systems that hold or process sensitive data should also be marked
     - Media is typically labeled with the highest classification level of data it contains. This prevents the data from being handled or accessed at a lower classification level.
     - Systems and media should be labeled with the highest level of sensitivity that they store or handle.
     - labels can be as granular and custom as required by the org
     - Data labeling can help ensure that controls are applied to the right systems and data.
     - Data labels are crucial to identify the classification level of information contained on the media, and labeling data at creation helps to ensure that it is properly handled throughout its lifecycle.
     - Requiring all media to have a label means that when unlabeled media is found, it should immediately be considered suspicious. This helps to prevent mistakes that might leave sensitive data unlabeled.
     - Tagging: Tags that include information about the lifespan of the data and when it has expired can help with lifecycle management processes. Tags can be as simple as timestamps, or they can include additional metadata like the data type, creator, or purpose that can help inform the retention and disposal process. Metadata tagging allow organisations to use technical tools like DLP and DRM to handle and track data, based on its type and content.
  - üè∑Ô∏è**Data Collection Limitation**: Providing ‚úèÔ∏èconsent, or agreeing to data collection and use, is important in many data collection scenarios and may be required by law. ‚úèÔ∏èOnly required data is collected, that individuals are made aware of the data collection, and that they consent to the collection. Similarly, data should only be collected ‚úèÔ∏èlawfully and via fair methods. Prevent loss by not collecting unnecessary sensitive data. ü•áSometimes it is better to collect limited informationü•á
  - üè∑Ô∏è**Data Location**: keep duplicate copies of backups, on- and off-site
  - üè∑Ô∏è**Storage**: define storage locations and procedures by storage type; use physical locks for paper-based media, and encrypt electronic data. It is cost effective to purchase high-quality media to contain sensitive data becuase the value of the data often far exceeds the cost of the media. This makes more expensive media that may have a longer life span or additional capabilities like encryption support a good choice for sensitive data. Backup media should be protected with the same level of protection afforded the data it contains e.g using a secure offsite storage facility
  - **Data Remanence**: data remaining on media after typical erasure; Remanence describes data left on media after an attempt is made to remove the data. 
- üî•sanitization is a series of processes that removes data from a system or media while ensuring that the data is unrecoverable by any means. Sanitization methods (such as clearing, purging, and destroying) help ensure that data cannot be recovered. üìùNote: Downgrading systems and media is rare due to the difficulty of ensuring that sanitization is complete. The need to completely wipe (or destroy) the media that systems use means that the ‚úèÔ∏ècost of reuse is often significant and may exceed the cost of purchasing a new system or media i.e sanitazation cost ‚ñ∂ cost of new media.
  - ‚ú¥Ô∏è**Destruction**: destroy data no longer needed by the organization; policy should define acceptable destruction methods by type and classification ([see NIST SP-800-88 for details](https://csrc.nist.gov/publications/detail/sp/800-88/rev-1/final)) ‚úèÔ∏èPhysical destruction: used for SSD/electronic components, or in combination with other less-secure methods. Due to problems with remnant data, the U.S. National Security Agency requires physical destruction of SSDs. Incineration, pulverizing, crushing, shredding, and disintegration all describe data destruction.
      - üçé**SSD**: Spare sectors, bad sectors, and space provided for wear leveling on SSDs (over provisioned space) may all contain data that was written to the space that will not be cleared when the drive is wiped. This is a form of data remanence and is a concern for organizations that do not want data to potentially be accessible. Many wiping utilities only deal with currently addressable space on the drive. SSDs cannot be degaussed, and wear leveling space cannot be reliably used to hide data. These spaces are still addressable by the drive, although they may not be seen by the operating system. The two valid options for destroying data on SSD drives are ATA Secure Erase and destruction. Destruction is the best method for SSD drives.
      - üçé**shredding**: is a type of physical destruction. Though this term is sometimes used in relation to overwriting of data, here shredding  refers to the process of making unrecoverable any data printed on hard copy or on smaller objects, such as credit cards, floppy or optical disks. There are industrial shredders capable of shredding larger devices like servers and hard disks.
  - ‚ú¥Ô∏è**Clearing**: removal of sensitive data from a storage device such that there is assurance data may not be reconstructed using normal functions or software recovery or software recovery utilities; over-writing existing data or scrubbing un-needed data. Clearing describes preparing media for üìùreuse in same-security/sensitivity level. When media is cleared, unclassified data is written over all addressable locations on the media. Once that's completed, the media can be reused
  - ‚ú¥Ô∏è**Purging (Sanitization)**: removal of sensitive data from a system or device with the intent that data cannot be reconstructed by any known technique; usually refers to mutliple clearing passes combined with other tools-- not considered acceptable for top secret data.
      - Purging overwrites the media with random bits multiple times and includes additional steps to ensure that data is removed. It ensures there isn‚Äôt any data remanence.
      - Purging is a more intensive form of clearing for üìùreuse in lower-security areas
      - The üçéNIST SP 800-88 process for sanitization and disposition shows that media that will be reused and was classified at a moderate level should be purged and then that purge should be validated. Finally, it should be documented. ‚úèÔ∏èValidation processes are conducted to ensure that the sanitization process was completed, avoiding data remanence. A validation form  helps to ensure that each device has been checked and that it was properly wiped, purged, or sanitized. 
  - ‚ú¥Ô∏è**Overwriting**: Same as Clearing and also known as üî•Purging. Overwriting the disks multiple times will remove all existing data. This is called purging, and purged media can then be üìùre-used again.
       - Single-pass wipe: involves overwriting the entire hard drive with random data once (e.g replacing with 0s and 1s). 
       - Multi-pass wipes: involves overwriting the drive multiple times, and is more secure against advanced recovery techniques.
       - Zero fill wipes a drive by replacing data with zeros
  - ‚ú¥Ô∏è**Degaussing**: used on ‚úèÔ∏èmagentic media. Degaussing the disks often damages the electronics but doesn‚Äôt reliably remove the data. Tapes can be erased by degaussing, but degaussing is not always fully effective. 
  - ‚ú¥Ô∏è**Erasing**: usually refers to a delete operation on media, leaving data remanence. It rarely remove the data from media but instead mark it for deletion. Erasing is the deletion of files or media and may not include all of the data on the device or media, making it the üìùworst choice here. Erasing, which describes a typical deletion process in many operating systems, typically removes only the link to the file and leaves the data that makes up the file itself. The data will remain in place but not indexed until the space is needed and it is overwritten. 
  - ‚ú¥Ô∏è**Cryptographic Erasure**: AKA cryptoshedding, basically destroying encryption key; may be only secure method for üß†cloud storage
  - The standard methods for clearing magnetic tapes, according to the NIST Guidelines for Media Sanitization, are overwriting the tape with nonsensitive data, degaussing, and physical destruction via shredding or incineration. Reformatting a tape does not remove remnant data.

[2.3](#2.3) Provision resources securely (OSG-9 Chpt 16)

- The primary purpose of security operations practices is to safeguard assets such as information, systems, devices, facilities, and apps; these practices help to identify threats, vulnerabilities, and implement controls to reduce the risk to these asssets
- Implementing common security operations concepts, along with performing periodic security audits and reviews demonstrates a level of due care
- **Need-to-know**: a principle that imposes the requirement to grant users access only to data or resources they need to perform assigned work tasks
- **Least privilege**: a principle stating that subjects are granted only the privileges necessary to perform assigned work tasks and no more

- 2.3.1 Information and asset ownership
  - ‚ò™Ô∏è**Data owner**: the person who has ultimate organizational responsibility for data; usually sr. manager (CEO,president, dept. head); data owners typically delegate data protection tasks to others in the org
  - ‚ò™Ô∏è**Asset Owner**: identifies the individual(s) responsible for protecting the asset or for delegating the task of protecting the asset

- 2.3.2 Asset inventory (e.g., tangible, intangible)
  - **Inventory**: complete list of items.  In most organizations, changing processes so that new systems and devices are added to inventory before they are deployed is the first step in making sure asset inventories are current. A system inventory is most frequently used to associate individuals with systems or devices. This can help when tracking their support history and aids in provisioning the proper tools, permissions, and data to a system. While it can be a lot of work, the most complete inventory of active systems and devices can be created by determining what is connected to the network by looking at logs, and then finding those assets. Barcodes & RFID tags are a common solution for tracking hardware assets and equipment. RFID tags can be queried wirelessly at varying ranges depending on the tags and may be built-in to hand-held readers or even included in doorways or arches to track items as they enter or leave a facility. Visual inventory relies on staff checking items, MAC addresses are hardware addresses for networked devices.
  - üçè**Tangible assets**: include hardware and software assets, cables, and buildings owned by the company. Tangible asset inventories include physical items owned by the organization. 
  - üçè**Intangible assets**: things like Patents, databases, and formulas, copyrights, a company‚Äôs reputation, Intellectual property, files stored on a server, and other assets representing potential revenue
    - an org should keep track of intangible assets, like intellectual property, patents, trademarks, and company‚Äôs reputation, and copyrights to protect them
    - To protect intangible inventories (like intellectual property, patents, trademarks, and company‚Äôs reputation, and copyrights), they need to be tracked
    - note: patents in the US are valid for 20 years
  - üçè**Personnel Assets**: Employees 

- 2.3.3 Asset management
  - Asset management refers to managing both tangible and intangible assets; this starts with inventories of assets, tracking the assets, and taking additional steps to protect them throughout their lifetime
  - **Accountability**: ensures that account management has assurance that only authorized users are accessing a system and using it properly
  - üçè**Hardware assets**: IT resources such as computers, servers, routers, switches and peripherals
    - use an automated configuration management system (CMS) to help with hardware asset management
    - use barcodes, RFID tags to track hardware assets
  - üçè**Software assets**: operating systems and applications 
    - important to monitor license compliance to avoid legal issues
    - software licensing also refers to ensuring that systems do not have unauthorized software installed
- **NIST Special Publication 1800-5**: IT Asset Management Reference Architecture
     -  üéàTier 1 Systems: Tier 1 systems collect, store, and analyze the data that they receive from the Tier 2 systems. They allow users to analyze the data and to visualize it for further analysis. e.g splunk
     -  üéàTier 2 Systems: Tier 2 is composed of systems that each perform a unique task. Each Tier 2 system is fully capable of collecting, storing, and analyzing data pertaining to its unique task. The middle tier systems filter relevant and desired data from the raw data collected and forward this data to the analysis engine and visualization tool for further analysis e.g Fathom, Bro, Snort, OpenVAS, WSUS, BelManage, BelManage Data Analytics, Puppet, openswan, iStar/C-Cure Controller
     -  üéàTier 3 Systems: Tier 3 systems are the assets (end points) on the enterprise network that are owned by the enterprise, such as workstations, switches, servers, users‚Äô laptops, virtual machines, and other devices. All enterprise assets are monitored from the start of their lifecycle until disposal by the systems in the Tier 2. Device location, owner, installed software catalog, current security vulnerabilities, and abnormal traffic activity are captured to allow for better visibility by administrators e.g AD, camera, PC, laptops, mobile phones, router and firewall

[2.4](#2.4) Manage data lifecycle (OSG-9 Chpt 5)
- 2.4.1 Data roles (i.e., owners, controllers, custodians, processors, users/subjects)
  - üî¥**Business/Mission Owners**:-  Typically own processes or programs. they ensure that all operations fit within the business goals and mission.
      - This task includes ensuring that collected data is necessary for the business to function. Collecting unnecessary data wastes time and resources.
      - Because the business/mission owner is primarily concerned with the overall business
      - Conflicts between data owners, data custodians, and system owners may need to be resolved by the business/mission owner, who will need to make the best decision for the organization.
      - business owners are tasked with ensuring that systems are fulfilling their business purpose
      - Business owners are most likely to select and apply COBIT to balance the need for security controls against business requirements.
      - Business owners have to balance the need to provide value with regulatory, security, and other requirements. This makes the adoption of a common framework like COBIT attractive.
      - Business owners are typically project or system owners who are tasked with making sure systems provide value to their users or customers.
      - Mission owners are typically program or information system owners.

  - üî¥**Data Owner**: the entity that collects/creates the PII and is legally responsible and accountable for protecting it and educating others about how to protect the data through dissemination of intellectual property rights documentation, policies and regulatory requirements, specific protective measures that are expected of custodians, and compliance requirements.
       - Data owners are tasked with making decisions about data, such as who receives access to it and how it is used.
       - The data owner bears responsibility for categorizing information systems 
       - The data owner has ultimate responsibility for data belonging to an organization and is typically the CEO, president, or another senior employee
       - Data owners are more likely to ask that those responsible for control selection identify a standard to use if they are not also acting as business owners.
       - They are responsible for classifying the data that they own as well as assisting with or advising the system owners on security requirements and control selection
       - The data owner is the person responsible for classifying data, delegates selection of the required controls for each classification to system owners, and selecting baseline security standards for the organization.
       - data owner is the person respsonible for classifying, categorizing, and permitting access to the data; the data owner is the person who is best familiar with the importance of the data to the business
       - The data owner sets the rules for use and protection of data.
   - üî¥**System owner**: controls the computer storing the data; usually includes software and hardware configurations and support services (e.g. cloud implementation)
    - system owners are responsible for the systems that process the data
    - system owner is responsible for system operation and maintenance, and associated updating/patching as well as related procurement activities
    - NIST SP 800-18 describes system owner responsibilities that include helping to develop system security plans, maintaining the plan, ensuring training, and identifying, implementing, and assessing security controls. 
    - Develops a system security plan
    - Ensures that system users receive appropriate security training
    - Identifies and implements security controls
    - system ownership is an important part of making sure baselines are implemented and maintained
    - Typically, system owners, such as a department head, delegate authority to system administrators/custodians
  - üî¥**Data controller**: decide what data to process and how to process it
    - the data controller is the person or entity that controls the processing of the data - deciding what data to process, why this data should be processed, and how it is processed
    - e.g. a company that collects personal information on employees for payroll is a data controller (but, if they pass this info to a third-party to process payroll, the payroll company is the data processor, see below)
    - The data controller is the entity that makes decisions about the data they are collecting.
    - A data controller decides what data to process and directs the data processor to process the data.
  - üî¥**Data processor**: üìØüÖæÔ∏èan entity working on behalf (or the direction) of the data controller, that processes PII; they have a responsibility to protect the privacy of the data and not use it for any purpose other than directed by the data controller; **OR** üìØüÖæÔ∏è Data processor is any system used to process data. 
    - a data controller can hire a third party to process data, and in this context, the third party is the data processor; data processors are often üß†third-party entities that process data for an org at the direction of the data controller
    - Third-party organizations that process personal data on behalf of a data controller are known as data processors. The organization that they are contracting with would act in the role of the business or mission owners, and others within the third party organization would have the role of data administrators, granting access as needed to the data based on their operational procedures and data classification.
    - note GDPR definition: "a natural or legal person, public authority, agency, or other body, which processes personal data soley on behalf of the data controller"
      - GDPR also restricts data tranfers to countries outside EU, with fines for violations
      - many orgs have created dedicated roles to oversee GDPR data laws are followed
  - üî¥**Data custodian**: a custodian is delegated, from the system owner, day-to-day responsibilities for properly storing and protecting data;
      - responsible for the protection of data through maintenance activities, backing up and archiving, and preventing the loss or corruption and recovering data.
      - They include ‚úèÔ∏èIT Staff in an information technology (IT) department who are delegated responsibility for day-to-day tasks.
      - Custodians are trusted to ensure the day-to-day security of the data and should do so by ensuring that the baseline is met and maintained.
      - Custodians are tasked with the day-to-day monitoring of the integrity and security of data. 
      - He/She is responsible for the technical environment, including things like database structures and the technical implementations of data policies.
      - Controls are scoped and tailored, applied and enforced by Custodians.
      - custodians implement the controls
      - custodians are granted rights to perform day-to-day tasks when handling data
  - üî¥**Administrators** have the rights to apply the permissions to access and handle data.
      - The system administrators can act in the roles of data administrators who grant access and will also act as custodians who are tasked with the day-to-day application of security controls such as providing/granting/managing user access.
  - üî¥**Data Steward**: a newer concept related to users of the data; those who use the data for the business purpose. In many organizations, data stewards are internal roles that oversee how data is used
  - üî¥**Security administrator**: responsible for ensuring the overall security of entire infrastructure; they perform tasks that lead to the discovery of vulnerabilities, monitor network traffic and configure tools to protect the network (like firewalls and antivirus software) 
    - security admins also devise security policies, plans for business continuity and disaster recovery and train staff
  - üî¥**Supervisors**: responsible for overseeing the activities of all the above entities and all support personnel; they ensure team activities are conducted smoothly and that personnel is properly skilled for the tasks assigned
  - üî¥**Data Object**: In the subject/object model, the object is the resource being requested by a subject.
  - üî¥**Data Subject**: the person who the information is about.
  - üî¥**Users**: any person who accesses data from a computer device or system to accomplish work (think of users as employees or end users)
    - users should have access to the data they need to perform tasks; users should have access to data according to their roles and their need to access info
    - must comply with rules, mandatory policies, standards and procedures
    - users fall into the category of subjects, and a subject is any entity that accesses an object such as a file or folder 
      - note that subjects can be users, programs, processes, services, computers, or anything else that can access a resource (OSG-9 Chpts 8, 13)
- 2.4.2 Data Collection
  - One of the easiest ways of preventing the loss of data is to simply not collect it
  - **Data Collection Guideline**: if the data doesn't have a clear purpose for use, don't collect it, and don't store it; this is why many privacy regulations mention limiting data collection. Providing consent, or agreeing to data collection and use, is important in many data collection scenarios and may be required by law.
- 2.4.3 Data location
  - **Data Location**: in this context, refers to the location of data backups or data copies
  - If a company's system is on-prem, keeps data on-site, but regularly backups up data, best practice is to keep a backup copy on site and backup copy off-site
  - Consider distance between data/storage locations to mitigate potential mutual (primary and backup) damage risk

- 2.4.4 Data maintenance
  - üîµ**Data Maintenance**: managing data through the data lifecycle (creation, usage, retirement); data maintenance is the process (often automated) of making sure the data is available (or not available) based on where it is in the lifecycle. During the data maintenance phase of a typical data lifecycle, activities like data scrubbing occur to remove unneeded, incorrect, or out-of-date data.
  - Data lifecycle:  Data Maintenance ‚Ü™Ô∏èüîÑ Data Collection ‚û°Ô∏è Data Analysis ‚û°Ô∏è Data Usage ‚û°Ô∏è Data Retention ‚û°Ô∏è Data Destruction ‚Ü©Ô∏èüîÑ Data Maintenance 
  - In a typical data lifecycle, collection is the first stage. Once collected, data can be analyzed, used, stored, and disposed of at the end of its useful life. Policies may be created at any time, and organizations often have data before they have policies. Labels are added to data during the analysis, usage, or retention cycle.
  - Ensuring appropriate asset protection requires that sensitive data be preserved for a period of not less than what is business-required, but for no longer than necessary
  - Encrypt sensitive data
  - Safeguard assets via basic security controls to enforce appropriate levels of confidentiality, integrity and availability and act per security policies, standards, procedures and guidelines
- 2.4.5 Data retention
  - Retention requirements apply to data or records, media holding sensitive data, systems that process sensitive data, and personnel who have access to sensitive data. Record retention policies define the amount of time to keep data, and laws or regulations often drive these policies.
    - üîµ**Record Retention**: Record retention is the process of retaining and maintaining information for as long as it is needed. 
      - note: a current trend in many orgs is to reduce legal liabilities by implementing short retention policies with email
      - A data storage policy describes how and why data is stored.
      - Record retention policies define the amount of time to keep data, and laws or regulations often drive these policies
      - A data retention policy can help to ensure that outdated data is purged, removing potential additional costs for discovery, and reducing the amount of data that may need to be produced for lawsuits. Many organizations have aggressive retention policies to both reduce the cost of storage and limit the amount of data that is kept on hand and discoverable.
      - Always consult the organization's record retentions policy to determine the appropriate length of time to preserve records
  - Three fundamental retention policy questions:
    - üî•**how to retain**: data should be kept in a manner that makes it accessible whenever required; take taxonomy (or the scheme for data classification) into account
    - üî•**how long to retain data**: general guidelines for business data is 7 years (but can vary by country/region/regulation)
    - üî•**what data**: to retain per org requirements
- 2.4.6 Data remanence
  - üîµ**Data Remanence**: the data remaining on media after the data is supposedly erased
    - typically refers to data on a hard drive as residual magnetic flux or slack space (unused space within a disk cluster)
      - note that many OSs store files in clusters, which are groups of sectors (the smallest storage unit on a hard disk drive)
    - if media includes any type of private and sensitive data, it is important to eliminate data remanence
    - note that some OSs fill slack space with data from memory, which is why personnel should never process classified data on unclassified systems
    - Remnant data is data that is left after attempts have been made to remove or erase it. itis also referred to as residual data.
    
- 2.4.7 Data destruction
  - Destroy sensitive data when it is no longer needed
  - An org's security or data policy should define the acceptable methods of destroying data based on the data's classification
  - a degausser can be used on a hard disk drives/magnetic media
  - the best SSD wiping method is destruction -- even when using manufacturers SSD wiping tools, data can remain, and therefore the best SSD wipe method is destruction
  - **Defensible destruction**: eliminating data using a controlled, legally defensible and regulatory compiant way

[2.5](#2.5) Ensure appropriate asset retention (e.g. End-of-Life EOL, End-of-Support (EOS)) (OSG-9 Chpt 5)
- Hardware: even if you maintain data for the appropriate retention period, it won‚Äôt do you any good if you don‚Äôt have hardware that can read the data
- Personnel: beyond retaining data for required time periods and maintaining hardware to read the data, you need personnel who know how to operate the hardware to execute restoraton processes

- üçÆ**End-Of-Life (EOL)**: often identified by vendors as the time when they stop offering a product for sale. At the end of their life (EOL) for workstations, they should be destroyed. Destruction is the most complete method of ensuring that data cannot be exposed, and organizations often opt to destroy either the drive or the entire workstation or device to ensure that data cannot be recovered or exposed.
- üçÆ**End-Of-Support (EOS)/End-Of-Service-Life (EOSL)**: often used to identify when support ends for a product. üìùInstead of vendor, a company company can intentionally end support and needs to address what happens to the devices next‚Äîsecure disposal, destruction, or re-sale‚Äîdepending on data security requirements and policies set by the company. For example, A company‚Äôs policy of issuing new phones every two years suggests that they have a planned schedule for replacement, which can be seen as aligning with an EOS policy, even though the old phones are still functional and receiving updates.
- EOL,EOS/EOSL can apply to either software or hardware

[2.6](#2.6) Determine data security controls and compliance requirements (OSG-9 Chpt 5)

- You need security controls that protect data in each possible state: at rest, in transit or in use
- Each state requires a different approach to security; note that there aren‚Äôt as many security options for data in use as there are for data at rest or data in transit

- 2.6.1 Data states (e.g., in use, in transit, at rest)
  - The three data states are at rest, in transit, and in use
    - üî¥**Data at rest**: any data stored on media such as hard drives or external media: Protecting Data at Rest: AES encryption, Access Control, redundancy/backup, Bitlocker, FileVault, symmetric encrytion e.g Serpent, IDEA.  Data breaches cause the greatest reputational damage as a result of threats to data at rest.  Data at rest with a high level of sensitivity is often encrypted to help prevent this.
    - üî¥**Data in transit**: any data transmitted over a network: Protecting Data in Motion: Data in transit is data that is traversing a network or is otherwise in motion. TLS, VPNs, and IPsec tunnels are all techniques used to protect data in transit. TLS encryption, email encrytion (SMIME, PGP), IPSEC, VPN, SSH
        - ‚≠êSSH-2: Provides improved security with more robust encryption e.g AES and key exchange mechanisms. It also adds support for simultaneous shell sessions over a single SSH connection and Supports optional compression of data to improve  performance
        - ‚≠êIPsec: Provides strong encryption and is used in many VPNs. Operates at the Network layer. Transport Mode: Encrypts only the payload of the IP packet, leaving the header intact. Tunnel Mode: Encrypts both the payload and the header, creating a new IP header.
        - ‚≠êL2TP: Provides tunneling but requires IPsec for encryption. Operates at the Data link layer. perates at the data link layer and provides a framework for tunneling protocols. L2TP alone does not offer encryption or security but is typically used with IPsec to provide these features (L2TP/IPsec). often used in Point-to-Point Protocol PPP scenarios.
        - ‚≠êL2TPV3: L2TPv3 (Layer 2 Tunneling Protocol version 3) is an extension of L2TP (Layer 2 Tunneling Protocol) designed to provide more advanced features and improved capabilities. Extends L2TP to support tunneling of Layer 2 frames over IP networks, similar to the original L2TP but with additional features and improvements. Primarily used to carry Layer 2 traffic (e.g., Ethernet frames) over IP networks. It is useful for applications that require the transmission of Layer 2 protocols across an IP backbone.
        - ‚≠êPPTP: Older and less secure, generally not recommended. Operates at the Data link layer. Encapsulates PPP (Point-to-Point Protocol) frames into IP packets for transmission over the internet. PPTP itself provides encryption but is considered less secure compared to more modern protocols.
        - ‚≠êSSL/TLS: Strong encryption, often used in remote access VPNs. Operates at the Application link layer. Often used in remote access VPNs (SSL VPNs) and for securing web traffic (HTTPS).
        - ‚≠êIKEv2: Enhances IPsec with better performance and security. Operates at the Network layer (enhances IPsec).  Provides strong security and supports features like NAT traversal and seamless connection resumption.
        - ‚≠êOpenVPN: Flexible and secure with strong encryption, highly configurable. Operates at the Application layer (uses SSL/TLS).      
      - encryption methods protect data at rest and in transit
      - Asymmetric encryption (along with symmetric encryption) protects data in transit.
    - üî¥**Data in use**: data in memory or in buffer and used by an application
      - Data in use is data that is in a temporary storage location while an application or process is using it. Thus, data in memory is best described as data in use or ephemeral data. 
      - applications should flush memory buffers to remove data after it is no longer needed: Protecting Data in Use (üìùHarder to protect): RAM/memory data: The most difficult location to secure for encryption keys and similar highly sensitive information is in active memory because the data needs to be decrypted to be used. 
      - pervasive encryption,
      - prevent shoulder surfing,
      - parameter checking against buffer overflow,
      - Address space layout randomization (ASLR) is a memory-protection process for operating systems (OSes) that guards against buffer-overflow attacks by randomizing the location where system executables are loaded into memory
      - Purging memory buffers removes all remnants of data after a program has used it.
      - keeping the systems patched, maintaining a standard computer build process, and running anti-virus/malware are typically the real-world primary protections for data in use
- **Memory**: Memory is a series of on/off switches representing bits: 0s (off) and 1s (on). Memory may be chip based, disk based, or tape based.
    - RAM is random-access memory: ‚Äúrandom‚Äù means the CPU may randomly access or jump to any location in memory.
    - Sequential memory, such as tape, must sequentially read memory, beginning at offset zero, to the desired portion of memory.
    - Volatile memory, such as RAM, loses integrity after a power loss; they  lose their contents when the computer is powered off.
    - Nonvolatile memory (such as read-only memory (ROM), disk, or tape) maintains integrity without power  
- **Different types of Memory**:
    - ‚ùÑÔ∏èCache memory: Cache memory is the fastest system memory, required to keep up with the CPU as it  fetches and executes instructions. The data most frequently used by the CPU is stored in cache memory. The fastest portion of the CPU cache is the ‚úèÔ∏èregister file, which contains multiple registers. Registers are small storage locations used by the CPU to store instructions and data. The next fastest form of cache memory is ‚úèÔ∏èLevel 1 cache, located on the CPU itself. Finally, ‚úèÔ∏èLevel 2 cache is connected to (but outside of) the CPU. Static random access memory (SRAM) is used for cache memory.
    - ‚ùÑÔ∏èRAM and ROM: RAM is volatile memory used to hold instructions and data of currently running programs. It loses integrity after loss of power. ROM is nonvolatile; data stored in ROM maintains integrity after loss of power. A computer basic input/output system (BIOS) firmware is stored in ROM. While ROM is ‚Äúread only,‚Äù some types of ROM may be written to via flashing.
    - ‚ùÑÔ∏èDRAM and SRAM: SRAM is fast, expensive memory that uses small latches called ‚Äúflip-flops‚Äù to store bits. Dynamic random-access Memory (DRAM) stores bits in small capacitors (like small batteries), and is slower and cheaper than SRAM. The capacitors used by DRAM leak charge, and so they must be continually refreshed to maintain integrity, typically every few to a few hundred milliseconds, depending on the type of DRAM. Refreshing reads and writes the bits back to memory. SRAM does not require refreshing and maintains integrity as long as power is supplied.
    - ‚ùÑÔ∏èFirmware: Firmware stores programs that do not change frequently, such as a computer‚Äôs BIOS (discussed below) or a router‚Äôs operating system and saved configuration. Various types of ROM chips may store firmware, including programmable read-only memory (PROM), erasable programmable read-only memory (EPROM), and EEPROM, defined next.
         - üé±PROM can be written to once, typically at the factory. Data can be written to PROM chips üî•only once.
         - üé±EPROM and EEPROM may be ‚Äúflashed,‚Äù or erased and written to multiple times. EPROM/UVEPROM chips may be erased with ultraviolet light. EEPROM chips may be erased with electrical current. A programmable logic device (PLD) is a field-programmable device, which means it is programmed after it leaves the factory. EPROMs, EEPROMs, and flash memory are examples of PLDs.
              - üé±Flash memory, such as a USB thumb drive, is a specific type of EEPROM that is used for storage. The difference is that any byte of an EEPROM may be written, while flash drives are written by larger sectors.
- üî•üß†A SSD is a combination of flash memory (EEPROM) and DRAM
    - üçé**Real Memory**: Static RAM and dynamic RAM are types of real memory and thus are all the same concept in relation to being üî•volatile‚Äî¬≠meaning they lose any data they were holding when power is lost or cycled. They are often classified under primary memory. Real or primary memory, such as RAM, is directly accessible by the CPU and is used to hold instructions and data for currently executing processes. 
         - ‚ú¥Ô∏èStatic RAM SRAM: Stores data using flip-flops. This means the data is ‚úèÔ∏èretained as long as power is supplied. ‚úèÔ∏èFaster because it doesn‚Äôt need to refresh. It can quickly read and write data. More expensive because it uses more transistors per bit of data and consumes more power. üî•volatile
         - ‚ú¥Ô∏èDynamic RAM DRAM: Stores data using capacitors and transistors. The data needs to be refreshed periodically to maintain its integrity. ‚úèÔ∏èSlower due to the need for periodic refreshing of the data stored in capacitors. cost-effective for ‚úèÔ∏èlarger memory capacities and ‚úèÔ∏èconsumes less power. üî•volatile
    - üçé**Secondary Memory** is a term used to describe magnetic, optical, or flash media (i.e., typical storage devices like HDD, SSD, CD, DVD, and thumb drives). These devices will ‚úèÔ∏èretain their contents after being removed from the computer and may later be read by another user. They are therefore üî•non-volatile. Secondary memory, such as disk-based memory, is not directly accessible by the CPU. üìùThe best way to ensure that data on DVDs is fully gone is to destroy them, and pulverizing DVDs is an appropriate means of destruction. DVD-ROMs are write-only media, meaning that secure erase and zero wipes won't work.

- 2.6.2 Scoping and tailoring
  - üî¥**Baseline**: documented, lowest level of security config allowed by a standard or organisation
       - After selecting a control baseline, orgs fine-tune with tailoring and scoping processes; a big part of the tailoring process is aligning controls with an org's specific security requirements
       - They provide a good starting point that can be tailored to organizational needs.
       - The Microsoft's Windows 10 security baseline, The NSA Windows 10 Secure Host Baseline, and The CIS Windows 10 baseline are all useful for building a Windows 10 security standard. Group Policy can then be used to monitor and apply settings in a security baseline. 
       - The Center for Internet Security (CIS) works with subject matter experts from a variety of industries to create lists of security controls for operating systems, mobile devices, server software, and network devices.
       - The controls implemented from a security baseline should match the data classification of the data used or stored on the system. 
  - üî¥**Tailoring**: refers to modifying the list of ‚úèÔ∏è security controls ‚úèÔ∏è within a baseline to align with the org's mission
    - includes the following activities:
      - identifying and designating common controls; specificaion of organization-defined parameters in the security controls via explicit assignment and selection statements
      - applying scoping guidance/considerations
      - selecting/specifying compensating controls
      - assigning control values
      - tailoring matches your organization's mission and the controls from a selected baseline.
      - Tailoring ensures that assessment methods are appropriate to the systems, services, and other assets that are being validated and is the best answer here. 
  - üî¥**Scoping**: limiting the general baseline recommendations by removing those that do not apply; part of the tailoring process and refers to reviewing a list of baseline ‚úèÔ∏è security controls ‚úèÔ∏è and selecting only those controls that apply to the systems you're trying to protect
    - scoping processes eliminate controls that are recommended in a baseline
    - Scoping is a part of the tailoring process and refers to reviewing a list of security controls and selecting the security controls that apply.
    - Scoping is the process of reviewing and selecting security controls based on the system that they will be applied to. 
    - Scoping involves selecting only the controls that are appropriate for your IT systems
    - Scoping involves setting the boundaries of security control implementations.

- 2.6.3 Standards selection
  - Organizations need to identify the standards (e.g. PCI DSS, GDPR etc) that apply and ensure that the security controls they select fully comply with these standards
  - Even if the org doesn't have to comply with a specific standard, using a well-designed community standard can be helpful (e.g. NIST SP 800 documents)
  - **Standards selection**: the process by which organizations plan, choose and document technologies or architectures for implementation
    - e.g. you evaluate three vendors for a security control; you could use a standards selection process to help determine which solution best fits the org
  - Vendor selection is closely related to standards selection but focuses on the vendors, not the technologies or solutions
  - The overall goal is to have an objective and measurable selection process 
    - if you repeat the process with a totally different team, the alternate team should come up with the same selection

- 2.6.4 Data protection methods (e.g., Digital Rights Management (DRM), Data Loss Prevention (DLP), Cloud Access Security Broker (CASB)) 
  - **Data protection methods** include: 
    - **Digital Rights Management (DRM)**: methods used in attempt to protect copyrighted materials. there is hardware and software based DRMs. Methods used with DRM include:
         - Persistent online authentication
         - Automatic expiration
         - Continuous audit trail  
    - **Cloud Access Security Brokers (CASBs)**: software placed logically between users and cloud-based resources ensuring that cloud resources have the same protections as resources within a network and  providing monitoring and policy enforcement capabilities
         - A cloud access security broker (CASB) is software placed logically between users and cloud-¬≠based resources, and it can enforce security policies used in an internal network.
      - note that entities must comply with the EU GDPR, and use additional data protection methods such as pseudonymization, tokenization, and anonymization
  - One of the primary methods of protecting the confidentiality of data is encryption
  - Options for protecting your data vary depending on its state:
    - üåü**Data at rest**: consider encryption for operating system volumes and data volumes, and backups as well
      - be sure to consider all locations for data at rest, such as tapes, USB drives, external drives, RAID arrays, SAN, NAS, and optical media
      - ‚úèÔ∏èAES is a strong modern symmetric encryption algorithm that is appropriate for encrypting data at rest. Using strong encryption, like AES-256, can help ensure that loss of removable media like tapes doesn't result in a data breach.
      - ‚úèÔ∏èFull disk encryption like Bitlocker can protect data at rest.
      - Tapes may be vulnerable to theft or loss in transit. That means that tapes that are leaving their normal storage facility should be handled according to the organization's classification schemes and handling requirements.
      - ‚úèÔ∏èDRM is useful for data at rest because DRM "travels with the data" regardless of the data state
      - For example, A watermark is used to digitally label data and can be used to indicate ownership, as well as to assist a digital rights management (DRM) system in identifying data that should be protected.
      - DRM is especially useful when you can‚Äôt encrypt data volumes
    - üåü**Data in transit**: think of data in transit wholistically -- moving data from anywhere to anywhere; use ‚úèÔ∏èencryption for data in transit 
      - TLS is frequently used to secure data when it is in transit.
      - e.g. a web server uses a certificate to encrypt data being viewed by a user, or IPsec encrypting a communication session 
      - most important point is to use encryption whenever possible, including for internal-only web apps
      - ‚úèÔ∏èDLP solutions are useful for data in transit, scanning data on the wire, and stopping the transmission/transfer, based on the DLP rules set (e.g. outbound data that contains numbers matching a social security number pattern, a DLP rule can be used to block that traffic)
    - üåü**Data in use**: 
      - ‚úèÔ∏èCASB solution often combines DLP, a web application firewall with some type of authentication and authorization, and a network firewall in a single solution; A CASB solution is helpful for protecting data in use (and data in transit)
  - üü¢**Pseudonymization**: refers to the process of using pseudonyms or alias to represent other data
       - A pseudonym is an alias, and pseudonymization can prevent data from directly identifying an entity (i.e. person)
       - the process can be reversed
  - üü¢**Tokenization**: use of a token, typically a random string of characters that ‚úèÔ∏èremains the same for each instance of that data, to replace other data. Tokenization replaces other data with a random string of characters. These tokens are then matched to the actual values for secure lookups as needed. 
       - note that tokenization is similar to pseudonymization in that they are both used to represent other data, and the token or pseudonym have no meaning or value outside the process that creates and links them to that data
       - If company are reselling products to the same customers, they can use tokenization to save tokens that match the credit card data, instead of saving and storing credit card data.
       - example of tokenization used in Credit Card transactions:
       - registration: app on user's smart phone securely sends CC info to the credit card processor (CCP)
       - The CCP sends the CC info to a tokenization vault, creating a token and associating it with the user's phone
       - usage: when the user makes a purchase, the POS system sends the token to the CCP for authorization
       - validation: the CCP sends the token to the tokenization vault; the vault replies with the CC info, the charge is processed
       - completing the sale: the CCP sends a reply to the POS indicating the charge is approved
       - this system prevents CC theft at the POS system
  - üü¢**Anonymization** removes all personally identifiable data to ensure that the original subject cannot be identified.
       - Anonymization techniques remove all personal data and make the data unusable for reuse on the website. Techniques of Data Anonymization
       - 1. Data masking: obscures some, but not all, data
         2. Pseudonymization: Pseudonymization involves replacing identifiable information with pseudonyms or identifiers that do not directly reveal the identity of individuals. The original data can only be restored if additional information, kept separate, is used.
         3. Generalization
         4. Data swapping: Swapping data involves exchanging data between different records, which can still potentially allow for some degree of identification, especially if the data structure or patterns are recognizable.
         5. Data perturbation: is the use of false or misleading data in a database management system in order to redirect or thwart information confidentiality attacks.
         6. Synthetic data
  - üü¢**Randomizing data** Randomizing data is a common approach to anonymization. It involves replacing PII with artificial but realistic-looking data. This technique ensures that the data in a test environment does not correspond to any actual individuals, thus protecting privacy while maintaining the data‚Äôs utility for testing purposes.

- **Policy ‚ñ∂Ô∏è Standard ‚ñ∂Ô∏è Baseline ‚ñ∂Ô∏è Guideline ‚ñ∂Ô∏èProcedure**
    - **Policy**: Policy is High level from Management 
    - **Standard** mandatory, must meet EXACTLY, no more, no less e.g DoD 8570, AR 25-2, NIST SP , 800 53
    - **Baseline** mandatory, must meet AT LEAST, can do more than it requires e.g CIS Benchmarks
    - **Guideline** suggested practices, not mandatory e.g DoD STIGs, Microsoft NSA, PCI DSS, NIST 800-88

- **Salami Attack** is a type of cyberattack or fraud where a perpetrator takes small, seemingly insignificant actions that individually appear harmless but collectively result in a substantial impact. The term "salami" is used metaphorically, implying that the attack slices off small pieces, like slicing a salami, which are not noticeable individually but accumulate to cause significant harm.
- **Bitrot** describes the slow loss of data on aging media
