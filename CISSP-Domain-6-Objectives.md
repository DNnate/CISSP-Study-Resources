[Domain 6](#domain6-top) **Security Assessment and Testing**
- Security assessment and testing programs are an important mechanism for validating the on-going effectiveness of security controls
    - they include a variety of tools, such as vulnerability assessments, penetration tests, software testing, audits, and other control validation
    - Every org should have a security assessment and testing program defined and operational
- **Artifact**: piece of evidence such as text, or a reference to a resource which is submitted in response to a question
- **Assessment**: testing or evaluation of controls to understand which are implemented correctly, operating as intended and producing the desired outcome in meeting the security or privacy requirements of a system or org
- **Audit**: process of reviewing a system for compliance against a standard or baseline (e.g. audit of security controls, baselines, financial records) can be formal and independent, or informal/internal
- **Chaos Engineering**: discipline of experiments on a software system in production to build confidence in the system's capabilities to withstand turbulent/unexpected conditions
- **Compliance Calendar**: tracks an org's audits, assessments, required filings, due dates and related
- **Compliance Tests**: an evaluation that determines if an org's controls are being applied according to management policies and procedures
- **Penetration Testing/Ethical Penentration Testing**: security testing and assessment where testers actively attempt to circumvent/defaut a system's security features; typically constrained by contracts to stay within specified Rules of Engagement (RoE)
- **Examination**: process of reviewing/inspecting/observing/studying/analyzing specs/mechanisms/activities to understand, clarify, or obtain evidence
- **Findings**: results created by the application of an assessment procedure
- **Judgement Sampling**: AKA purposive or authoritative sampling, a non-probability sampling technique where members are chosen only on the basis of the researcher's knowledge and judgement
- **Misue Case Testing**: testing strategy from a hostile actor's point of view, attempting to lead to integrity failures, malfunctions, or other security or safety compromises
- **Plan of Action and Milestones (POA&M)**: a document indentifying tasks to be accomplished, including details, resources, milestones, and completion target dates
- **RoE**: Rules of Engagement, set of rules/constraints/boundaries that establish limits of participant activity; in ethical pen testing, an RoE defines the scope of testing, and to establish liabilty limits for both testers and the sponsoring org or system owners
- **Statistical Sampling**: process of selecting subsets of examples from a population with the objective of estimating properties of the total population
- **Substantive Test**: testing technique used by an auditor to obtain the audit evidence in order to support the auditor's opinion
- **Testing**: process of exersizing one or more assessment objects (activities or mechanisms) under specified conditions to compare actual to expected behaior
- **Trust Services Criteria (TSC)**: used by an auditor when evaluating the suitability of the design and operating effectiveness of controls relevant to the security, availabiliity, or processing integrity of information and systems or the confidentiality or privacy of the info processed by the entity

- ğŸ“‚**Security assessments**: comprehensive reviews of the security of a system, application, or other tested environment
    - during a security assessment, a trained information security professional performs a risk assessment that identifies vulnerabilities in the tested environment that may allow a compromise and makes recommendations for remediation, as needed
    - a security assessment includes the use of security testing tools, but go beyond scanning and manual penetration tests. Security assessments are comprehensive reviews of the security of a system, application, or other tested environments. During a security assessment, a trained information security professional performs a risk assessment that identifies vulnerabilities in the environment that may allow a compromise and makes recommendations for remediation.
    - Security assessments involve a trained professional performing a comprehensive review of the environment under test. It is a more thorough assessment of the security of the threat environment than a security test.
    - Security assessments are generally performed at the direction of ğŸ“senior management. They evaluate the effectiveness of the organization's information security program. Reports should be written in non-technical language that management can understand. More technical info can be distributed by management to their teams to implement the report's recommendations.
        - ğŸ› ï¸Security Controls Tests: are conducted to verify that a control reduces risk to an acceptable level. Security tests validate that a security control is doing its job. They range from fully-automated scans to manual penetration testing. Among other things, these tests can include scans, penetration tests, and social engineering. A score can be given if needed, but the purpose is not to generate a score. Security testing comes in a variety of different forms. Some of the main types include:
            - âœˆï¸Penetration testing: Penetration testing includes vulnerability assessments but also attempts to exploit the vulnerabilities that they discover. Additionally, they involve more manual testing than a fully-automated vulnerability scan.
            - âœˆï¸Vulnerability assessments: Vulnerability assessments use automated tools to identify vulnerabilities within an organizationâ€™s systems, including networks, web applications, and databases. Typically, these check an application against the list of Common Vulnerabilities and Exposures (CVEs) to identify known issues. These vulnerabilities are scored using the Common Vulnerability Scoring System (CVSS), which helps in identifying how severe the vulnerability is.
            - âœˆï¸Breach and attack simulations (BASs): BAS platforms are designed to perform automated penetration testing. They introduce indicators of compromise (IoCs) into the system â€” malicious files, C2 traffic, etc. â€” and check if the corporate security controls identify it.
            - âœˆï¸Compliance checks: Compliance checks verify that an organization implements the security controls required by applicable regulations and that they are functional.
        - ğŸ› ï¸Security Program Test: If all of the controls are tested, then it is possible to test the effectiveness of the security program. Having a security program shows due diligence and due care. The  International Standards Organization ğŸ”¥(ISO) 27001 requires the occasional review of any security program by management.
    - the main work product of a security assessment is normally an assessment report addressed to management that contains the results of the assessment in nontechnical language and concludes with specific recommendations for improving the security of the tested environment
    - Security assessment reports should be addressed to the organization's management. For this reason, they should be written in plain English and avoid technical jargon.
    - Security assessments include many types of tests designed to identify vulnerabilities, and the assessment report normally includes recommendations for mitigation. The assessment does not, however, include actual mitigation of those vulnerabilities.
    - The ğŸ“sensitivity of information stored on the system, ğŸ“difficulty of performing the test, and ğŸ“likelihood of an attacker targeting the system are all valid considerations when planning a security testing schedule.
    - IT staff may perform security assessments to evaluate the security of their systems and applications. However, audits must be performed by internal or external auditors who are independent of the IT organization.
    - 
- ğŸ“‚**Security Audit**: Security audits are performed by accredited, ğŸ“third-party auditors. They are designed to prove to regulators or customers that the organizationâ€™s security controls meet the requirements of a particular regulation or standard. An organizationâ€™s audit strategy will depend on its size, industry, financial status and other factors
    - Security audits are performed to validate that security controls have been implemented and work as desired. Security audits generally compare results against an ğŸ“external standard.
    - Unlike security assessments, security audits are generally performed by an ğŸ“external group to prevent conflicts of interest.
    - The audience of a security audit report would be people outside of the company's day-to-day operations, such as the board of directors, government regulators, or third parties.
    - a small non-profit, a small private company and a small public company will have different requirements and goals for their audit strategies
    - the audit strategy should be assessed and tested regularly to ensure that the organization is not doing a disservice to itself with the current strategy
    - there are three types of audit strategies: internal, external, and third-party
    - SSAE 18 is a US auditing system while ISAE 3402 is a very similar system used internationally. Both are standards used by companies to audit other companies in their jurisdiction.
    - ğŸ“Non-disclosure agreements should be prepared prior to an audit. This is so all parties acknowledge that a reasonable time frame will be given to remediate potential vulnerabilities found. Not only is this done to conduct business in an ethical manner by being reasonable, but it's also to avoid jeopardizing the business and opening the doors to an increase in threats.

[6.1](#6.1) Design and validate assessment, test, and audit strategies (OSG-9 Chpt 15)
- 6.1.1 ğŸ”´Internal
    - An organizationâ€™s security staff can perform security tests and assessments, and the results are meant for internal use only, designed to evaluate controls with an eye toward finding potential improvements
    - An internal audit strategy should be aligned to the organizationâ€™s business and day-to-day operations
        - e.g. a publicly traded company will have a more rigorous internal auditing strategy than a privately held company
    - Designing the audit strategy should include laying out applicable regulatory requirements and compliance goals
    - Internal audits are performed by an organizationâ€™s internal audit staff and are typically intended for internal audiences
    - Having an audit done by someone inside the company, such as an internal auditor, functional management, or the chief information security officer, would be a ğŸ“conflict of interest.
    - Internal audits should not be used to demonstrate compliance against an external standard for government or industry compliance. This is because internal audits present a conflict of interest. An organization should use a third party to audit them when audit reports are used for compliance reasons.
    - Internal audits should be used in preparation for an external or third-party audit, which is where a company can demonstrate compliance with external standards, laws, or regulations. Regular internal audits also show due care and due diligence. Internal audits ultimately are used to provide assurances to the organization's ğŸ“stakeholders.
    - It is entirely appropriate to distribute internal audit reports to anyone in the organization who has a valid need to know. This may include both management and individual contributors responsible for remediating issues as well as board members charged with oversight
    - Internal auditors are useful when reporting to senior management of the organization but are typically not asked to report to third parties. 
    - The chief audit executive (CAE) should report to the most senior possible leader e.g CEO to avoid conflicts of interest.  It is also possible to provide an added degree of independence by having the CAE report to the board of directors, either as a primary reporting line or as a dotted line relationship.
- 6.1.2 ğŸ”´External
    - An external audit strategy should complement the internal strategy, providing regular checks to ensure that procedures are being followed and the organization is meeting its compliance goals
    - External audits are performed by an outside auditing firm
        - these audits have a high degree of external validity because the auditors performing the assessment theoretically have no conflict of interest with the org itself
        - audits by these firms are generally considered acceptable by most investors and governing bodies
        - External audits are performed by an external organization. Often, they are ğŸ“paid for by the organization, which has a certain level of control over them. However, external audits are considered more trustworthy than internal audits, since external auditors are independent of the company under test.
        - External auditors can provide an unbiased and impartial view of an organization's controls to third parties. 
- 6.1.3.1 ğŸ”´Second-party
    - Second-Party audits typically involve ğŸ“business partners auditing each other. A great example of this would be an enterprise auditing a ğŸ“cloud provider prior to migrating data. These are useful to ensure all parties are comfortable working with each other and for enhancing trust, but they aren't as useful when it comes to meeting regulations. 
- 6.1.3 ğŸ”´Third-party
    - Third-party audits are conducted by, or on behalf of, another org. This can also be an unrelated firm that has the ability to be fair and impartial regarding the audited business.
    - The cost and lack of familiarity with an organization's systems can be a drawback to third-party audits. For example, someone who is unfamiliar with the systems will take longer to conduct an audit. Additionally, they may require additional resources that an internal employee wouldn't need, which may include having someone show them around the building or familiarizing the auditor with the structure of networks.
    - Third-party audits are controlled and conducted by third parties. A ğŸ“regulator may select or accredit the auditor and can use the results to determine compliance.
    - Since a third party has no vested interest in the company they're auditing, it's fair to say the auditor would be impartial during the audit. With that said, they would have less of a reason to not be transparent with whoever they may be auditing.
    - In the case of a third-party audit, the org initiating the audit generally selects the auditors and designs the scope of the audit
    - Large organizations hire  Qualified Security Assessor QSAs, or qualified security assessors, to conduct compliance checks. Third-party certification is required for large organizations by PCI-DSS, although smaller organizations can self-certify.
    - The major audit firms are PricewaterhouseCoopers, KPMG, Deloitte, and Ernst & Young. 
    - â„ï¸**Standards for Attestation Engagements document 18 (SSAE 18)**, The statement on SSAE 18 titled Reporting on Controls, provides a common standard to be used by auditors performing assessments of service orgs with the intent of allowing the org to conduct external assessments, instead of multiple third-party assessments, and then sharing the resulting report with customers and potential customers
        - outside of the US, similar engagements are conducted under the International Standard for Attestation Engagements (ISAE) 3402, Assurance Reports on Controls at a Service Organization
    - SSAE 18 and ISAE 3402 engagements are commonly referred to as a service organization controls (SOC) audits. Both ISAE 3402 and SSAE 18 result in Service Organization Controls (SOC) one, two, and three reports, which determine the level of disclosure between the company and the public on matters. These organizations also conduct type one and type two audits, which can be simply verifying documentation or conducting a physical assessment.
    - ğŸ”¥SSAE 18: The Statements on Standards for Attestation Engagements 18 (SSAE 18) is conducted in the ğŸ“United States. Specifically, it is a set framework for companies to conduct third-party audits. It upholds the integrity of the industry by maintaining a set standard for all audits.
    - ğŸ”¥ISAE 3402: ISAE 3402, which covers international auditing anywhere except the USA. ISAE 3402 is used within the ğŸ“European Union (EU) as well as most of the planet (aside from the US).
    - Three forms of SOC audits:
        - ğŸ“™**SOC 1 Engagements**: assess the organizationâ€™s controls that might impact the accuracy of financial reporting
        - ğŸ“™**SOC 2 Engagements**: assess the organizationâ€™s controls that affect the security (confidentiality, integrity, and availability) and privacy of information stored in a system.  SOC-2 reports focus on more than just financials, they cover the five trust principles. The five trust principles are security, confidentiality, processing integrity, availability, and privacy. 
            - SOC 2 audit results are confidential and are usually only shared outside an org under an NDA
        - ğŸ“™**SOC 3 Engagements**: assess the organizationâ€™s controls that affect the security (confidentiality, integrity, and availability) and privacy information stored in a system. 
            - however, SOC3 audit results are intended for ğŸ“public disclosure, and intended for distribution to third parties
    - Two types of SOC reports:
        - ğŸ’¥**Type I Reports**: provide the auditorâ€™s opinion on the description provided by management and the suitability of the design of the controls
            - type I reports also cover only a specific ğŸ“point in time, rather than an extended period
            - think of Type I report as more of a documentation review
            -  takes a â€œsnapshot-in-timeâ€ approach, setting a baseline for future audits of your service organizationâ€™s system.
            -  Type I reports provide the auditor's opinion on the description of controls provided by management and the suitability of the design of those controls, ğŸ“without performing any testing of the controls.
        - ğŸ’¥**Type II Reports**: go further and also provide the auditorâ€™s opinion on the operating effectiveness of the controls
            - the auditor actually confirms the controls are functioning properly
            - Type II reports also cover an extended ğŸ“period of time, at least 6 months
            - think of Type II report as similar to a traditional audit; the auditor is checking the paperwork, and verifying the controls are functioning properly
            -  Type II reports go further and also provide the auditor's opinion on the operating effectiveness of the controls based upon testing. 
        - Type II reports are considered much more reliable than Type I reports (Type I reports simply take the service orgs word that the controls are implemented as described)
     
- 6.1.4 Location for Auditing
    - Third-party audits are conducted by, or on behalf of, another org
    - ğŸŸ¢**On-Premise**:
        - Physical Access: Auditors typically need physical access to hardware, network devices, and data centres for inspection, configuration reviews and testing
        - Control and Visibility: Organisation retains greater control over its infrastructure thus auditors have ore direct visibility into security configurations
        - Scope: Audits may be focused on traditional IT infrastructure , including network security, operating systems, physical safeguards etc
    - ğŸŸ¢**Cloud**:
        - Shared Responsibility: Security is a shared responsibility between the organisation and the cloud provider. Audit focus shifts towards how the organisation uses the cloud services
        - Documentation & APIs: Auditors typically will rely on heavily on cloud provider documentations, service configurations, and access to relevant APIs for gathering evidence.
        - Compliance Focus: Emphasis often on meeting cloud-specific security standards (e.g SOC 2, ISO 27001, FedRAMP), and the organisations configuration of cloud services. CSPs provide attestation documents for certification of the underlying infrastructure but the customer still needs to configure the cloud service in a compliant manner.
        - Customer right-to-audit is often limited, and be  mindful of rules of engagement when it comes to penetration testing
    - ğŸŸ¢**Hybrid**:
        - Increased Complexity: presents the most complexity due to the mix of on-premise and cloud components that need auditing
        - Data Flows: Understanding how data moves between on-premise and cloud environments is crucial for risk assessment
        - Integrated Controls: Auditors must evaluate the evaluate the effectiveness of security controls accros the entire hybrid landscape.

- **Assurance Challenges with Virtualization & Cloud**: The cloud is made possible by virualization technologies that enable dynamic environments needed for a global provider platform.
    -  Depending on the cloud arhitecture deployed, the cloud security professional may need to perorm multiple layers of auditing.
    -  To be effective, the auditor must understand the virualization architecture of the cloud provider
        - ğŸ”¨Provider (Cloud Service Provider): The CSP i.e Microsoft, Amazon & Google will usually be responsible for the audits of controls over the hypervisor. Most CSPs will not allow customers to conduct an audit of their systems and infrastructure. You are likely to be able to obtain third-party audit reports which typically either make these available publicly or to paying or prospective customers
        - ğŸ”¨Customer (Cloud conusmers):  VMs deployed on top of the hardware are usually owned by the customer
        - **Right-to-Audit**:
            - The customer can request right-to-audit the service provider to ensure compliance with the security requirements agreed in the contract
            - Contracts are often written to allow CSPs standard audits (SOC 2, ISO 27001) to be used in place of customer-performed audit.
        - **Metrics**: If there are specific indicators that the CSP must provide to the customer, they should be documented in a contract
            - Metrics tell you how compliance with the agreement will be measured
        - Major CSPs make SOC 2 Type II (or other report) available on demand 
        - The use of vulnerability scanners and pen testers may be limited by the CSP's terms of service or cloud contractual arrangment.

[6.2](#6.2) Conduct security control testing (OSG-9 Chpt 15)
- Security control testing can include testing of the physical facility, logical systems and applications; common testing methods:
- 6.2.1 Vulnerability assessment
    - **Vulnerabilities**: weaknesses in systems and security controls that might be exploited by a threat
        - Vulnerability assessments: examining systems for these weaknesses
        - The goal of a vulnerability assessment is to identify elements in an environment that are not adequately protected and not necessarily from a technical perspective; you can also assess the vulnerability of physical security or the external reliance on power, for instance
        - can include personnel testing, physical testing, system and network testing, and other facilities tests
        - Vulnerability assessments are some of the most important testing tools in the information security professionalâ€™s toolkit
    - â„ï¸**Security Content Automation Protocol (SCAP)**: provides a common framework for discussion and facilitation of automation of interactions between different security systems (sponsored by NIST)
        - SCAP components related to vulnerability assessments:
            - ğŸ“š**Common Vulnerabilities and Exposures (CVE)**: MITRE's CVE provides a naming system for describing security vulnerabilities. The Common Vulnerabilities and Exposures (CVE) database provides a consistent reference for identifying security vulnerabilities.  NIST's National Vulnerability Database (NVD) also provide information about vulnerabilities. It is used to reconcile the identity of vulnerabilities generated by different security assessment tools
            - ğŸ“š**Common Vulnerability Scoring Systems (CVSS)**: CVSS, the Common Vulnerability Scoring System, is used to describe the severity of security vulnerabilities. It provides a standardized scoring system for describing the severity of security vulnerabilities. The score can range from 0.0 (least severe) to 10.0 (most severe). The CVSS includes metrics and calculation tools for exploitability, impact, how mature exploit code is, and how vulnerabilities can be remediated, as well as a means to score vulnerabilities against users' unique requirements. The CVSS 3.1 scoring system rates vulnerabilities on a low, medium, high, or critical scale. They are defined by the following range:
                - 0.1-3.9 (Low)
                - 4.0-6.9 (Medium)
                - 7-8.9 (High)
                - 9-10 (Critical)
            - ğŸ“š**Common Configuration Enumeration (CCE)**: CCE is Common Configuration Enumeration, a naming system for configuration issues. It provides a naming system for system config issues. Configuration issues naming and used to describe system configurations during vulnerability analysis.
            - ğŸ“š**Common Platform Enumeration (CPE)**: provides a naming system for ğŸ“operating systems, applications, and devices. CPE is Common Platform Enumeration, which names operating systems, applications, and devices. 
            - ğŸ“š**Extensible Configuration Checklist Description Format (XCCDF)**: provides a standardised language for specifying security checklists
            - ğŸ“š**Script Check Engine (SCE)** is designed to make scripts Â­interoperable with security policy definitions.
            - ğŸ“š**Open Vulnerability and Assessment Language (OVAL)**: provides a language for describing security testing procedures
    - â„ï¸**Vulnerability Scanners** Vulnerability scans automatically probe systems, applications, and networks looking for weaknesses that could be exploited by an attacker. Vulnerability Scanners scan remote machines to gather information, including fingerprints from responses to queries and connections, banner information from services, and related data.
    - Vulnerability scanners cannot detect vulnerabilities for which they do not have a test, plug-in, or signature. Signatures often include version numbers, service fingerprints, or configuration data. They can detect local vulnerabilities as well as those that require authentication if they are provided with credentials, and of course, they can detect service vulnerabilities.
    - Vulnerability scanners can check systems to ensure they are up-to-date with current patches (along with other checks) and are an effective tool to ğŸ“verify the patch management program.
    - Four main categories of vulnerability scans:
        - ğŸ“—Network discovery scans: Network discovery scanners use many different techniques to identify open ports on remote systems:
            - ğŸ””TCP SYN Scanning (half-Â­open) Sends a single packet to each scanned port with the SYN flag set. This indicates a request to open a new connection. If the scanner receives a response that has the SYN and ACK flags set, this indicates that the system is moving to the second phase in the three-Â­way TCP handshake and that the port is open. TCP SYN scanning is also known as ğŸ“â€œhalf-Â­openâ€ scanning. The TCP SYN scan sends a SYN packet and receives a SYN ACK packet in response, but it does not send the final ACK required to complete the three-way handshake. TCP SYN scans require ğŸ“elevated privileges on most Linux systems due to the need to write raw packets.
            - ğŸ””TCP Connect Scanning Opens a ğŸ“full connection to the remote system on the specified port, , meaning that the scanner replies with an ACK to complete the TCP three-way handshake. This scan type is used when the user running the scan does not have the â›”necessary permissions to run a half-Â­open scan. Most other scan types require the ability to send raw packets, and a user may be restricted by the operating system from sending hand-crafted packets. The TCP SYN scan sends a SYN packet and receives a SYN ACK. When a tester ğŸ“does not have raw packet creation privileges, such as when they have not escalated privileges on a compromised host, a TCP connect scan can be used.
            - ğŸ””TCP ACK Scanning Sends a packet with the ACK flag set, indicating that it is part of an open connection. This type of scan may be done in an attempt to determine the rules enforced by a firewall and the firewall methodology. The TCP ACK scan sends an ACK packet, simulating a packet from the middle of an already established/open connection. It is also a half-open scan and only sends a packet with the ACK flag set
            - ğŸ””UDP Scanning Performs a scan of the remote system using the UDP protocol, checking for active UDP services. This scan type does not use the three-Â­way handshake, because UDP is a connectionless protocol.
            - ğŸ””Xmas Scanning Sends a packet with the FIN, PSH, and URG flags set. A packet with so many flags set is said to be â€œlit up like a Christmas tree,â€ leading to the scanâ€™s name.
            - ğŸ©¹**Nmap** only scans 1000 TCP and UDP ports by default, including ports outside the 0â€“1024 range of â€œwell-knownâ€ ports. By using the defaults, you miss 64,535 ports! ğŸ“Both TCP and UDP port numbers are a 16-digit binary number, which means there can be 216 ports, or 65,536 ports, numbered from 0 to 65,535.
            - Nmap states include:
                - ğŸ”¨Open Port: The port is accessible on the remote system and an application is accepting connections on that port.
                - ğŸ”¨Closed Port: TPort is accessible but not accepting connections. The system is responding to Nmapâ€™s probes, but no service is running on that port. It means the port is accessible but not currently in use.
                - ğŸ”¨Filtered Port: Nmap us unable to determine if it is open or closed due to a firewall. The port is behind a firewall, router, or some security device that is blocking or filtering Nmapâ€™s probes, preventing Nmap from determining its status.
        - ğŸ“—Network vulnerability scans e.g nmap, nikto, OpenVAS, QualysGuard, Nessus, Rapid7, Tenable. Nmap, Nessus, and Nikto all have OS fingerprinting or other operating system identification capabilities. Network vulnerability scanners are used to discover systems that have known vulnerabilities such as missing patches.
            - ğŸ©¹**OpenVAS**: OpenVAS is an open source vulnerability scanning tool that provide report of the vulnerabilities that it can identify from a remote, network-based scan.
            - Web vulnerability scanner is a type of network scanner.
        - ğŸ“—Web application vulnerability scans e.e OWASP, Nikto, Burp Suit, Nessus, Arachni, W3af, Wapiti. They detect web vulnerabilities such as ğŸ“SQL injection and ğŸ“Cross-site scripting attacks. While SQL injection attacks do target databases, they do so by using web servers as intermediaries. Therefore, SQL injection attacks take place over web ports, such as 80 and 443, and not database ports, such as 1433 and 1521.
        - ğŸ“—Database vulnerability scans e.g SQLMap, DBProtect, Rapid7, Oracle DBSAT, IBM Guardium, Nessus. sqlmap is custom-designed to detecting database vulnerabilities.
    - âœ´ï¸Active Scanning is useful for testing IDS or IPS systems. Scripted attacks are part of active scanning
    - âœ´ï¸Passive scanning can help identify rogue devices by capturing MAC address vendor IDs that do not match deployed devices, by verifying that systems match inventories of organizationally owned hardware by hardware address, and by monitoring for rogue SSIDs or connections.
    - ğŸ””Authenticated Vulnerability scans use a read-only account to access configuration files, allowing more accurate testing of vulnerabilities. It  provides the most accurate and detailed information about the security state of a server.
        - Authenticated scans can read configuration information from the target system and reduce the instances of false positive. Authenticated scanning utilizes credentials for a more complete (and accurate) scan of the device. This may reduce ğŸ“false positive results (where a vulnerability that isn't present is reported as being present); 
    - ğŸ””Vulnerability scanners rely on a database of known vulnerabilities for detection. If this database is outdated, vulnerabilities that are present may go undetected (which is referred to as false negative results). Therefore, the database should be updated before each scan. This will help to reduce ğŸ“false negative results. 

- 6.2.2 â„ï¸Penetration testing
    - Penetration testing is the attempt to bypass security controls to test overall system security
        - Scope and depth come into play during every penetration test. A client will tell a penetration tester exactly what needs to be tested and how it will be tested. This is especially important, as failing to abide by these specifications may jeopardize an active workplace or even damage equipment.
    - Scope and depth also relate to the permission given to a penetration tester, so going outside that permission range could lead to liability if things go wrong.
        - Scope defines the systems involved in the test;
        - Depth defines the detail of the act 
    - Penetration tests goes beyond vulnerability testing techniques because it actually attempts to exploit systems. In most organizations, senior management needs to approve penetration tests due to the risk to the organization and the potential impact of the test. In a small number of organizations, the service owner may be able to make this decision, but penetration tests often have broader impacts than a single service, meaning that senior management is the proper approval path.
        - âš’ï¸Advantages:
            - Can be fast (and therefore cheap)
            - Requires a relatively lower skill-set than source code review
            - Tests the code that is actually being exposed
        - âš’ï¸Disadvantages:
            - Too late in the SDLC
            - Front-impact testing only
            - penetration testing can only cover the point in time when it is conducted. 
    -  ğŸ“Application crashes; ğŸ“denial of service due to system, ğŸ“network failures, or ğŸ“application failures; and ğŸ“data corruption can all be hazards of penetration tests.
    - The following are the five phases of a penetration test:
        - ğŸˆPlanning/Footprinting
        - ğŸˆInformation gathering and discovery/Enumeration
        - ğŸˆVulnerability scanning
        - ğŸˆExploitation
        - ğŸˆReporting
    - NIST defines the penetration testing process as consisting of four phases: PIAR ğŸŸ¡
    - ğŸš¡**planning**: includes agreement on the scope of the test and the rules of engagement
        - ensures that both the testing team and management are in agreement about the nature of the test and that it is explicitly authorized
        - determining how vulnerability data should be stored and sent is critical becuase penetration test reports often include information that could result in additional exposure if they were accidentally released or stolen.
    - ğŸš¡**information gathering and discovery**: uses manual and automated tools to collect information about the target environment
        - basic reconnaissance (website mapping)
        - Discovery can include both active and passive discovery.
            - ğŸˆPassive: Searching Domain Name System (DNS) records involves using command line input to gather open-source information which the client would not be alerted to. Physical observation and visiting the company's website would not necessarily alert a client.
            - ğŸˆActive: Synchronize (SYN) scans would alert the client that a SYN packet reached their port. A pattern of these SYN packets to different ports may then trigger an alert in an intrusion detection system.
        - Port scanning is commonly done during discovery to assess what services the target provides, and nmap is one of the most popular tools used for this purpose. 
        - network discovery e.g using nmap, dumpster diving, War dialing (an attempt to locate modems and fax machines by dialing phone numbers).
        - testers probe for system weaknesses using network, web and db vuln scans e.g Nessus and Nikto
    - ğŸš¡**attack**: seeks to use manual and automated exploit tools to attempt to defeat system security
        - step where pen testing goes beyond vuln scanning as vuln scans donâ€™t attempt to actually exploit detected vulns
        - NIST 800-115 specifies four attack phase steps: gaining access, escalating privileges, system browsing, and installing additional tools. Once attackers install additional tools, penetration testers will typically use them to gain additional access.
        - e.g John, Metasploit, privilege escalation
            - Metasploit is an exploitation package that is designed to assist penetration testers. A tester using Metasploit can exploit known vulnerabilities for which an exploit has been created or can create their own exploits using the tool. Metasploit provides an extensible framework, allowing penetration testers to create their own exploits in addition to those that are built into the tool. While Metasploit provides built-in access to some vulnerability scanning functionality, a tester using Metasploit should primarily be expected to perform actual tests of exploitable vulnerabilities. Similarly, Metasploit supports creating buffer overflow attacks, but it is not a purpose-built buffer overflow testing tool,
    - ğŸš¡**Reporting**: summarizes the results of the pen testing and makes recommendations for improvements to system security. Penetration testing reports often do not include the specific/sensitive data captured during the assessment, as the readers of the report may not be authorized to access all of the data, and exposure of the report could result in additional problems for the organization. A listing of the issues\vulnerabilities discovered, risk ratings, and remediation guidance are all common parts of a penetration test report. Pentest reports should be secured because they contain information about the vulnerabilities of the system and disclosure of such vulnerabilities to the wrong person could lead to security breaches. 
        - Vulnerability reports can be classified as:
            - ğŸ”–True positive report: the scan detected the vulnerability and the vulnerability actually existed.
            - ğŸ”–True negatives occur when scans correctly note the absence of a vulnerability.
            - ğŸ”–False positives occur when scans report the presence of a vulnerability that does not actually exist.
            - ğŸ”–False negatives occur when scans report that no vulnerability exists when one does, in fact, exist.
    - Penetration tests are normally categorized into three groups:
        - ğŸ”´**white-box penetration test (crystal or Full knowledge or Open-box or Clear-box)**:
            - provides the attackers with **detailed information** about the systems they target
            - this bypasses many of the reconnaissance steps that normally precede attacks, shortening the time of the attack and increasing the likelihood that it will find security flaws
            - When the penetration testers are provided detailed information, it is referred to as a full-knowledge test (white box).
            - these tests are sometimes called "**known environment**" tests
            - To fully test code, a white-box test is required. Without full visibility of the code, error conditions or other code could be missed.
            - Open-box (formerly white) testing is the exact opposite, where the penetration tester has full knowledge of the environment. This test is sometimes more accurate, providing fewer false positives of vulnerabilities and leaving no stone unturned.
            - This is more likely to find and exploit vulnerabilities used by insider threats. 
        - ğŸ”´**gray-box penetration test**:
            - AKA **partial knowledge tests**, these are sometimes chosen to balance the advantages and disadvantages of white- and black-box penetration tests
            - In a gray-box test, the tester evaluates the software from a user perspective but has access to the source code as the test is conducted.
            - In grey-box or partial testing, the penetration testers are provided with partial information before testing. This can help the tester potentially find vulnerabilities and exploit them more than if the tester knew no information about the environment before testing.
            - this is particularly common when black-box results are desired but costs or time constraints mean that some knowledge is needed to complete the testing
            - Partial-box (formerly grey) testing is when the tester has some internal knowledge of the environment. Partial knowledge emulates an attack coming from someone such as a customer, contractor, vendor, or employee who has some knowledge of the business before the attack.
            - these tests are sometimes called "**partially known environment**" tests
        - ğŸ”´**black-box penetration test (Closed-box or Zero-knowledge testing)**:
            - does not provide attackers with any information prior to the attack
            - this simulates an external attacker trying to gain access to information about the business and technical environment before engaging in an attack
            - When the penetration testers are provided no information, it is referred to as a zero-knowledge test (black box). 
            - these tests are sometimes called "**unknown environment**" tests
            - With closed-box (formerly black) testing, the penetration tester isn't given any prior knowledge of the network and has very little or no information to work with. It is commonly referred to as the most realistic test, as it provides the same point of view as an outside attacker.
        - ğŸ”´**physical or internal penetration test**:
            - If the penetration testers are provided building and system access they could be performing a physical test or an internal test
- **Excersice Types**:
    - ğŸ“—**Red Team (Tiger Team)**: A group of security professionals who simulate real-world ğŸ“attacks to test the defenses of an organization.
        - They are an internal or external entity dedicated to testing the effectiveness of a security program by emulating tools and techniques of likely attackers in the most realistic way possible
        - They are offence
        - Think of attackers testing the system for weaknesses.
    - ğŸ“—**Blue Team**: A group of security professionals responsible for defending an organizationâ€™s IT infrastructure from attacks.
        - They are internal security team that ğŸ“defends against Red Team and real attackers
        - They are defence
        - Think of defenders protecting the system from attacks.
    - ğŸ“—**Purple Team**: A collaboration between the red team and blue team to improve the overall security of an organization and for knowledge sharing.
        - This could involve a team playing both roles, members switching between the teams during the exercise, or a retrospective where the red and blue teams share findings.
        - exist to ensure and maximise the effectiveness of the red and blue teams
        - They are for process improvement
        - Think of a ğŸ“cooperative effort where attackers and defenders work together to strengthen security.
   - ğŸ“—**White Team**: responsible for overseeing an engagement/competition between a Red Team of mock attackers and a Blue Team of actual defenders.
        - They do not participate directly in the attack (red team) or defense (blue team) activities but ensure fair play and adherence to the rules.
        - Facilitate communication and coordination between red and blue teams.
        - Observe the security exercise to ensure that it is realistic and that all parties follow the guidelines.
        - Set the boundaries and objectives of the security exercise to ensure it is controlled and focused.
        - Think of judge or ğŸ“referee
      
- 6.2.3 Log reviews
    -  Log management system designs must take into account
        -  â™ˆthe volume of log data
        -  â™ˆthe network bandwidth it consumes
        -  â™ˆthe security of the data
        -  â™ˆthe amount of effort required to analyze the data.
    - **Security Information and Event Management (SIEM)**: packages that collect information using the syslog functionality present in many devices, operating systems, and applications. NoteğŸ“ Windows systems generate logs in the Windows native logging format. To send syslog events, Windows systems require a helper application or tool.
        - SIEM products are designed to log and analyze networks, providing the user with a variety of ways to observe their network.
        - SIEMs also aggregate data from various sources, making event correlation and threat hunting very simple for security analysts.
        - SIEM being remote or a secondary log source on the network at least, there is a second copy of time-organized logs available in case an attacker deletes their firewall logs.
        - Admins may choose to deploy logging policies through Windows Group Policy Objects (GPOs)
        - Logging systems should also make use of the Network Time Protocol (NTP) to ensure that clocks are synchronized on systems sending log entries to the SIEM as well as the SIEM itself, ensuring info from multiple sources have a consistent timeline
        - Examples include SPlunk, LogRythm, Microsoft Azure Sentinel, IBM Qradar
        - Information security managers should also periodically conduct log reviews, particularly for sensitive functions, to ensure that privileged users are not abusing their privileges
    - **Network flow** (NetFlow) NetFlow records contain an entry for every network communication session that took place on a network and can be compared to a list of known malicious hosts. It is routinely saved as a matter of normal activity.
        - Cisco has developed a version of NetFlow called Flexible NetFlow (FNF), which can be combined with secure transmission methods. However, the specific built-in decryption capability for NetFlow data is generally referred to as **Encrypted Traffic Analytics (ETA)**. ETA is designed to provide visibility into network traffic, even when the traffic is encrypted. 
    - ğŸ“š**Active monitoring (Proactive/Synthetic monitoring)** uses emulated or recorded transactions to monitor for performance changes in response time, functionality, or other performance monitors. e.g Grafana, Zabbix, Selenium. It performs artificial transactions against a website to assess performance. Synthetic monitoring uses simulated or recorded traffic and thus can be used to ğŸ“proactively identify problems. Synthetic monitoring uses emulated or recorded transactions to monitor for performance changes in response time, functionality, or other performance monitors. Proactive monitoring, aka synthetic monitoring, uses recorded or generated traffic to test systems and software.
        - Synthetic monitoring completes transactions against a website to track website ğŸ“performance, such as refreshing a page to count response time or inputting various user inputs to tally outputs.
    - ğŸ“š**Passive monitoring** uses a span port or other method to copy traffic and monitor it in real time. e.g wireshark, PRTG, Cacti, ntoping, prometheus.  Passive monitoring uses a network tap or other capture technology to allow monitoring of actual traffic to a system or application. ğŸ“Systems that respond to ping will show the time to live for packets that reach them. Since TTL is decremented at each hop, this can help build a rough network topology map. In addition, some firewalls respond differently to ping than a normal system, which means pinging a network can sometimes reveal the presence of firewalls that would otherwise be invisible. 
        - **Real user monitoring (RUM)** is a variant of passive monitoring where the monitoring tool reassembles the activity of individual users to track their interaction with a website. RUM records user interaction with an application or system to ensure performance and proper application behavior. RUM is often used as part of a predeployment process using the actual user interface. It is a passive monitoring technique records all user interaction with an application or website to ensure quality and performance?
        - RUM analyzes the traffic or status of transactions for real user traffic. This is also known as passive monitoring. RUM provides real-time updates on the status of user interactions for a given service.
        - ğŸ“Passive monitoring works only after issues have occurred because it requires actual traffic.
    - **Identity and access management (IAM)** systems combine lifecycle management and monitoring tools to ensure that identity and authorization are properly handled throughout an organization. e.g AD, DUO, OKTA. Identity and access management (IAM) systems can be configured to use appropriate workflows and to generate the logs and reports to tracks all changes to accounts through their lifecycle. 

- 6.2.4 Synthetic transactions
    - **Synthetic transactions**: scripted transactions with ğŸ“known expected results.  synthetic transactions, which can use recorded or generated transactions with data that fits the expected requirements of the web application to verify that it responds to typical customer behavior.
        - Synthetic transactions ensure that whatever text is expected comes out when requested. This is excellent practice for testing code and validating input, especially against potential attacks. The main premise behind synthetic transactions is to put code in and determine what the output is.
        - ğŸ“Dynamic testing may include the use of synthetic transactions to verify system performance; synthetic transactions are run against code and compare out to expected state

- 6.2.5 Code review and testing
    - Code review and testing is "one of the most critical components of a software testing program"
    - Code review involves another developer reviewing code for potential issues. Often, this is accomplished by requiring a review before code is incorporated into a code repository. This can be performed earliest because it is performed before any code is accepted, and before the application is executable.
    - These procedures provide third-party reviews of the work performed by developers before moving code into a production environment, possibly discovering security, performance, bugs or reliability flaws in apps before they go live and negatively impact business operations
    - In code review, AKA peer review, developers other than the one who wrote the code review it for defects
        - **Fagan inspections**: the most formal code review process follows six steps:
            1) planning
            2) overview
            3) preparation
            4) inspection
            5) rework
            6) follow-up
    - ğŸ”–**Static application security testing (SAST)**: evaluates the security of software without running it by analyzing either the source code or the compiled application, but not a running application. Static reviews are typically performed by an automated tool.
    - ğŸ”–**Dynamic application security testing (DAST)**: evaluates the security of software in a runtime environment and is often the only option for organizations deploying applications written by someone else. Dynamic testing is also used to determine how code handles variables that change over time.
    - ğŸ”–**Interactive Application Security Testing (IAST)** involves analyzing a running application while also looking at the source code as it is being accessed. 

- 6.2.6 Misuse case testing
    - ğŸ”¨**Misuse case testing**: used by software testers to evaluate the vulnerability of their software to known risks
        - In misuse case testing, testers first enumerate the known misuse cases, then attempt to exploit those use cases with manual or automated attack techniques
        - Misuse testing is a type of testing that involves trying to accomplish more than the user should be able to do when using a running piece of software.
        - a list of possible ways that an attacker might exploit the application is identified, and testers work through each scenario, testing the application to see if it is vulnerable to that exploit
        - Misuse case testing is commonly used to describe abuse case testing, but its focus is on testing to ensure ğŸ“incorrect inputs or other types of misuse don't reveal any information about company servers or software.
    - ğŸ”¨**Abuse case testing**: Abuse case testing is a test to determine if a website, its hardware, software, and interactions with customers have security ğŸ“vulnerabilities that could be used by attackers..
    - ğŸ”¨**Use case testing**: use case testing is to verify that the application responds properly to actual use cases. It involves testing for desired functionality.

- 6.2.7 **Test coverage analysis**
    - A test coverage analysis is used to estimate the degree of testing conducted against new software
    - ğŸ§ Test coverage = number of use cases tested / total number of use cases
        - Â­Coverage rates are used to measure how effective code testing is.
        - requires enumerating possible use cases (which is a difficult task), and anyone using test coverage calcs to understand the process used to develop the input values
    - Five common criteria used for test coverage analysis:
        - ğŸ›ï¸**branch coverage**: has every IF statement been executed under all IF and ELSE conditions?
        - ğŸ›ï¸**condition coverage**: has every logical test in the code been executed under all sets of inputs?
        - ğŸ›ï¸**functional coverage**: has every function in the code been called and returned results?
        - ğŸ›ï¸**loop coverage**: has every loop in the code been executed under conditions that cause code execution multiple times, only once, and not at all?
        - ğŸ›ï¸**statement coverage**: has every line of code been executed during the test?
    
- âœ´ï¸**Test coverage report**: measures how many of the test cases have been completed and is used as a way to provide test metrics when using test cases.
    - Penetration test report is provided when a penetration test is conducted.
    - Code coverage report covers how much of the code has been tested
    - Line coverage report is a type of code coverage report.
    - Code coverage testing most frequently requires that every function has been called, that each statement has been executed, that all branches have been fully explored, and that each condition has been evaluated for all possibilities.
- 6.2.8 Interface testing
    - Interface testing assesses the performance of modules against the interface specs to ensure that they will work together properly when all the development efforts are complete
    - Three types of interfaces should be tested:
        - ğŸ“š**application programming interfaces (APIs)**: offer a standardized way for code modules to interact and may be exposed to the outside world through web services. API interface testing would identify flaws in a program's ability to interact with other programs via web services.
            - should test APIs to ensure they enforce all security requirements
            - API Security: API Security focuses on strategies and solutions to understand and mitigate the unique vulnerabilities and security risks of Application Programming Interfaces (APIs). OWASP API Security Top 10 2023 are:
                - API1:2023 - ğŸ›ï¸**Broken Object Level Authorization**: APIs tend to expose endpoints that handle object identifiers, creating a wide attack surface of Object Level Access Control issues. Object level authorization checks should be considered in every function that accesses a data source using an ID from the user.
                - API2:2023 - ğŸ›ï¸**Broken Authentication**: Authentication mechanisms are often implemented incorrectly, allowing attackers to compromise authentication tokens or to exploit implementation flaws to assume other userâ€™s identities temporarily or permanently. Compromising a systemâ€™s ability to identify the client/user, compromises API security overall.
                - API3:2023 - ğŸ›ï¸**Broken Object Property Level Authorization**: This category combines API3:2019 Excessive Data Exposure and API6:2019 - Mass Assignment, focusing on the root cause: the lack of or improper authorization validation at the object property level. This leads to information exposure or manipulation by unauthorized parties.
                - API4:2023 - ğŸ›ï¸**Unrestricted Resource Consumption**: Satisfying API requests requires resources such as network bandwidth, CPU, memory, and storage. Other resources such as emails/SMS/phone calls or biometrics validation are made available by service providers via API integrations, and paid for per request. Successful attacks can lead to Denial of Service or an increase of operational costs.
                - API5:2023 - ğŸ›ï¸**Broken Function Level Authorization**: Complex access control policies with different hierarchies, groups, and roles, and an unclear separation between administrative and regular functions, tend to lead to authorization flaws. By exploiting these issues, attackers can gain access to other usersâ€™ resources and/or administrative functions.
                - API6:2023 - ğŸ›ï¸**Unrestricted Access to Sensitive Business Flows**: APIs vulnerable to this risk expose a business flow - such as buying a ticket, or posting a comment - without compensating for how the functionality could harm the business if used excessively in an automated manner. This doesnâ€™t necessarily come from implementation bugs.
                - API7:2023 - ğŸ›ï¸**Server Side Request Forgery**: Server-Side Request Forgery (SSRF) flaws can occur when an API is fetching a remote resource without validating the user-supplied URI. This enables an attacker to coerce the application to send a crafted request to an unexpected destination, even when protected by a firewall or a VPN.
                - API8:2023 - ğŸ›ï¸**Security Misconfiguration**: APIs and the systems supporting them typically contain complex configurations, meant to make the APIs more customizable. Software and DevOps engineers can miss these configurations, or donâ€™t follow security best practices when it comes to configuration, opening the door for different types of attacks.
                - API9:2023 - ğŸ›ï¸**Improper Inventory Management**: APIs tend to expose more endpoints than traditional web applications, making proper and updated documentation highly important. A proper inventory of hosts and deployed API versions also are important to mitigate issues such as deprecated API versions and exposed debug endpoints.
                - API10:2023 - ğŸ›ï¸**Unsafe Consumption of APIs**: Developers tend to trust data received from third-party APIs more than user input, and so tend to adopt weaker security standards. In order to compromise APIs, attackers go after integrated third-party services instead of trying to compromise the target API directly.
        - ğŸ“š**user interfaces (UIs)**: examples include graphical user interfaces (ğŸ“GUIs) and ğŸ“command-line interfaces
            - UIs provide end users with the ability to interact with the software, and tests should include reviews of all UIs
        - ğŸ“š**physical interfaces**: exist in some apps that manipulate machinery, logic controllers, or other objects
            - software testers should pay careful attention to physical interfaces because of the potential consequences if they fail

- 6.2.9 Breach attack simulations
    - ğŸ“**Breach and attack simulation (BAS)**: platforms that seek to ğŸ“automate some aspects of penetration testing
        - The BAS platform is not actually waging attacks, but conducting automated testing of security controls to identify deficencies
        - BAS tools typically leverage SaaS platforms as well as software agents and virtual machines to perform simulated attacks, which they leverage to provide detailed reports about security issues and their relative risk levels.
        - BAS systems are designed to inject threat indicators onto systems and networks in an effort to trigger other security controls.
        - Breach and Attack Simulation, systems are systems that combine red team (attack) and blue team (defense) techniques together with automation to simulate advanced persistent threats and other advanced threat actors when run against your environment. This allows a variety of threats to be replicated and assessed in an environment without as much overhead as a fully staffed purple team would.
        - Designed to inject threat indicators onto systems and networks in an effort to trigger other security controls (e.g. place a suspicious file on a server)
        - detection and prevention controls should immediately detect and/or block this traffic as potentially malicious
        - Advantages:
            - ğŸˆOver time, a BAS would reduce costs compared to hiring a third-party vendor once a year or every quarter. Howver note that this is not effective becuase since tests\audits are internal, there will still be conflict of interest, hence not compliance effective. 
            - ğŸˆAdditionally, a BAS would be more convenient and more "on-demand", while allowing for faster and more consistent testing.
            - ğŸˆAn automated breach attack simulation would not be as disruptive to a business as a traditional penetration test would be.
            - ğŸˆAutomated testing would allow for consistent testing, which would detect new vulnerabilities and real threats faster than a third party conducting a quarterly audit.
    - See:
        - OWASP Web Security Testing Guide 
            - ğŸ–±ï¸Phase 1 Before Development Begins: Define a SDLC, Review Policies and Standards, Develop Measurement and Metrics Criteria and Ensure Traceability
            - ğŸ–±ï¸Phase 2 During Definition and Design: Review Security Requirements, Review Design and Architecture, Create and Review UML Models, Create and Review Threat Models
            - ğŸ–±ï¸Phase 3 During Development: Code Walkthrough, Code Reviews
            - ğŸ–±ï¸Phase 4 During Deployment: Application Penetration Testing, Configuration Management Testing
            - ğŸ–±ï¸Phase 5 During Maintenance and Operations: Conduct Operational Management Reviews, Conduct Periodic Health Checks, Ensure Change Verification
        - OSSTMM (Open Source Security Testing Methodology Manual)
        - NIST 800-115 (ğŸ¤Planning, ğŸ¤Discovery, ğŸ¤Attack, ğŸ¤Reporting)
        - FedRAMP Penetration Test Guidance
        - PCI DSS Information Supplemental on Penetration Testing NoteğŸ“ PCI DSS requires a rescan of  application at least annually and after any change in the application.
        - ğŸ“Infection Monkey is an open-source Breach Attack Simulation (BAS) program, which can be found on Github or the Akamai website.

- 6.2.10 ğŸ”´Compliance checks
    - Orgs should create and maintain compliance plans documenting each of their regulatory obligations and map those to the specific security controls designed to satisfy each objective
    - Compliance checks are an important part of security testing and assessment programs for regulated firms: these checks verify that all of the controls listed in a compliance plan are functioning properly and are effectively meeting regulatory requirements

[6.3](#6.3) Collect security process data (e.g. technical and administrative) (OSG-9 Chpts 15,18)
- 6.3.1 ğŸ”´Account management
    - Preferred attacker techniques for obtaining privilege user access include:    
        - compromising an existing privileged account: mitigated through use of strong authentication (strong passwords and multifactor), and by admins use of privileged accounts only for specific tasks
        - privelege escalation of a regular account or creation of a new account: these approaches can be mitigated by paying attention to the creation, modification, and use of user accounts
        - The most frequent target of account management reviews are highly ğŸ“privileged accounts, as they create the greatest risk.

- 6.3.2 ğŸ”´Management review and approval
    - Account management reviews ensure that users only retain authorized permissions and that unauthorized modifications do not occur
    - The management review and approval process ensures that each account was appropriately authorized and remains necessary to meet business needs.
    - Full review of accounts: time-consuming to review all, and often done only for highly privileged accounts. 
    - Organizations that donâ€™t have time to conduct a full review process may use sampling, but only if sampling is truely random
    - The two main methods of choosing records from a large pool for further analysis are sampling and clipping.
        - âš›ï¸**Sampling** uses statistical techniques to choose a sample that is representative of the entire pool. Sampling should be done randomly to avoid human bias. Sampling is an effective process if it is done on a truly random sample of sufficient size to provide effective coverage of the userbase. It is infeasible for a single person to review every single record. 
        - âš›ï¸**Clipping** uses threshold values to select those records that exceed a predefined threshold because they may be of most interest to analysts.
    - Adding accounts: should be a well-defined process, and users should sign AUP
    - Adding, removing, and modifying accounts and permissions should be carefully controlled and documented
    - Accounts that are no longer needed should be suspended
    - ğŸ“ISO 9000 standards use a Plan-Do-Check-Act loop
        - ğŸ¤plan: foundation of everything in the ISMS, determines goals and drives policies
        - ğŸ¤do: security operations
        - ğŸ¤check: security assessment and testing (this objective)
        - ğŸ¤act: formally do the management review

- 6.3.3 ğŸ”´Key performance and risk indicators
    - ğŸ®**Key Performance Indicator (KPIs)**: measures that provide significance of showing the performance an Information Security Management System ISMS compared to stated goals. KPIs are used to determine how effective practices, procedures, and staff are. Key Performance Indicators (KPIs) are a class of metrics used to measure activity success. KPI measurements enable performance to be tracked, trended, monitored, and prioritized or targeted for improvement. Examples of KPIs include, "percent of system uptime" and "mean time to detect."
        - Choose the factors that can show the state of security
        - Define baselines for some (or better yet all) of the factors
        - Develop a plan for periodically capturing factor values (use automation!)
        - Analyze and interpret the data and report the results
        - Time to remediate a vulnerability is a commonly used key performance indicator for security teams and patching related KPIs.
        - Rate of defect recurrence  is an appropriate measure for regression testing.
    - Key metrics or KPIs that should be monitored by security managers may vary from org to org, but could include:
        - ğŸ‘½number of open vulnerability
        - ğŸ‘½time to resolve vulnerability
        - ğŸ‘½vulnerability/defect recurrence
        - ğŸ‘½number of compromised accounts
        - ğŸ‘½number of software flaws detected in pre-production scanning
        - ğŸ‘½repeat audit findings
        - ğŸ‘½number of account compromise
        - ğŸ‘½number of times user attempts to visit known malicious sites
        - ğŸ‘½Number of times a malicious IP address is blocked before and after a new security rule is implemented
    - Develop a dashboard of metrics and track them
    - There are many key performance indicators(KPIs) that are relevant to ğŸ“physical security. Some are:
        - ğŸ©¹number of successful intrusions
        - ğŸ©¹number of successful disruptions
        - ğŸ©¹number of unsuccessful incidents
        - ğŸ©¹time to detect incidents
        - ğŸ©¹time to respond to incidents
        - ğŸ©¹level of organizational impact of incidents
        - ğŸ©¹number of false positives (i.e., false detection alerts/alarms).
        - ğŸ©¹increase in closed service tickets allows a company to track the performance of the service department.
        -  ğŸ©¹decrease in amount of downtime is a terrific KPI to track to show the capability and performance of the Information Technology (IT) department.
        -  ğŸ©¹decreased number of tickets should indicate a decreased number of problems with a system
   - ğŸ®**Key Risk Indicator (KRIs)**: Key risk indicators are used to tell those in charge of risk management how risky an activity is and how much impact changes are having on that risk profile. Identifying key risk indicators and monitoring them can help to identify high-risk areas earlier in their lifecycle.  Key Risk Indicators (KRIs) allow for a more proactive approach to cybersecurity.
   - Key Risk Indicators (KRIs) are a class of metrics used to measure exposure to potential harm. Examples of KRIs include,
       - Hardware and Software entering entering EOS as this naturally increases risk over time since there are no future patches. 
       - Increase global phishing campaigns. With phishing campaigns on the rise and many businesses using e-mail, it is safe to say this would be a major security risk and an indication that the use of Sender Policy Framework (SPF) records and Domain Keys Identify Mail (DKIM) should be used as a mitigation strategy.
       - Supply chain issues are a risk to a business, as they can naturally increase costs or jeopardize the reputation of a company that isn't conducting quality control. Understanding a supply chain and potential issues can ensure an understanding of current supply and demand, and the likelihood of untrusted or counterfeit products. Ways to mitigate this would be to reduce the company's scope of purchase from less-trusted third parties and to implement quality control upon delivery of items.
       - Low storage volumes could be an indicator of a future denial of service, for example. Imagine attempting to update a game before playing, but you cannot update the game because you are out of storage space. This would be an example of a denial of service.
       - An increase in blocked firewall connections could be an indication of attempted cyberattacks, most notably a denial of service or distributed denial of service. Imagine an attacker sending traffic during a distributed denial of service, but you notice this and have the bandwidth to support it. An attacker may see that you are still online and send even more data as a result, but these blocked connections would be an indicator of this potentially happening.
       - High CPU usage on a router could be an indicator that you should upgrade before the router becomes overworked and enters a failed state, eliminating your internet connection.
       - An increase in Intrusion Detection System (IDS) alerts is a Key Risk Indicator (KRI).
- 6.3.4 ğŸ”´Backup verification data
    - Managers should periodically inspect the results of backups to verify that the process functions effectively and meets the organizationâ€™s data protection needs
        - this might include reviewing logs, inspecting hash values, or requesting an actual restore of a system or file
        -  ensures that the organizationâ€™s ğŸ“data protection requirements are met effectively
        -  The backup verification process ensures that backups are running properly and thus meeting the organization's data protection objectives.
            - For verifying backup integrity, automated integrity checking can determine if files have been changed. If the files have been altered, automated integrity checks can occur with the backup drives upon integrating them into production. There are local programs that can also do this. Sigverif is a Windows program that can check the integrity of files and be useful in the event that backups are not automated, to check integrity.
        -  The recovery point objective (RPO) specifies the maximum amount of data that may be lost during a disaster and should be used to guide backup strategies. 

- 6.3.5 ğŸ”´Training and awareness
    - Training and awareness programs play a crucial role in preparing an organizationâ€™s workforce to support information security programs
    - They educate employees about current threats and advise them on best practices for protecting information and systems under their care from attacks
        - Employees are the ğŸ“largest security threat to a company, and their training would reduce the largest number of security threats.
        - Employees are ğŸ“prime targets for a variety of attacks involving social engineering and hoaxes.
        - Employees make ğŸ“mistakes that can have serious consequences, which BAS tests, vulnerability scanning, and ISACs won't identify and fix.
        - Empoyees may lack of knowledge, and specifically, cyberattacks such as phishing attempts could be thwarted with regular education.
    - Program should begin with initial training designed to provide foundation knowledge to employees who are joining the org or moving to a new role; the initial training should be tailored to an individualâ€™s role
    - Training and awareness should continue to take place throughout the year, reminding employees of their responsibilities and updating them on changes to the organizationâ€™s operating environment and threat landscape
    - Training activities should be ğŸ“tracked to ensure that all employees attend and, even better, are learning.
    - The number of staff who took a given training and the average change in their awareness from ğŸ“before the training to ğŸ“after the training can provide insight into how many trained staff you have and how impactful the training was.
    - Training employees ğŸ“frequently will ensure better security for the organization as a whole. Generally speaking, employees are the greatest threat to an organization. 
    - Use phishing simulations to evaluate the effectiveness of their security awareness programs
    - ğŸ“Phishing simulations test the effectiveness of security training by actually identifying the percentage of users likely to fall victim to a phishing attack.
    - ğŸ“Surveys, which is the most common, would also measure this but with lesser effectiveness because they depend upon self-reported perceptions

- 6.3.6 ğŸ”´Disaster Recover (DR) and Business Continuity (BC): ğŸ©¹BCP: Keeps the business running during a disaster (includes all functions). ğŸ©¹DR: Focuses on restoring IT systems after a disaster.
    - ğŸ“**Business Continuity (BC)**: the processes used by an organization to ensure, holistically, that its vital business processes remain ğŸ“unaffected or can be quickly restored following a serious incident. Preventing business interruption is the goal of business continuity
        - The main purpose of BC is to ensure that critical business functions continue operating during and after a disruption, no matter how big or small.
        - Itâ€™s a ğŸ“broad plan that covers all aspects of the business. This includes keeping people, processes, and resources running to prevent complete shutdown.
        - For example, if thereâ€™s a major power outage, BC will outline steps to ensure the company can still functionâ€”by switching to generators, relocating staff, or working remotely.
        - Ensures that operations ğŸ“continue with minimal interruption. The goal is to keep the entire business running, not just IT systems.
        - Business Continuity Planning (BCP) Response Example where A large retail company, like Walmart or Amazon, experiences a severe snowstorm that shuts down roads and makes it impossible for employees to reach their warehouse.
            - ğŸªShifting operations to a nearby warehouse that is unaffected by the storm.
            - ğŸªHaving employees work from home to coordinate deliveries and manage online orders.
            - ğŸªUsing backup suppliers or transportation routes to ensure products are still delivered to customers.
            - ğŸªThe company reroutes deliveries from alternative suppliers to maintain product availability.
            - ğŸªNon-critical employees are given time off until the situation improves.
            - ğŸªOnline shopping systems, which run on backup generators or alternative power sources, remain operational so customers can still place orders.
            - ğŸ‰‘Outcome: Even though thereâ€™s a snowstorm, the company continues to fulfill customer orders and keep the business running by adapting its operations.
            - **Business Continuity Planning (BCP)** is defined by NIST as, â€œThe documentation of a predetermined set of instructions or procedures that describe how an organizationâ€™s mission/business processes will be sustained during and after a significant disruption.â€
            - Between DRP and BCP, the most common way to look at the two is that BCP is a bigger, ğŸ“broader document that is for all critical missions and processes.
    - ğŸ“**Disaster Recovery (DR)**: is a ğŸ“subset of BC, that focuses on restoring information systems after a disaster. the goal of DRP is to restore regular business activity as quickly as possible.
    - These processes need to be periodically accessed, and regular testing of disaster recovery and business continuity controls provide organizations with the assurance they are effectively protected against disruptions to business ops
    - The main objective of DR is to restore IT systems and data after a disaster (e.g., cyberattacks, natural disasters).
    - Itâ€™s more IT-focused and typically forms a part of the overall BCP. DR involves specific steps to recover technology systems, such as servers, networks, and databases.
    - For example, after a ransomware attack, DR would cover restoring backup data, rebuilding systems, and ensuring IT services are back online. When planning a disaster recovery solution, the amount of data loss should be lower than the Recovery Point Objective (RPO). RPO is the threshold (expressed as an interval of time) established by the organization to indicate the maximum amount of data loss it can tolerate without adverse impact. An organization that is unable to tolerate more than a day's worth of data loss would have an RPO of 24 hours.
    - Focuses on recovering IT infrastructure and data to bring systems back to normal after theyâ€™ve been disrupted.
    - Disaster Recovery (DR) Example where a fire in the main warehouse destroys critical IT equipment and disrupts the companyâ€™s ability to process online orders
        - ğŸªThe IT department switches operations to a backup data center, where all customer and inventory data are replicated.
        - ğŸªBackup servers are used to restore the order processing system, allowing customers to continue shopping online.
        - ğŸªThe company restores its internal systems and communication tools to enable employees to coordinate shipments from unaffected locations.
        - ğŸªData is restored from secure off-site backups to bring the banking systems back online.
        - ğŸªCustomer data and transaction history are restored from secure backups, ensuring that no information is lost.
        - ğŸªOnline systems are restored to normal operation, and branches are back online to process orders within hours
        - ğŸªThe United States National Institute for Standards and Technology (NIST) defines a DRP as â€œA written plan for processing critical applications in the event of a major hardware or software failure or destruction of facilities.â€ Since the main warehouse is on fire, the DRP is the best match.
        - ğŸ‰‘Outcome: After a short downtime, IT systems are restored, and the company resumes online order processing. The focus here is on recovering the IT systems after the fire.
    - ğŸ“Protection of life is of the utmost importance and should be dealt with first before attempting to save material things
        - Crisis management is the process of planning and responding to unexpected events that can have a negative impact on an organization. One of the key traits of crisis management is the ability to anticipate potential problems and develop plans to mitigate their impact. This means being able to think ahead and identify potential risks, as well as having a plan in place to deal with them if they do occur.
        - Crisis management involves handling the overall strategy and high-level decisions during a crisis or disaster. It focuses on managing the organization's response at a strategic level, ensuring that the organization can navigate through the crisis effectively and make decisions that align with its long-term goals and survival.
        - **Disaster Recovery Planning (DRP)** involves being prepared for a disaster and recovering from it. Failing to keep track of a disaster recovery site could mean it's not reliable to use when a disaster hits and, due to the lack of effort, plenty of money will be wasted. DRP is defined by the United States (US) National Institute of Standards and Technology (NIST) as, â€œA written plan for recovering one or more information systems at an alternate facility in response to a major hardware or software failure or destruction of facilities.â€
        - Between DRP and BCP, the most common way to look at the two is that DRP involves the ğŸ“data center.

[6.4](#6.4) Analyze test output and generate report (OSG-9 Chpt 15)
- ğŸ’¥Step 1: review and understand the data
    - The goal of the analysis process is to proceed logically from facts to actionable info
    - A list of vulns and policy exceptions is of little value to business leaders unless it's used in context, so once all results have been analyzed, you're ready to start writing the official report
- ğŸ’¥Step 2: determine the business impact of those facts
    - Ask "so what?"
    -  validation is necessary to verify that the issue exists. 
- ğŸ’¥Step 3: determine what is actionable
    - The analysis process leads to valuable results only if they are actionable
- 6.4.1 Remediation
    - Rather than software defects, most vulnerabilities in average orgs come from misconfigured systems, inadequate policies, unsound business processes, or unaware staff
    - Vuln remediation should include all stakeholders, not just IT 

- 6.4.2 Exception handling
    - **Exception handling**: the process of handling unexpected activity, since software should never depend on users behaving properly
        -  Errors should be logged or noted in a way that the administrator can handle, but end users (and attackers!) should not see that information.
        - "expect the unexpected", gracefully handle invalid input and improperly sequenced activity etc 
    - Sometimes vulns can't be patched in a timely manner (e.g. medical devices needing re-accreditation) and the solution is to implement compensitory controls, document the excpetion and decision, and revisit
        - **compensitory controls**:measures taken to address any weaknesses of existing controls or to compensate for the inability to meet specific security requirements due to various different constraints
        - e.g. micro-segmentation of device, access restrictions, monitoring etc
    - Exception handling may be required due to system crash as the result of patching (requiring roll-back)

- 6.4.3 Ethical disclosure
    - While conducting security testing, cybersecurity pros may discover previously undiscovered vulns (perhaps implementing compensating controls to correct) that they may be unable to correct
    - **Ethical disclosure**: the idea that security pros who detect a vuln have a responsibility to report it to the vendor, providing them with enough time to patch or remediate
        - the disclosure should be made privately to the vendor providing reasonable amount of time to correct
        - if the vuln is not corrected, then public disclosure of the vuln is warrented, such that other professionals can make informed decisions about future use of the product(s)
        - Unlike bug bounties, this does not rely on organizations providing financial or other rewards.
    - **Bug bounties** are programs offered by organizations to incentivize security researchers, ethical hackers, and sometimes the general public to find and report security vulnerabilities in their software, websites, or systems. These programs are a proactive approach to identifying and fixing potential security issues before they can be exploited by malicious actors.
[6.5](#6.5) Conduct or facilitate security audits (OSG-9 Chpt 15)
- 6.5.1 ğŸ”´Internal
    - Having an internal team conduct security audits has several advantages:
        - understanding of the internal environment reduces time
        - an internal team can delve into all parts of systems, because they have insider knowledge
        - internal auditors can be more agile in adapting to changing needs, rescheduling failed assessment components quickly
    - Disadvantages of using an internal team to conduct security audits:
        - the team may have limited exposure to new/other methodologies (e.g. the team may have depth but not breadth of experience and knowledge)
        - potential conflicts of interest (e.g. reluctance to throw other teams under the bus and accurately report their findings)
        - audit team members may start with an agenda (say to secure funding) and overstate faults, or have interpersonal motives
- 6.5.2 ğŸ”´External
    - An external audit (sometimes called a second-party audit) is one conducted by (or on behalf of) a business partner
    - External audits are tied to contracts; by definition, an external audit should be scoped to include only the contractual obligations of an organization
- 6.5.3 ğŸ”´Third-party
    - Third-party audits are often needed to demonstrate compliance with some government regulation or industry standard
    - Advantages of having a third-party audit an organization:
        - they likely have breadth of experience auditing many types of systems, across many types of organizations
        - they are not affected by internal dynamics or org politics
    - Disadvantage of using a third-party auditor:
        - cost: third-party auditors are going to be much more costly than internal teams; this means that the organization is likely to conduct audits as frequently
        - internal resources are still required to assist or accompany auditors, to answer questions and guide
- 6.5.4 Location: Facilitating Security Audit
    - On-Premise:
    - ğŸ“**Cloud**: distributed nature and scale of cloud infrastructure makes conducting an audit of cloud infrasturcture challenging.
        - You generally cannot audit the underlying infrastructure managed by the CSP
        - cloud computing enables distributed services , with systems that can replicate globally. The impact is the additional geographic locations auditors must consider when performing an audit
        - Common technique in cloud auditing is sampling - picking a subset of the physical infrastructure to inspect
        - CSPs can collect evidence that provides auditors with sufficient assurance that they have collected a representative sample to save time and expense while still mainting accuracy
        - In relation to audit compliance, the 3 audit standards commonly used by the big CSPs include SSAE, ISAE, and CSA.
            - ğŸ“—ï¸**SSAE (Statement on Standards for Attestation Engagements)**: SSAE is a standard developed by the American Institute of Certified Public Accountants ğŸ“(AICPA) for reporting on the controls at a service organization.
                - SSAE 18 is designed to enhance the quality and usefulness of SOC reports
                - SSAE is the framework for creating System and Organization Controls (SOC) reports, such as SOC 1, SOC 2, and SOC 3.
                  - ğŸ› ï¸SOC 1: report that focuses on financial reporting by certified public accountants.
                  - ğŸ› ï¸SOC 2 Type I: report that focuses on security, availability, processing integrity, confidentiality, and privacy controls at a specific point in time. Also Acts as baseline for future audits.
                  - ğŸ› ï¸SOC 2 Type II: report that asseses the security, availability, processing integrity, confidentiality, and privacy controls over time by overving operations for at least 6 months. You can get these reports on demand from CSPs but may require you to sign an NDA. SOC 2 reports are confidential.
                  - ğŸ› ï¸SOC 3: Similar to SOC 2 but intended for broader distribution and contains auditors general opinions.
                - It includes audit standards and suggested report formats to guide and assits auditors
                - Example a cloud service provider undergoes an SSAE 18 audit to prove to clients that their data is secure and managed properly. SOC 3 report, is intended for distribution to third parties. They include the auditor's opinions and management assertions, along with information about the service organization. SOC3 reports are specifically intended for external release.
            - ğŸ“—**ISAE (International Standard on Assurance Engagements)**:  ISAE is a set of standards issued by the International Auditing and Assurance Standards Board (IAASB) for assurance engagements, including reports on controls at service organizations. 
                - The board and its standards are both similar to those of SSAE.
                - ISAE 3402: The most relevant standard for cloud services, similar to SSAE 18, used ğŸ“internationally. it is roughly the equivalent of SOC 2 reports in SSAE.
            - ğŸ“—**CSA (Cloud Security Alliance)**: CSA is an organization that defines best practices and provides certifications for securing cloud computing environments.
                - it offers the CSA STAR program: The Security, Trust, Assurance, and Risk (STAR) program is a certification for cloud service providers based on CSA's Cloud Controls Matrix (CCM).
                - STAR consists of two level of certification which provide increasing levels of assurance
                   - ğŸ› ï¸Level 1 Self-Assesment: is based on self-assessment where Providers assess their own security practices against the CCM. It documents the security offering provided by the CSP.
                   - ğŸ› ï¸Level 2 Third Party Audit: Requires the CSP to engage an independent auditor to evaluate the CSPs controls against the CSA standard. It is stronger.
                - Can be used by CSPs, cloud consumers, auditors or consultants
                - It is designed to demonstrate compliance to a desired level of assurance.
    - Hybrid:
    - As Security Organizational Controls (SOC) engagement reports increase in level, their public disclosure is more likely. Additionally, SOC 1 and SOC 2 audit reports function similarly. The greater the involvement of people, the higher the level. SOC 1 reports are based on documentation review. SOC 2 reports are based on a more thorough review of security practices and physical evaluation. All three SOC engagement reports stress the Confidentiality, Integrity, Availability (CIA) triad. As we get closer to SOC 3, each aspect of the CIA triad also changes. The levels usually begin at level one with low availability and confidentiality and end at level three with availability to the public and no confidentiality.
      
- [6.6](#6.6) Software testing: Security Assessment and Testing (OSG-9 Chpt 15)
    - â„ï¸**Code Review**: Code review is the foundation of software assessment programs. During a code review, also known as a peer review, developers other than the one who wrote the code review it for defects. Static program reviews are typically performed by an automated tool. Program understanding, program comprehension, pair programming, software inspections, and software walk-throughs are all human-centric methods for reviewing code.
        - ğŸAdvantages:
            - Completeness and effectiveness
            - Accuracy
            - Fast (for competent reviewers)
        - ğŸDisadvantages
            - Requires highly skilled security aware developers
            - Can miss issues in compiled libraries
            - Cannot detect runtime errors easily
            - The source code actually deployed might differ from the one being analyzed 
        - an example of Code review is by using the 6 steps ğŸ”¥Fagan process (Fagan inspection is a highly formalized review and testing process): ğŸŸ¡ POPIRU
            - 1.Planning
            - 2.Overview
            - 3.Preparation
            - 4.Inspection
            - 5.Rework
            - 6.Follow-Â­up
    - â„ï¸**Static Testing** SAST: evaluates the security of software without running it by analyzing either the source code or the compiled application
        - Developers use static code analysis tools network
        - Static program reviews are typically performed by an automated tool
    - â„ï¸**Dynamic Testing** DAST: evaluates the security of software in a runtime environment and is often the only option for organizations deploying applications written by someone else. In those cases, testers often do not have access to the underlying source code.
        - common example of dynamic software testing is the use of web application scanning tools to detect the presence of cross-Â­site scripting, SQL injection, or other flaws in web applications. NoteğŸ“ SQL injection is a web vulnerability.
        - Another example is the use of synthetic transactions to verify system performance. These are scripted transactions with known expected results.
    - â„ï¸**Fuzz Testng**: is a specialized dynamic testing technique that provides many different, and sometimes invalid types of input to software to stress its limits and find previously undetected flaws.  Fuzz testing is a technique used to find flaws or vulnerabilities by sending randomly generated or specially crafted inputs into the software. Fuzzing uses modified inputs to test software performance under unexpected circumstances.
        - It is imited to detecting ğŸ“simple vulnerabilities.
        - zzuf is a fuzzing tool thats tests web browsers ability to handle unexpected data. zzuf is specifically designed to work with tools like web browsers, image viewers, and similar software by modifying network and file input to application.
        - Fuzzers are tools that are designed to provide invalid or unexpected input to applications, testing for vulnerabilities like format string vulnerabilities, buffer overflow issues, and other problems.
    - Fuzzing involves sending unexpected inputs to a program to see how it responds
        - ğŸ””Mutation (Dumb) Fuzzing Takes previous input values from actual operation of the software and manipulates (or mutates) it to create fuzzed input. It uses bit flipping and other techniques to slightly modify previous inputs to a program in an attempt to detect software flaws. It  modifies known inputs to generate synthetic inputs that may trigger unexpected behavior
        - ğŸ””Generational (Intelligent) Fuzzing Develops data models and creates new fuzzed input based on an understanding of the types of data used by the program. It develops inputs based on models of expected inputs to perform the same task.
    - â„ï¸**Interface Testing**: assesses the performance of modules against the interface specifications to ensure that they will work together properly.  Interface testing verifies that the different components of a system will work properly when linked together. Interfaces to be tested include
      - ğŸˆApplication Programming Interfaces (APIs)
      - ğŸˆUser Interfaces (UIs)
      - ğŸˆ Network Interface
      - ğŸˆPhysical Interfaces (For applications that manipulate machinery and logic controllers)
    - â„ï¸**Misuse Case Testing**: or abuse case testing to evaluate the vulnerability of their software to users attempt to misuse the application and tests its responses to these malicious inputs.
    - â„ï¸**Mutation Testing**: Mutation testing (automatically) modifies a program in small ways and then tests that mutant to determine if it behaves as it should or if it fails. This technique is used to design and test software tests through mutation. It is a method used to automatically design new software tests and to ensure the quality of tests
    - â„ï¸**Regression Testing**: In cases where the project is releasing updates to an existing system, regression testing formalizes the process of verifying that the new code performs in the same manner as the old code, other than any changes expected as part of the new release. They Key performance measure of Regression testing is more specifically covered by defect recurrence rates.        

- ğŸ”´**Time of Check to Time of Use (TOCTTOU or TOC/TOU)**: Computer systems perform tasks with rigid precision. Computers excel at repeatable tasks. Attackers can develop attacks based on the predictability of task execution. The common sequence of events for an algorithm is to check that a resource is available and then access it if you are permitted.
    - âš”ï¸**Time of Check (TOC)** is the time at which the subject checks on the status of the object. There may be several decisions to make before returning to the object to access it.
    - âš”ï¸**Time of Use (TOU)**: When the decision is made to access the object, the procedure accesses it at the "time of use (TOU). The difference between the TOC and the TOU is sometimes large enough for an attacker to replace the original object with another object that suits their own needs.
    - âš”ï¸**Race Conditions (TOCTTOU or TOC/TOU)**: Time of check to time of use (TOCTTOU or TOC/TOU) attacks are often called race conditions. ğŸ“When testing in a non-production environment, the changes from a testing environment with instrumentation inserted into the code and the production environment for the code can mask timing-related issues like race conditions.

- ğŸ”´**Common Audit Frameworks**
    - ğŸ‘½SSAE 18 is the Statement on Standards for Attestation Engagements that is used by auditors when performing audits for SOC 2.
    - ğŸ‘½ISO/IEC 15408-Â­1:2009, â€œInformation technology â€”Â­Security techniques â€”Â­ Evaluation criteria for IT security,â€ is the foundation for the Common Criteria certification, which is a formal assessment process for technology products against a defined set of security functional requirements. This document and ISO/IEC 18045 are available free of charge.
    - ğŸ‘½ISO/IEC 18045:2008, â€œInformation technology â€”Â­Security techniques â€”Â­Methodology for IT security evaluation,â€ is a companion to ISO 15048 and provides ğŸ“standards for consistent criteria and evaluation methods.
    - ğŸ‘½ISO/IEC 27006:2015, â€œInformation technology â€”Â­Security techniques â€”Â­ Requirements for bodies providing audit and certification of information security management systems,â€ is the official set of requirements and guidance for auditors performing certification ğŸ“audits against ISO 27001.
    - ğŸ‘½NIST Special Publication (SP) 800-Â­53A, â€œAssessing Security and Privacy Controls in Federal Information Systems and Organizations,â€ is a guide to assessing the ğŸ“controls outlined in NIST SP 800-Â­53. It introduces a simple set of testing procedures to assess control effectiveness: test, examine, or interview and covers methods for assessing and measuring controls.
    - ğŸ‘½The NIST Cybersecurity Framework (CSF) and FedRAMP Security Assessment Framework (SAF) are both freely available as well and may be used to perform assessments with evaluation methods similar to those in NIST SP 800-Â­53A.
    - ğŸ‘½COBIT the Control Objectives for Information and Related Technologies, is commonly used as an audit framework for organizations. The COBIT, or Control Objectives for Information and related Technologies, framework describes common requirements that organizations should have in place for their information systems. 

- ğŸ”´**Common Ports**
    - ğŸ¨Line Printer Daemon (LPD) protocol: Port 515
    - ğŸ¨RAW or JetDirect protocol: Port 9100
    - ğŸ¨FTP (File Transfer Protocol): Port 20 (Data Transfer), Port 21 (Control)
    - ğŸ¨FTPS (FTP Secure): Ports 989 and 990
    - ğŸ¨SFTP (SSH File Transfer Protocol): Port 22
    - ğŸ¨SSH (Secure Shell): Port 22
    - ğŸ¨Telnet: Port 23
    - ğŸ¨SMTP (Simple Mail Transfer Protocol):Port 25
    - ğŸ¨IMAP (Internet Message Access Protocol): Port 143
    - ğŸ¨IMAPS (IMAP Secure): Port 993
    - ğŸ¨POP3 (Post Office Protocol version 3): Port 110
    - ğŸ¨POP3S (POP3 Secure): Port 995
    - ğŸ¨TFTP (Trivial File Transfer Protocol): Port 69
    - ğŸ¨SNMP (Simple Network Management Protocol): Port 161 (Agent), Port 162 (Manager)
    - ğŸ¨LDAP (Lightweight Directory Access Protocol): Port 389
    - ğŸ¨LDAPS (LDAP Secure): Port 636
    - ğŸ¨RDP (Remote Desktop Protocol): Port 3389
    - ğŸ¨SMB (Server Message Block): Port 445
    - ğŸ¨Syslog: Port 514
    - ğŸ¨Kerberos: Port 88
    - ğŸ¨SQL Server: Port 1433-1434
    - ğŸ¨MySQL: Port 3306
    - ğŸ¨Mongodb: TCP ports 27017-27019
    - ğŸ¨Elasticsearch: TCP port 9200
    - ğŸ¨Oracle DB: Port 1521
    - ğŸ¨NTP (Network Time Protocol): Port 123
    - ğŸ¨BGP (Border Gateway Protocol): Port 179
    - ğŸ¨IKE (Internet Key Exchange): Port 500
    - ğŸ¨L2TP (Layer 2 Tunneling Protocol): Port 1701
    - ğŸ¨PPTP (Point-to-Point Tunneling Protocol): Port 1723
    - ğŸ¨RADIUS (Remote Authentication Dial-In User Service): Port 1812 (Authentication), Port 1813 (Accounting)
    - ğŸ¨RADIUS over TLS (RadSec): Port 2083
    - ğŸ¨Diameter: TCP or SCTP (Stream Control Transmission Protocol) on port 3868.
    - ğŸ¨NetBIOS: Ports 137-139
    - ğŸ¨TACAS: Port 49
    - ğŸ¨XMPP (Extensible Messaging and Presence Protocol): Port 5222
    - ğŸ¨VNC (Virtual Network Computing): Port 5900
    - ğŸ¨NFS (Network File System): Port 2049
    - ğŸ¨OpenVPN: Port 1194
    - ğŸ¨PostgreSQL: Port 5432
    - ğŸ¨Oracle: 1521
    - ğŸ¨H.323: 1720

- ğŸ”´**NIST Special Publication (SP) 800 Series:**
    - âš’ï¸NIST 800-12 provides foundational guidance on ğŸ“computer security. It serves as an introductory framework for managing and securing information systems.
    - âš’ï¸NIST SP 800-30: "Guide for Conducting Risk Assessments" - Provides guidance on conducting ğŸ“risk assessments.
    - âš’ï¸NIST 800-34 focuses on contingency planning, providing guidelines for preparing for, responding to, and recovering from disruptions to information systems. ğŸ“DR and BCP
    - âš’ï¸NIST SP 800-37: "Risk Management Framework for Information Systems and Organizations" - Outlines the Risk Management Framework ğŸ“(RMF) for federal information systems.
    - âš’ï¸NIST SP 800-39: "Managing Information Security Risk" - Provides guidelines for an integrated, organization-wide program for managing ğŸ“information security risk.
    - âš’ï¸NIST SP 800-53: "Security and Privacy Controls for Information Systems and Organizations" - Provides a catalog of security and privacy ğŸ“controls for federal information systems and organizations.
    - âš’ï¸NIST SP 800-61: "Computer Security Incident Handling Guide" - Offers guidelines for handling computer security ğŸ“incidents.
    - âš’ï¸NIST SP 800-82: "Guide to Industrial Control Systems (ICS) Security" - Provides guidance on securing industrial control systems ğŸ“ICS.
    - âš’ï¸NIST SP 800-86: "Guide to Integrating Forensic Techniques into Incident Response" - Discusses how to integrate ğŸ“forensic techniques into incident response.
    - âš’ï¸NIST SP 800-94 Guide to Intrusion Detection and Prevention Systems provides comprehensive coverage of both ğŸ“IDS and ğŸ“IPS
    - âš’ï¸NIST SP 800-137: titled "Information Security Continuous Monitoring (ISCM) for Federal Information Systems and Organizations," provides guidelines for implementing continuous ğŸ“monitoring strategies
    - âš’ï¸NIST SP 800-115: titled "Technical Guide to Information Security Testing and Assessment," provides guidance on the planning, conducting, analyzing, and documenting information security/ğŸ“penetration testing and assessment activities. (Plan, Discover, Attack, Report _PDAC_)
    - âš’ï¸NIST SP 800-122: "Guide to Protecting the Confidentiality of Personally Identifiable Information (PII)" - Provides guidelines for protecting the confidentiality of ğŸ“PII.
    - âš’ï¸NIST SP 800-171: "Protecting Controlled Unclassified Information in Nonfederal Systems and Organizations" Specifies requirements for protecting controlled ğŸ“unclassified information.
    - âš’ï¸NIST Cybersecurity Framework (CSF): The NIST Cybersecurity Framework consists of standards, guidelines, and best practices to manage ğŸ“cybersecurity risk. It is organized into five core functions: Identify, Protect, Detect, Respond, and Recover.

- ğŸ”´**Federal Information Processing Standards (FIPS):**
    - âœˆï¸FIPS 199: "Standards for Security Categorization of Federal Information and Information Systems" - Establishes security categories for information and information systems.
    - âœˆï¸FIPS 200: "Minimum Security Requirements for Federal Information and Information Systems" - Specifies minimum security requirements for federal information systems.
    - âœˆï¸FIPS 140-2: "Security Requirements for Cryptographic Modules" - Specifies requirements for cryptographic modules.
    - âœˆï¸FIPS 140-3: is the successor to FIPS 140-2, providing updated requirements for cryptographic modules

- ğŸ”´**ISO/IEC 27000 Series:**
    - ğŸš¡ISO/IEC 27001: "Information Security Management Systems (ISMS) â€“ Requirements" - Specifies the requirements for establishing, implementing, maintaining, and continually improving an information security management system.
    - ğŸš¡ISO/IEC 27002: "Code of Practice for Information Security Controls" - Provides guidelines and general principles for initiating, implementing, maintaining, and improving information security management in an organization.
    - ğŸš¡ISO/IEC 27005: "Information Security Risk Management" - Provides guidelines for information security risk management, including risk assessment and treatment.
    - ğŸš¡ISO/IEC 27017: "Code of Practice for Information Security Controls Based on ISO/IEC 27002 for Cloud Services" - Provides guidelines for information security controls applicable to the provision and use of cloud services.
    - ğŸš¡ISO/IEC 27018: "Code of Practice for Protection of Personally Identifiable Information (PII) in Public Clouds Acting as PII Processors" - Provides guidelines for the protection of PII in cloud computing environments.
    - ğŸš¡ISO/IEC 27031: "Guidelines for Information and Communication Technology Readiness for Business Continuity" - Provides guidelines for ICT readiness for business continuity.
    - ğŸš¡ISO/IEC 27035: "Information Security Incident Management" - Provides guidelines for the management of information security incidents, including planning and response.
    - ğŸš¡ISO/IEC 27701: "Privacy Information Management System" - Provides guidelines for establishing, implementing, maintaining, and continually improving a Privacy Information Management System (PIMS) based on the requirements of ISO/IEC 27001.
    - ğŸš¡**ISO 31000 Series:**
        - ISO 31000: "Risk Management â€“ Guidelines" - Provides principles and generic guidelines on risk management.
        - ISO 31010: "Risk Management â€“ Risk Assessment Techniques" - Provides guidance on the selection and application of techniques for assessing risk in a wide range of situations.
    - ğŸš¡**ISO/IEC 20000 Series:**
        - ISO/IEC 20000-1: "Information Technology â€“ Service Management â€“ Part 1: Service Management System Requirements" - Specifies requirements for establishing, implementing, maintaining, and continually improving a service management system (SMS).
        - ISO/IEC 20000-2: "Information Technology â€“ Service Management â€“ Part 2: Guidance on the Application of Service Management Systems" Provides guidance on the application of an SMS based on ISO/IEC 20000-1.

- â„ï¸**SSAE 18:** "Attestation Standards: Clarification and Recodification" - This standard, effective since May 1, 2017, provides guidance on performing and reporting on examination, review, and agreed-upon procedures engagements. It includes guidelines for reports on controls at service organizations, consolidating and clarifying previous standards.
    - SOC Reports under SSAE 18:
        - ğŸ¼SOC 1 (which focuses on controls at a service organization relevant to user entitiesâ€™ internal control over financial reporting)
        - ğŸ¼SOC 2 (which controls relevant to security, availability, processing integrity, confidentiality, and privacy) both have Type I (provides an opinion on the operating effectiveness of those controls at a ğŸ”¥specific point in time) and Type II (provides opinion on the suitability and effectiveness of security controls after evaluating them over a ğŸ”¥specified period of time) reports.
        - ğŸ¼SOC 3 does not have separate Type I or Type II reports; it provides a general-purpose report suitable for public distribution.
    - NoteğŸ“ There are only two types of SOC report: Type I and Type II. Both reports provide information on the suitability of the design of security controls.

